{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"0bae0877624796503feec562c67e2d798a0d0ad3","modified":1625127331709},{"_id":"source/_posts/first-test.md","hash":"a5787c78815510a431c6ee0c3e539a1bfef20407","modified":1625122179866},{"_id":"source/_posts/Machine Learning.md","hash":"565493805e442d6b6446a48504dd4bbf11520a82","modified":1623547559000},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1625121186646},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1514806389000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1514806389000},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1514806389000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1514806389000},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1514806389000},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1514806389000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1514806389000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1514806389000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1514806389000},{"_id":"themes/next/README.cn.md","hash":"58ffe752bc4b7f0069fcd6304bbc2d5ff7b80f89","modified":1514806389000},{"_id":"themes/next/README.md","hash":"898213e66d34a46c3cf8446bf693bd50db0d3269","modified":1514806389000},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1514806389000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1514806389000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1514806389000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1514806389000},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1514806389000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1514806389000},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1514806389000},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1514806389000},{"_id":"themes/next/_config.yml","hash":"5ff37e90e4d6812c8fc40c03fa6e1d903d743470","modified":1514806389000},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1514806389000},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1514806389000},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1514806389000},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1514806389000},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1514806389000},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1514806389000},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1514806389000},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1514806389000},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1514806389000},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1514806389000},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1514806389000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1514806389000},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1514806389000},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1514806389000},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1514806389000},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1514806389000},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1514806389000},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1514806389000},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1514806389000},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1514806389000},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1514806389000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1514806389000},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1514806389000},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1514806389000},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1514806389000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1514806389000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1514806389000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1514806389000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1514806389000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1514806389000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1514806389000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1514806389000},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1514806389000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1514806389000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1514806389000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1514806389000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1514806389000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1514806389000},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1514806389000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1514806389000},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1514806389000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1514806389000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1514806389000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1514806389000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1514806389000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1514806389000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1514806389000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1514806389000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1514806389000},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1514806389000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1514806389000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1514806389000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1514806389000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1514806389000},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1514806389000},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1514806389000},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1514806389000},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1514806389000},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1514806389000},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1514806389000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1514806389000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1514806389000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1514806389000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1514806389000},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1514806389000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1514806389000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1514806389000},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1514806389000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1514806389000},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1514806389000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1514806389000},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1514806389000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1514806389000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1514806389000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1514806389000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1514806389000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1514806389000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1514806389000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1514806389000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1514806389000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1514806389000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1514806389000},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1514806389000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1514806389000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1514806389000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1514806389000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1514806389000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1514806389000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1514806389000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1514806389000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1514806389000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1514806389000},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1514806389000},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1514806389000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1514806389000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1514806389000},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1514806389000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1514806389000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1514806389000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1514806389000},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1514806389000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1514806389000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1514806389000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1514806389000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1514806389000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1514806389000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1514806389000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1514806389000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1514806389000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1514806389000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1514806389000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1514806389000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1514806389000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1514806389000},{"_id":"public/2021/07/01/hello-world/index.html","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1625128708914},{"_id":"public/archives/index.html","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1625128708914},{"_id":"public/2021/07/01/first-test/index.html","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1625128708914},{"_id":"public/archives/2021/index.html","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1625128708914},{"_id":"public/2021/05/25/Machine Learning/index.html","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1625128708914},{"_id":"public/archives/2021/05/index.html","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1625128708914},{"_id":"public/archives/2021/07/index.html","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1625128708914},{"_id":"public/index.html","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1625128708914},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1625128708914},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1625128708914},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1625128708914},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1625128708914},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1625128708914},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1625128708914},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1625128708914},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1625128708914},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1625128708914},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1625128708914},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1625128708914},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1625128708914},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1625128708914},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1625128708914},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1625128708914},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1625128708914},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1625128708914},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1625128708914},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1625128708914},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1625128708914},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1625128708914},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1625128708914},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1625128708914},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1625128708914},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1625128708914},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1625128708914},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1625128708914},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1625128708914},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1625128708914},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1625128708914},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1625128708914},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1625128708914},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1625128708914},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1625128708914},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1625128708914},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1625128708914},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1625128708914},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1625128708914},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1625128708914},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1625128708914},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1625128708914},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1625128708914},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1625128708914},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1625128708914},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1625128708914},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1625128708914},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1625128708914},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1625128708914},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"bc6cf8951a99074bdc6ec9172f03fb7c0e412729","modified":1625128708914},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1625128708914},{"_id":"public/lib/fastclick/README.html","hash":"c88ed76304392b9e77b266fcbbc05f443c5df133","modified":1625128708914},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1625128708914},{"_id":"public/lib/jquery_lazyload/README.html","hash":"96a5c2cc00de5b338ae972c1e8de879d2d919608","modified":1625128708914},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1625128708914},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1625128708914},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1625128708914},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1625128708914},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1625128708914},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1625128708914},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1625128708914},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1625128708914},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1625128708914},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1625128708914},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1625128708914},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1625128708914},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1625128708914},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1625128708914},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1625128708914},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1625128708914},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1625128708914},{"_id":"public/css/main.css","hash":"c977ecfb62bfd2e992a896bee252728b1d2c9150","modified":1625128708914},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1625128708914},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1625128708914},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1625128708914},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1625128708914},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1625128708914},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1625128708914},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1625128708914},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1625128708914},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1625128708914},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1625128708914},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1625128708914},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1625128708914},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1625128708914},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1625128708914},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1625128708914},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1625128708914},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1625128708914},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1625128708914},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1625128708914},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1625128708914},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1625128708914},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1625128708914},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1625128708914},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1625128708914},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1625128708914},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1625128708914}],"Category":[],"Data":[],"Page":[],"Post":[{"_content":"\n\n# [Machine Learning](https://www.bilibili.com/video/BV164411b7dx)\n\n\n\n## Index\n\n[toc]\n\n\n\n## 1 Introduction\n\n### 1.1 Supervised Learn\n\nA \"right answer\" given\n\n#### Regression\n\nPredict continuous valued output (e.g. housing price)\n\n**Related algorithms**:\n\n- Linear regression\n- Neural Networks\n- Nearest Neighbor\n\n#### Classification\n\nDiscrete valued output (0 or 1)\n\n**Related algorithms**:\n\n- Logistic regression\n- K-Nearest Neighbor (KNN)\n- Support Vector Machines (SVM)\n- Nave Bayes\n- Decision Trees\n- Neural Networks\n\n\n\n### 1.2 Unsupervised Learn\n\n#### [Cluster](https://zhuanlan.zhihu.com/p/78382376)\n\n**Applications**:\n\n> \n\n- Market segmentation\n- Social network analysis\n- Organize computing cluster\n- Astronomical data analysis\n\n[**Related algorithms**](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68):\n\n- K-Means Clustering\n- Mean-Shift Clustering\n- Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n- ExpectationMaximization (EM) Clustering using Gaussian Mixture Models (GMM)\n\n\n\n## 2 Linear Regression with One Variable\n\n### 2.0 Model Representation - Notation\n\n$m$: number of training examples\n\n$x$'s: input variable/feature\n\n$y$'s: output variable/feature\n\n$(x,y)$: a training example\n\n$(x^i,y^i)$: $i$ represents the $i^{th}$\n\n\n\n### 2.1 Model and Cost Function\n\n**Hypothesis **: \n$$\nh_(x)=_0+_1x\n$$\n**Parameters**: $\\theta_0,\\theta_1$\n\n> $\\theta_1$$\\theta_0$\n\n[**Cost Function **](https://www.cnblogs.com/geaozhang/p/11442343.html):\n$$\nJ(_0,_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2\n$$\n\n> minimize squared error cost function ()\n\n**Goal**: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$\n\n\n\n### 2.2 Parameter Learning - Gradient Descent\n\n#### Outline\n\n- Start with some $\\theta_0,\\theta_1$\n\n- Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum\n\n#### Gradient descent algorithm\n\n\tRepeat until convergence {\n\n\t\t$\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,\\theta_1)$\t(for j=0 and j=1) \n\n}\n\n> $:=$ colon equals, which used to denote assignment ()\n>\n> $\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent\n>\n> $\\frac{\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term ()\n>\n> **Assert**: simultaneous update $\\theta_0,\\theta_1$ \"at the same time\"\n\n#### Gradient descent intuition\n\n$$\n\\theta_1:=\\theta_1-\\alpha\\frac{\\theta_j}J(\\theta_1)\n$$\n\n1. $\\alpha$\n\n   if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.\n\n   if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.\n\n2. Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.\n\n3. As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.\n\n#### Gradient descent for linear regression\n\n> Apply gradient descent to minimize squared error cost function \n\n$$\\frac{\\theta_j}J(\\theta_0,\\theta_1)=\\frac{\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$\t\t*Expanding the formula*\n\nSubstituting 0 and 1 into $j$\n\n$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})$$\n\n$$\\theta_1:j=1:\\frac{\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}$$\n\nWhen work out the derivatives, which is the slope of the cost function *J*, plug them back in to gradient descent algorithm (remember to update simultaneously).\n\n**Convex function**\n\n> It doesn't have any local optimum except for the global optimum\n\n**\"Batch\" Gradient Descent**\n\n> \"Batch\": Each step of gradient descent uses all the training examples (entire training set)\n\n\n\n## 3 Linear Algebra review (optional)\n\n### 3.1 Matrices and vectors\n\n#### Matrix\n\n> Rectangular array of numbers\n\n32 matrix: $\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\\\ 5&6\\end{bmatrix}$\t23 matrix: $\\begin{bmatrix}1 &2&3 \\\\6& 3 & 4\\\\ \\end{bmatrix}$\n\n**Dimension of matrix**: number of rows $\\times$ number of columns\n\n> The above matrix can be also write as $\\mathbb R^{3\\times2}$ \n\n**Refer to specific elements of the matrix **(entries of matrix)\n\n$A=\\begin{bmatrix}1402&191 \\\\ 1371 &821\\\\ 949&1437\\\\147&1448\\end{bmatrix}$\n\n$A_{ij}=$ \"$i$,$j$ entry\" in the $i^{th}$ row, $j^{th}$ column.\n\n$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$\n\n#### Vector\n\n> An $n\\times1$ matrix\n\n$y=\\begin{bmatrix}460\\\\232\\\\315\\\\178\\end{bmatrix}$\n\n$y_i=i^{th}$ element\n\n> It is often customary to use uppercase letters for matrices and lowercase letters for vectors\n\n\n\n### 3.2 Addition and Scalar multiplication\n\n\n\n### 3.3 Matrix-vector multiplication\n\n**Details**:\n\n            $A$           $\\times$   $x$       $=$       $y$\n\n$\\begin{bmatrix}&&&&&\\\\ \\\\ \\\\ \\end{bmatrix}\\times\\begin{bmatrix}\\\\ \\\\ \\\\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\\\ \\\\ \\\\ \\\\ \\end{bmatrix}$\n\n   m$\\times$n matrix        n$\\times$1    m-dimensional vector\n\nTo get $y_i$, multiply $A$' $i^{th}$ row with elements of vector $x$, and add them up.\n\n**Calculation Tips**\n\nHouse sizes: 2104,1216, 1534, 852\n\nCompeting hypotheses: $h_\\theta(x)=-40+0.25x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\\\0.25\\end{bmatrix}$ \n\n\n\n### 3.4 Matrix-matrix multiplication\n\n**Details**:\n\n$A\\times B=C$\n\n[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o\n\nThe $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,...,0)\n\n**Example**:\n\n$\\begin{bmatrix}1&3\\\\2&5\\end{bmatrix}\\begin{bmatrix}0&1\\\\3&2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&1\\times1+3\\times2 \\\\2\\times0+5\\times3&2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&7\\\\15&12\\end{bmatrix}$\n\n**Calculation Tips II**\n\nHouse sizes: 2104,1216, 1534, 852\n\nThree competing hypotheses: \n\n1. $h_\\theta(x)=-40+0.25x$\n2. $h_\\theta(x)=200+0.1x$\n3. $h_\\theta(x)=-150+0.4x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40&200&-150 \\\\ 0.25&0.1&0.4 \\end{bmatrix}$\n\n\n\n### 3.5 Matrix multiplication properties\n\nLet $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (**Not commutative**) \n\n#### Identity Matrix\n\nDenoted $I$ (or $I_{n\\times n}$).\n\nExample of identity matrices:\n\n2$\\times$2: $\\begin{bmatrix}1&0 \\\\ 0&1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&0&0 \\\\ 0&1&0 \\\\ 0&0&1\\end{bmatrix}$      $\\cdots$\n\nFor any matrix $A$,\n\n$$\nA\\cdot I=I\\cdot A=A\n$$\n\n> Implicit conditions of the formula: \n>\n> $A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$\n\n\n\n### 3.6 Inverse and Transpose\n\n> \n\nNot all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&0 \\\\ 0&0\\end{bmatrix}$)\n\n> Matrices that don't have an inverse are \"singular\" or \"degenerate\".\n\n#### Matrix inverse\n\nIf A is an m$\\times$m matrix, and if it has an inverse,\n$$\nAA^{-1}=A^{-1}A=I\n$$\n\n> An m$\\times$m matrix called a square matrix (), only square matrix has an inverse. \n\n#### Matrix transpose\n\nExample:\n$$\nA=\\begin{bmatrix}1&2&0 \\\\ 3&5&9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&3 \\\\ 2&5 \\\\ 0&9 \\end{bmatrix}\n$$\nLet $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.\n\n\n\n## 4 Linear Regression with Multiple Variables\n\n### 4.1 Multiple feature\n\n#### Notation\n\n$n$ = number of features\n\n$x^{(i)}$ = input (features) of $i^{th}$ training example\n\n$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example\n\n#### Multivariate linear regression\n\n> \n\n**Hypothesis**:\n\n$h_(x)=_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\nFor convenience of notation, define $x_0=1$. Then\n\n$h_(x)=_0x_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n          $=\\theta^Tx$\n\n          $=\\begin{bmatrix}\\theta_0&\\theta_1&\\cdots&\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix}$\n\n\n\n### 4.2 Gradient descent for multiple variables\n\n- \n\n- \n\n**Hypothesis**: $h_(x)=\\theta^Tx=_0x_1+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n**Parameters**: $\\theta_0$,$\\theta_1$,...,$\\theta_n$\n\n**Cost function**: $J(\\theta_0$,$\\theta_1$,...,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2$\n\n**Gradient descent**:\n\n\tRepeat {\n\n\t\t$\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,...,\\theta_n)$\t\t\n\n}\t\t(simultaneously update for every $j=0,...,n$)\n\n> $J(\\theta_0,...,\\theta_n)$ can be instead by $J(\\theta)$\n\n**New algorithm** for $n\\ge1$:\n\n\tRepeat {\n\n\t\t$\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Previously $(n=1)$ is in 2.2 \n\n\n\n### 4.3 Gradient descent in practice I: Feature Scaling\n\n> \n\n#### Feature Scaling\n\n> **Idea**: Make sure flatten are on a similar scale.\n\nE.g. $x_1$= size(0-2000 $feet^2$)\n\n      $x_2$= number of bedrooms (1-5)\n\nIt will be better to limit both $x_1$ and $x_2$ in [0,1]\n\n$x_1=\\frac{size(feet^2)}{2000}$\n\n$x_2=\\frac{numberOfBedroom}{5}$\n\n> \n\nMore general, get every feature into approximately a $-1\\le x_i\\le1$ range.\n\n> -11\n\n#### Mean normalization\n\nReplace $x_i$ with $x_i-_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)\n**E.g.** \n\n$x_1=\\frac{size(feet^2)-1000}{2000}$\n\n$x_2=\\frac{numberOfBedroom-2}{5}$\n\n> $_i$ (1000 and 2) is considered as the average value of $x_i$ in training set\n\n$$\nx_i\\leftarrow \\frac{x_i-_i}{range(max-min)}\n$$\n\n\n\n### 4.4 Gradient descent in practice II: Learning rate\n\n- The chapter will center around the learning rate $\\alpha$\n\n**Gradient descent**\n\n* $\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta)$\n* \"Debugging\": How to make sure gradient descent is working correctly.\n* How to choose learning rate $\\alpha$\n\nDeclare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.\n\n> $min_\\theta J(\\theta)$$y=|\\frac 1x|$$(\\epsilon)$threshold$\\epsilon$\n\n> $\\alpha$0$\\alpha$\n\n**Summary**:\n\n* If $\\alpha$ is too small: slow convergence.\n* If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.\n\nRecommended choices for $\\alpha$:\n\n..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1,... \n\n\n\n### 4.5 Features and polynomial regression\n\n> \n\nHousing prices prediction:\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth\n$$\n\n> It's better to use $area$ which is equal to $frontage\\times depth$ as new feature.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times area\n$$\n\n#### Choice of feature\n\nSuppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2\n$$\n\n> -\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3\n$$\n\n> $(size,(size)^2,(size)^3)$\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}\n$$\n\n> xarea\n\n**Summary**:\n\nYou have a choice in what features to use to fix more complex functions to your data!\n\n\n\n### 4.6 Normal equation (unfinished)\n\n> $\\theta$$\\theta$\n\n**Normal equation**: Method to solve for $\\theta$ analytically.\n\n\n\n#### Compare to Gradient Descent\n\n$m$ training example, $n$ features.\n\n| Gradient Descent            | Normal Equation               |\n| --------------------------- | ----------------------------- |\n| Need to choice a $\\alpha$   | No need to choice a $\\alpha$  |\n| Needs many iterations       | Don't need to iterate         |\n| Work well even $n$ is large | Need to compute $(X^TX)^{-1}$ |\n|                             | Slow if $n$ is very large     |\n\n\n$$\n\\theta=(X^TX)^{-1}X^Ty\n$$\n$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$\n\n**Octave**: `pinv(X'*X)*X'*y` \n\n> `X'` is the transpose of $X$\n>\n> `pinv` is a function  to compute the inverse of a matrix\n\n\n\n<!--unfinished-->\n\n### 4.7 Normal equation and non-invertibility (optional) (unfinished)\n\n<!--unfinished-->\n\n\n\n## 5 Octave Tutorial (ignored)\n\n<!--unfinished-->\n\n\n\n## 6 Logistic Regression\n\n> \n\n### 6.1 Classification\n\n**Classification**\n\n$y\\in \\{0,1\\} $\n\n0: \"Negative Class\" (e.g., benign tumor)\n\n1: \"Positive Class\" (e.g., malignant tumor)\n\n> There are multi-class problems as well that y can take value from 0, 1, 2, 3,...\n\nLearning regression isn't fit the classification problem\n\nIn the [video](https://www.bilibili.com/video/BV164411b7dx?p=32&t=160) there is an example to explain it. \n\nAnother example:\n\nClassification: y = 0 or 1\n\n\t$H_\\theta(x)$ can be $>1$ or $<0$ if we use the linear regression\n\n> Obviously, the label is either 0 or 1.\n\nLogistic Regression: $0\\le h_\\theta(x)\\le1$\n\n> This is a classification algorithm whose output always between 1 and 0. Besides, it's a classification algorithm instead of linear regression algorithm though there is a \"regression\" in its name.\n\n\n\n### 6.2 Hypothesis Representation\n\n> \n\n- What is the function we're going to use to representation hypothesis when we have a classification problem.\n\n#### Logistic Regression Model\n\n\tWant $0\\le h_\\theta(x)\\le1$\n\n$$\nh_\\theta(x)=g(\\theta^Tx)\n$$\n**Sigmoid function (Logistic function)**: \n$$\ng(z)=\\frac1{1+e^{-z}}\n$$\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" />\n\n> It's graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$\n\nThus,\n$$\nh_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}\n$$\n\n\n> $\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx<0$ then  $h_\\theta(x)=0$\n\n**Interpretation of Hypothesis Output**\n\n$h_\\theta(x)=$ estimated probability that y = 1 on input x\n\nExample: if $x=\\begin{bmatrix}x_0 \\\\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\\\ tumorSize\\end{bmatrix}$\n\n\t\t\t\t$h_\\theta(x)=0.7$\n\nTell patient that 70% chance of tumor being malignant.\n\n#### Mathematical formula definition of the hypothesis for logistic regression \n\n\"Probability that y=1, given x, parameterized by $\\theta$\": \n$$\nP(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\\\\nP(y=0|x;\\theta)=1-P(y=1|x;\\theta)\n$$\n\n\n### 6.3 Decision boundary\n\n> \n\n- What logistic regression hypothesis function is computing?\n\n  \n\nAccording to Logistic regression, \n\nsuppose predict \"$y=1$\" If $h_\\theta(x)\\ge0.5$\n\npredict \"$y=0$\" If $h_\\theta(x)\\le0.5$\n\n#### Decision Boundary\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\nSuppose the variable procedure to be specified. \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$\n\nAnd, $\\theta=\\begin{bmatrix}-3 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nPredict \"$y=1$\"if $-3+x_1+x_2\\ge0$\n\n\t\t\t         \t\t\t\t$x_1+x_2\\ge3$\n\nThe magenta line is called **Decision Boundary**.\n\n> The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.\n\n#### Non-linear decision boundaries\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\nAssuming hypothesis likes this: \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$\n\nAnd assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nThen, predict \"$y=1$\" if $-1+x_1^2+x_2^2\\ge0$\n\n                                               $x_1^2+x_2^2\\ge1$\n\n> The training set used to fit the parameters $\\theta$\n\n\n\n### 6.4 Cost function\n\n- How to automatically choose the parameters $\\theta$ to a training set.\n\n- Define the optimization objective or the cost function that used to fit the parameters.\n\n\n\nHere is to supervised learning problem of fitting a logistic regression model.\n\nTraining set: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})\\}$\n\n$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in\\{0,1\\}$\n\n$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\nHow to choose parameters $\\theta$ ? (The next sections will focus on this problem)\n\n\n\n#### Cost function - Logistic regression cost function\n\n\tLinear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_(x^{(i)})-y^{(i)})^2$\n\n\t$Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$\n\n> $\\theta$non-convex function$\\frac1{1+e^{-\\theta^T x}}$\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n**Logistic regression cost function**\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases}\n$$\n\n\nIf y = 1\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\nIf y = 0\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n### 6.5 simplified cost function and gradient descent\n\n- Figure out a simpler way to write the cost function\n\n- Also figure out how to apply gradient descent to fit the parameters of logistic regression\n\n#### Logistic regression cost function\n\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})\n$$\n\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases} \\\\ \\;\\\\\nNote:y=0\\;or\\;1\\;always\n$$\n\nCompress them into one equation:\n\n$$\nCost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))\n$$\n\n\n**Logistic regression cost function**\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})\\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\;\\;\\; \n=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]\n$$\n\n> the principle maximum likelihood estimation\n\nTo fit parameters $\\theta$: \n$$\n\\min_\\theta J(\\theta)\n$$\n\n> find the $\\theta$ which minimizes $J(\\theta)$\n\nTo make prediction given new $x$:\n\n\tOutput $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\n> So how to minimize $J(\\theta)$, or how to choose parameter $\\theta$ ?\n\n\n\n#### Implementation of logistic regression\n\n**Gradient Descent**\n\n$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$\n\nWhat $\\min_\\theta J(\\theta)$:\t\n\n\tRepeat {\n\n\t\t\t\t$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n\t\t}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.\n\n\n\n### 6.6 Advanced optimization\n\n- Some advanced optimization algorithms \n- Some advanced optimization concepts\n\n> :see_no_evil:\n\n#### Optimization algorithm\n\nCost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.\n\nGiven $\\theta$, we have code that compute\n\n- $J(\\theta)$\n- $\\frac{\\theta_j}J(\\theta)$      (For $j=0,1,,n$)\n\nOptimization algorithms:\n\n- Gradient descent\n- Conjugate gradient\n- BFGS ()\n- L-BFGS\n\nOthers algorithm compare to gradient descent\n\n| Advantages                         | Disadvantages |\n| ---------------------------------- | ------------- |\n| No need to manually pick $\\alpha$  | More complex  |\n| Often faster than gradient descent |               |\n\n> These complex algorithms have a \"clever inner-loop\" called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.\n>\n> :older_man:\n\n\n\n### 6.7 Multi-class classification: One-vs-all (unfinished)\n\n- How to get logistic regression to work for multi-class classification problems\n- One-versus-all classification algorithm\n\n#### Multiclass classification\n\n<!--unfinished-->\n\n\n\n## 7 Regularization (unfinished)\n\n> \n\n### 7.1 The problem of overfitting\n\n- Explain what is overfitting problem\n\nExample: Linear regression (housing prices)\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n> \"Underfit\"\"High bias\",\n\n> \"Overfit\"\"High variance\",\n\n#### Overfitting\n\nIf we have too many features, the learned hypothesis may fit the training set very well ($J()=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).\n\nExample: Logistic regression\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n#### Addressing overfitting\n\nIf we have la lot of features, and very little training data, then overfitting can become a problem.\n\nTwo main options:\n\n1. Reduce number of features\n   - Manually select which features to keep (but some feature must be abandoned)\n   - Model selection algorithm (later will explain)\n2. Regularization\n   - Keep all the features, but reduce magnitude/values of parameters $\\theta_j$\n   - Works well when we have a lot of features, each of which contributes a bit to prediction $y$.\n\n<!--unfinished-->\n\n\n\n## 8 Neural Networks: Representation\n\n### 8.1 Non-linear hypothesis\n\nWhy we need Neural Networks?\n\n> \n\nThe neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.\n\n### 8.2 Neurons and the brain\n\n**History of  neural networks**\n\n- Origins: Algorithms that try to mimic the brain.\n- WAs very widely used in 80s and early 90s; popularity diminished in late 90s.\n- Recent resurgence: State-of-the-art technique for many applications.\n\n**The \"one learning algorithm\" hypothesis**\n\n- Neuro-rewiring experiments\n- Sensor representations in the brain\n\n\n\n### 8.3 Model representation I\n\n- How we represent  Neural Networks (hypothesis or model).\n\n#### Neuron model: Logistic unit\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n**Sigmoid (logistic) activation function**\n\n> (activation function)$g(z)=\\frac1{1+e^{-z}}$\n\nSometimes we add an extra $x_0$ node (if necessary) called bias unit () or the bias neuron (). It's always equal to 1 so sometime we don't draw it.\n\nIn the neural networks literature, the parameters of model $\\theta$ is also called **weights of a model**.\n\n#### Neural Network\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\nLayer 1: Input layer\n\nLayer 2 : Hidden layer\n\nLayer 3: Output layer\n\n$a_i^{(j)}=$ \"activation\" of unit $i$ in layer $j$.\n\n$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$\n\n> $\\Theta^{(j)}$matrix of weights\n\n$$\na_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\\\\na_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\\\\na_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\\\\nh_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})\n$$\n\nIf networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.\n\n> The superscript $j$ in parentheses means that these values associated with layer $j$\n\n\n\n### 8.4 Model representation II\n\n- How to carry out computation efficiently and show a vectorized implementation.\n- Intuition about why these neural network representation\n\n#### Forward propagation: Vectorized implementation\n\n**Define:**\n\n$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$\n\nThus,\n\n$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$\n\nWe observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.\n\n**Define:**\n$$\nx=\\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)}\\end{bmatrix}\n$$\nFurther towards vectorization: \n$$\nz^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\\\\na^{(2)}=g(z^{(2)})\n$$\n\n>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.\n\nNext, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$\n\n$z^{(3)}=\\Theta^{(2)}a^{(2)}$\n\n$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$\n\n>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.\n\nThis process of computing $h(x)$ is also called **forward propagation**, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.\n\n#### Neural Network learning its own features\n\n> $a_1^{(2)},a_2^{(2)},a_3^{(2)}...$$x_1,x_2,x_3...$ \n\n#### Other network architectures\n\nThe way that neural networks are connected are called the architecture (). So the architecture refers to how different neurons are connected to each other.\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n### 8.5 Examples and intoitions (unfinished)\n\n- A detail example which shows how a neural network can compute a complex nonlinear function of the input\n\n- Why neural network can be used to learn complex nonlinear hypothesis\n\n\n\n## 9 Neural Networks: Learning (unfinished)\n\n\n\n## 10 Advice for applying machine learning (unfinished)\n\n10.1 Decide what to try next\n\n10.2 Evaluating a hypothesis\n\n10.3 Model selection and training/validation/test sets\n\n\n\n## 11 Machine Learning system design (unfinished)\n\n11.1 Prioritizing what to work on: Spam classification example\n\n\n\n## 12 Support Vector Machines (unfinished)\n\n12.1 Optimizaion object\n\n- Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions\n\n\n\n## 13 Clustering\n\n### 13.0 Notation\n\nTrain set: $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$  (without labels)\n\n\n\n### 13.1 K-means algorithm\n\n> K\n\nK-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.\n\n**Input**:\n\n- $K$ (number of clusters)\n- Training set $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$\n\n$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)\n\n#### K-means algorithm\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\t// <u>cluster assignment step</u>\n\n\tfor $i$ = 1 to $m$\n\n\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)\n\n\t// <u>move centroids step</u>\n\n\tfor $k$ = 1 to $K$\n\n\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n> $K$ means the number of centroids and the $k$ means the index of each centriod.\n>\n> If you have a cluster with no points assigned to it, the usual practice is to delete it.\n\n\n\n### 13.2 Optimization objective\n\n- How we can use it to help K-means algorithm find better clusters and avoid local optima.\n\n#### K-means optimization objective\n\n$c^{(i)}$ = index of cluster (1,2,,$K$) to which example $x^{(i)}$ is currently assigned\n\n$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)\n\n$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n\n#### Optimization objective:\n\n$$\nJ(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\\\\n\\min_{c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K} J(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)\n$$\n\n> The cost function $J$ is also called discotion function\n\n**Review K-means algorithm**\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\n\t//minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed\n\n\tfor $i$ = 1 to $m$\n\n\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$\n\n\t//minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed\n\n\tfor $k$ = 1 to $K$\n\n\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n\n\n### 13.3 Randomly initialization\n\n- How to initialize K-means\n- How to avoid local optima\n\n> Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\n**Randomly initialization**\n\nUsually, we have $K<m$\n\nRandomsly pick $K$ training examples.\n\nSet $\\mu_1,,\\mu_K$ equal to these $K$ examples.\n\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n> Local optima we should avoid, but randomly initialization may cause this situation.\n\n> \n\n\n\n### 13.4 Choosing the number of cluster\n\n- Manual\n- Elbow method\n- Later downstream purpose\n\n\n\n## 14 Dimensionaliy Reduction (unfinished)\n\n14.1 Motivation I: Data Compression\n\n14.2 Motivation II: Visualization\n\n14.3 Principle Component Analysis problem formulation (PCA)\n\n> \n\n- Compression algorithm\n\n14.4 Principle Component Analysis algorithm\n\nData preprocessing\n\n\n\n## 15 Anomaly Detection (unfinished)\n\n15.1 Problem motivation\n\n\n\n## 16 Recommeder Systems (unfinished)\n\n\n\n## 17 Large Scale Machine Learning (unfinished)\n\n### 17.1 Learning with large datasets\n\n$$\n\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x_j^{(i)}\n$$\n\n> \t\n\n\n\n### 17.2 Stochastic gradient descent\n\n#### Review linear regression with gradient descent\n\n<!-- unfinished -->\n\n#### Batch gradient descent\n\n#### Stochastic gradient descent\n\n\n\n### 17.3 Mini-batch gradient descent\n\n\n\n\n\n\n\n## Markdown\n\n\\begin{bmatrix}\\end{bmatrix}\n\n\\mathbb R\n\n[](https://www.jianshu.com/p/25f0139637b7)\n\n[2](https://www.jianshu.com/p/e74eb43960a1)\n\n[](https://www.jianshu.com/p/191d1e21f7ed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Machine Learning.md","raw":"\n\n# [Machine Learning](https://www.bilibili.com/video/BV164411b7dx)\n\n\n\n## Index\n\n[toc]\n\n\n\n## 1 Introduction\n\n### 1.1 Supervised Learn\n\nA \"right answer\" given\n\n#### Regression\n\nPredict continuous valued output (e.g. housing price)\n\n**Related algorithms**:\n\n- Linear regression\n- Neural Networks\n- Nearest Neighbor\n\n#### Classification\n\nDiscrete valued output (0 or 1)\n\n**Related algorithms**:\n\n- Logistic regression\n- K-Nearest Neighbor (KNN)\n- Support Vector Machines (SVM)\n- Nave Bayes\n- Decision Trees\n- Neural Networks\n\n\n\n### 1.2 Unsupervised Learn\n\n#### [Cluster](https://zhuanlan.zhihu.com/p/78382376)\n\n**Applications**:\n\n> \n\n- Market segmentation\n- Social network analysis\n- Organize computing cluster\n- Astronomical data analysis\n\n[**Related algorithms**](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68):\n\n- K-Means Clustering\n- Mean-Shift Clustering\n- Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n- ExpectationMaximization (EM) Clustering using Gaussian Mixture Models (GMM)\n\n\n\n## 2 Linear Regression with One Variable\n\n### 2.0 Model Representation - Notation\n\n$m$: number of training examples\n\n$x$'s: input variable/feature\n\n$y$'s: output variable/feature\n\n$(x,y)$: a training example\n\n$(x^i,y^i)$: $i$ represents the $i^{th}$\n\n\n\n### 2.1 Model and Cost Function\n\n**Hypothesis **: \n$$\nh_(x)=_0+_1x\n$$\n**Parameters**: $\\theta_0,\\theta_1$\n\n> $\\theta_1$$\\theta_0$\n\n[**Cost Function **](https://www.cnblogs.com/geaozhang/p/11442343.html):\n$$\nJ(_0,_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2\n$$\n\n> minimize squared error cost function ()\n\n**Goal**: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$\n\n\n\n### 2.2 Parameter Learning - Gradient Descent\n\n#### Outline\n\n- Start with some $\\theta_0,\\theta_1$\n\n- Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum\n\n#### Gradient descent algorithm\n\n\tRepeat until convergence {\n\n\t\t$\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,\\theta_1)$\t(for j=0 and j=1) \n\n}\n\n> $:=$ colon equals, which used to denote assignment ()\n>\n> $\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent\n>\n> $\\frac{\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term ()\n>\n> **Assert**: simultaneous update $\\theta_0,\\theta_1$ \"at the same time\"\n\n#### Gradient descent intuition\n\n$$\n\\theta_1:=\\theta_1-\\alpha\\frac{\\theta_j}J(\\theta_1)\n$$\n\n1. $\\alpha$\n\n   if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.\n\n   if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.\n\n2. Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.\n\n3. As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.\n\n#### Gradient descent for linear regression\n\n> Apply gradient descent to minimize squared error cost function \n\n$$\\frac{\\theta_j}J(\\theta_0,\\theta_1)=\\frac{\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$\t\t*Expanding the formula*\n\nSubstituting 0 and 1 into $j$\n\n$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})$$\n\n$$\\theta_1:j=1:\\frac{\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}$$\n\nWhen work out the derivatives, which is the slope of the cost function *J*, plug them back in to gradient descent algorithm (remember to update simultaneously).\n\n**Convex function**\n\n> It doesn't have any local optimum except for the global optimum\n\n**\"Batch\" Gradient Descent**\n\n> \"Batch\": Each step of gradient descent uses all the training examples (entire training set)\n\n\n\n## 3 Linear Algebra review (optional)\n\n### 3.1 Matrices and vectors\n\n#### Matrix\n\n> Rectangular array of numbers\n\n32 matrix: $\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\\\ 5&6\\end{bmatrix}$\t23 matrix: $\\begin{bmatrix}1 &2&3 \\\\6& 3 & 4\\\\ \\end{bmatrix}$\n\n**Dimension of matrix**: number of rows $\\times$ number of columns\n\n> The above matrix can be also write as $\\mathbb R^{3\\times2}$ \n\n**Refer to specific elements of the matrix **(entries of matrix)\n\n$A=\\begin{bmatrix}1402&191 \\\\ 1371 &821\\\\ 949&1437\\\\147&1448\\end{bmatrix}$\n\n$A_{ij}=$ \"$i$,$j$ entry\" in the $i^{th}$ row, $j^{th}$ column.\n\n$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$\n\n#### Vector\n\n> An $n\\times1$ matrix\n\n$y=\\begin{bmatrix}460\\\\232\\\\315\\\\178\\end{bmatrix}$\n\n$y_i=i^{th}$ element\n\n> It is often customary to use uppercase letters for matrices and lowercase letters for vectors\n\n\n\n### 3.2 Addition and Scalar multiplication\n\n\n\n### 3.3 Matrix-vector multiplication\n\n**Details**:\n\n            $A$           $\\times$   $x$       $=$       $y$\n\n$\\begin{bmatrix}&&&&&\\\\ \\\\ \\\\ \\end{bmatrix}\\times\\begin{bmatrix}\\\\ \\\\ \\\\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\\\ \\\\ \\\\ \\\\ \\end{bmatrix}$\n\n   m$\\times$n matrix        n$\\times$1    m-dimensional vector\n\nTo get $y_i$, multiply $A$' $i^{th}$ row with elements of vector $x$, and add them up.\n\n**Calculation Tips**\n\nHouse sizes: 2104,1216, 1534, 852\n\nCompeting hypotheses: $h_\\theta(x)=-40+0.25x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\\\0.25\\end{bmatrix}$ \n\n\n\n### 3.4 Matrix-matrix multiplication\n\n**Details**:\n\n$A\\times B=C$\n\n[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o\n\nThe $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,...,0)\n\n**Example**:\n\n$\\begin{bmatrix}1&3\\\\2&5\\end{bmatrix}\\begin{bmatrix}0&1\\\\3&2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&1\\times1+3\\times2 \\\\2\\times0+5\\times3&2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&7\\\\15&12\\end{bmatrix}$\n\n**Calculation Tips II**\n\nHouse sizes: 2104,1216, 1534, 852\n\nThree competing hypotheses: \n\n1. $h_\\theta(x)=-40+0.25x$\n2. $h_\\theta(x)=200+0.1x$\n3. $h_\\theta(x)=-150+0.4x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40&200&-150 \\\\ 0.25&0.1&0.4 \\end{bmatrix}$\n\n\n\n### 3.5 Matrix multiplication properties\n\nLet $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (**Not commutative**) \n\n#### Identity Matrix\n\nDenoted $I$ (or $I_{n\\times n}$).\n\nExample of identity matrices:\n\n2$\\times$2: $\\begin{bmatrix}1&0 \\\\ 0&1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&0&0 \\\\ 0&1&0 \\\\ 0&0&1\\end{bmatrix}$      $\\cdots$\n\nFor any matrix $A$,\n\n$$\nA\\cdot I=I\\cdot A=A\n$$\n\n> Implicit conditions of the formula: \n>\n> $A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$\n\n\n\n### 3.6 Inverse and Transpose\n\n> \n\nNot all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&0 \\\\ 0&0\\end{bmatrix}$)\n\n> Matrices that don't have an inverse are \"singular\" or \"degenerate\".\n\n#### Matrix inverse\n\nIf A is an m$\\times$m matrix, and if it has an inverse,\n$$\nAA^{-1}=A^{-1}A=I\n$$\n\n> An m$\\times$m matrix called a square matrix (), only square matrix has an inverse. \n\n#### Matrix transpose\n\nExample:\n$$\nA=\\begin{bmatrix}1&2&0 \\\\ 3&5&9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&3 \\\\ 2&5 \\\\ 0&9 \\end{bmatrix}\n$$\nLet $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.\n\n\n\n## 4 Linear Regression with Multiple Variables\n\n### 4.1 Multiple feature\n\n#### Notation\n\n$n$ = number of features\n\n$x^{(i)}$ = input (features) of $i^{th}$ training example\n\n$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example\n\n#### Multivariate linear regression\n\n> \n\n**Hypothesis**:\n\n$h_(x)=_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\nFor convenience of notation, define $x_0=1$. Then\n\n$h_(x)=_0x_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n          $=\\theta^Tx$\n\n          $=\\begin{bmatrix}\\theta_0&\\theta_1&\\cdots&\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix}$\n\n\n\n### 4.2 Gradient descent for multiple variables\n\n- \n\n- \n\n**Hypothesis**: $h_(x)=\\theta^Tx=_0x_1+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n**Parameters**: $\\theta_0$,$\\theta_1$,...,$\\theta_n$\n\n**Cost function**: $J(\\theta_0$,$\\theta_1$,...,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2$\n\n**Gradient descent**:\n\n\tRepeat {\n\n\t\t$\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,...,\\theta_n)$\t\t\n\n}\t\t(simultaneously update for every $j=0,...,n$)\n\n> $J(\\theta_0,...,\\theta_n)$ can be instead by $J(\\theta)$\n\n**New algorithm** for $n\\ge1$:\n\n\tRepeat {\n\n\t\t$\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Previously $(n=1)$ is in 2.2 \n\n\n\n### 4.3 Gradient descent in practice I: Feature Scaling\n\n> \n\n#### Feature Scaling\n\n> **Idea**: Make sure flatten are on a similar scale.\n\nE.g. $x_1$= size(0-2000 $feet^2$)\n\n      $x_2$= number of bedrooms (1-5)\n\nIt will be better to limit both $x_1$ and $x_2$ in [0,1]\n\n$x_1=\\frac{size(feet^2)}{2000}$\n\n$x_2=\\frac{numberOfBedroom}{5}$\n\n> \n\nMore general, get every feature into approximately a $-1\\le x_i\\le1$ range.\n\n> -11\n\n#### Mean normalization\n\nReplace $x_i$ with $x_i-_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)\n**E.g.** \n\n$x_1=\\frac{size(feet^2)-1000}{2000}$\n\n$x_2=\\frac{numberOfBedroom-2}{5}$\n\n> $_i$ (1000 and 2) is considered as the average value of $x_i$ in training set\n\n$$\nx_i\\leftarrow \\frac{x_i-_i}{range(max-min)}\n$$\n\n\n\n### 4.4 Gradient descent in practice II: Learning rate\n\n- The chapter will center around the learning rate $\\alpha$\n\n**Gradient descent**\n\n* $\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta)$\n* \"Debugging\": How to make sure gradient descent is working correctly.\n* How to choose learning rate $\\alpha$\n\nDeclare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.\n\n> $min_\\theta J(\\theta)$$y=|\\frac 1x|$$(\\epsilon)$threshold$\\epsilon$\n\n> $\\alpha$0$\\alpha$\n\n**Summary**:\n\n* If $\\alpha$ is too small: slow convergence.\n* If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.\n\nRecommended choices for $\\alpha$:\n\n..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1,... \n\n\n\n### 4.5 Features and polynomial regression\n\n> \n\nHousing prices prediction:\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth\n$$\n\n> It's better to use $area$ which is equal to $frontage\\times depth$ as new feature.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times area\n$$\n\n#### Choice of feature\n\nSuppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2\n$$\n\n> -\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3\n$$\n\n> $(size,(size)^2,(size)^3)$\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}\n$$\n\n> xarea\n\n**Summary**:\n\nYou have a choice in what features to use to fix more complex functions to your data!\n\n\n\n### 4.6 Normal equation (unfinished)\n\n> $\\theta$$\\theta$\n\n**Normal equation**: Method to solve for $\\theta$ analytically.\n\n\n\n#### Compare to Gradient Descent\n\n$m$ training example, $n$ features.\n\n| Gradient Descent            | Normal Equation               |\n| --------------------------- | ----------------------------- |\n| Need to choice a $\\alpha$   | No need to choice a $\\alpha$  |\n| Needs many iterations       | Don't need to iterate         |\n| Work well even $n$ is large | Need to compute $(X^TX)^{-1}$ |\n|                             | Slow if $n$ is very large     |\n\n\n$$\n\\theta=(X^TX)^{-1}X^Ty\n$$\n$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$\n\n**Octave**: `pinv(X'*X)*X'*y` \n\n> `X'` is the transpose of $X$\n>\n> `pinv` is a function  to compute the inverse of a matrix\n\n\n\n<!--unfinished-->\n\n### 4.7 Normal equation and non-invertibility (optional) (unfinished)\n\n<!--unfinished-->\n\n\n\n## 5 Octave Tutorial (ignored)\n\n<!--unfinished-->\n\n\n\n## 6 Logistic Regression\n\n> \n\n### 6.1 Classification\n\n**Classification**\n\n$y\\in \\{0,1\\} $\n\n0: \"Negative Class\" (e.g., benign tumor)\n\n1: \"Positive Class\" (e.g., malignant tumor)\n\n> There are multi-class problems as well that y can take value from 0, 1, 2, 3,...\n\nLearning regression isn't fit the classification problem\n\nIn the [video](https://www.bilibili.com/video/BV164411b7dx?p=32&t=160) there is an example to explain it. \n\nAnother example:\n\nClassification: y = 0 or 1\n\n\t$H_\\theta(x)$ can be $>1$ or $<0$ if we use the linear regression\n\n> Obviously, the label is either 0 or 1.\n\nLogistic Regression: $0\\le h_\\theta(x)\\le1$\n\n> This is a classification algorithm whose output always between 1 and 0. Besides, it's a classification algorithm instead of linear regression algorithm though there is a \"regression\" in its name.\n\n\n\n### 6.2 Hypothesis Representation\n\n> \n\n- What is the function we're going to use to representation hypothesis when we have a classification problem.\n\n#### Logistic Regression Model\n\n\tWant $0\\le h_\\theta(x)\\le1$\n\n$$\nh_\\theta(x)=g(\\theta^Tx)\n$$\n**Sigmoid function (Logistic function)**: \n$$\ng(z)=\\frac1{1+e^{-z}}\n$$\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" />\n\n> It's graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$\n\nThus,\n$$\nh_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}\n$$\n\n\n> $\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx<0$ then  $h_\\theta(x)=0$\n\n**Interpretation of Hypothesis Output**\n\n$h_\\theta(x)=$ estimated probability that y = 1 on input x\n\nExample: if $x=\\begin{bmatrix}x_0 \\\\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\\\ tumorSize\\end{bmatrix}$\n\n\t\t\t\t$h_\\theta(x)=0.7$\n\nTell patient that 70% chance of tumor being malignant.\n\n#### Mathematical formula definition of the hypothesis for logistic regression \n\n\"Probability that y=1, given x, parameterized by $\\theta$\": \n$$\nP(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\\\\nP(y=0|x;\\theta)=1-P(y=1|x;\\theta)\n$$\n\n\n### 6.3 Decision boundary\n\n> \n\n- What logistic regression hypothesis function is computing?\n\n  \n\nAccording to Logistic regression, \n\nsuppose predict \"$y=1$\" If $h_\\theta(x)\\ge0.5$\n\npredict \"$y=0$\" If $h_\\theta(x)\\le0.5$\n\n#### Decision Boundary\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\nSuppose the variable procedure to be specified. \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$\n\nAnd, $\\theta=\\begin{bmatrix}-3 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nPredict \"$y=1$\"if $-3+x_1+x_2\\ge0$\n\n\t\t\t         \t\t\t\t$x_1+x_2\\ge3$\n\nThe magenta line is called **Decision Boundary**.\n\n> The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.\n\n#### Non-linear decision boundaries\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\nAssuming hypothesis likes this: \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$\n\nAnd assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nThen, predict \"$y=1$\" if $-1+x_1^2+x_2^2\\ge0$\n\n                                               $x_1^2+x_2^2\\ge1$\n\n> The training set used to fit the parameters $\\theta$\n\n\n\n### 6.4 Cost function\n\n- How to automatically choose the parameters $\\theta$ to a training set.\n\n- Define the optimization objective or the cost function that used to fit the parameters.\n\n\n\nHere is to supervised learning problem of fitting a logistic regression model.\n\nTraining set: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})\\}$\n\n$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in\\{0,1\\}$\n\n$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\nHow to choose parameters $\\theta$ ? (The next sections will focus on this problem)\n\n\n\n#### Cost function - Logistic regression cost function\n\n\tLinear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_(x^{(i)})-y^{(i)})^2$\n\n\t$Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$\n\n> $\\theta$non-convex function$\\frac1{1+e^{-\\theta^T x}}$\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n**Logistic regression cost function**\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases}\n$$\n\n\nIf y = 1\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\nIf y = 0\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n### 6.5 simplified cost function and gradient descent\n\n- Figure out a simpler way to write the cost function\n\n- Also figure out how to apply gradient descent to fit the parameters of logistic regression\n\n#### Logistic regression cost function\n\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})\n$$\n\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases} \\\\ \\;\\\\\nNote:y=0\\;or\\;1\\;always\n$$\n\nCompress them into one equation:\n\n$$\nCost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))\n$$\n\n\n**Logistic regression cost function**\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})\\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\;\\;\\; \n=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]\n$$\n\n> the principle maximum likelihood estimation\n\nTo fit parameters $\\theta$: \n$$\n\\min_\\theta J(\\theta)\n$$\n\n> find the $\\theta$ which minimizes $J(\\theta)$\n\nTo make prediction given new $x$:\n\n\tOutput $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\n> So how to minimize $J(\\theta)$, or how to choose parameter $\\theta$ ?\n\n\n\n#### Implementation of logistic regression\n\n**Gradient Descent**\n\n$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$\n\nWhat $\\min_\\theta J(\\theta)$:\t\n\n\tRepeat {\n\n\t\t\t\t$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n\t\t}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.\n\n\n\n### 6.6 Advanced optimization\n\n- Some advanced optimization algorithms \n- Some advanced optimization concepts\n\n> :see_no_evil:\n\n#### Optimization algorithm\n\nCost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.\n\nGiven $\\theta$, we have code that compute\n\n- $J(\\theta)$\n- $\\frac{\\theta_j}J(\\theta)$      (For $j=0,1,,n$)\n\nOptimization algorithms:\n\n- Gradient descent\n- Conjugate gradient\n- BFGS ()\n- L-BFGS\n\nOthers algorithm compare to gradient descent\n\n| Advantages                         | Disadvantages |\n| ---------------------------------- | ------------- |\n| No need to manually pick $\\alpha$  | More complex  |\n| Often faster than gradient descent |               |\n\n> These complex algorithms have a \"clever inner-loop\" called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.\n>\n> :older_man:\n\n\n\n### 6.7 Multi-class classification: One-vs-all (unfinished)\n\n- How to get logistic regression to work for multi-class classification problems\n- One-versus-all classification algorithm\n\n#### Multiclass classification\n\n<!--unfinished-->\n\n\n\n## 7 Regularization (unfinished)\n\n> \n\n### 7.1 The problem of overfitting\n\n- Explain what is overfitting problem\n\nExample: Linear regression (housing prices)\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n> \"Underfit\"\"High bias\",\n\n> \"Overfit\"\"High variance\",\n\n#### Overfitting\n\nIf we have too many features, the learned hypothesis may fit the training set very well ($J()=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).\n\nExample: Logistic regression\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n#### Addressing overfitting\n\nIf we have la lot of features, and very little training data, then overfitting can become a problem.\n\nTwo main options:\n\n1. Reduce number of features\n   - Manually select which features to keep (but some feature must be abandoned)\n   - Model selection algorithm (later will explain)\n2. Regularization\n   - Keep all the features, but reduce magnitude/values of parameters $\\theta_j$\n   - Works well when we have a lot of features, each of which contributes a bit to prediction $y$.\n\n<!--unfinished-->\n\n\n\n## 8 Neural Networks: Representation\n\n### 8.1 Non-linear hypothesis\n\nWhy we need Neural Networks?\n\n> \n\nThe neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.\n\n### 8.2 Neurons and the brain\n\n**History of  neural networks**\n\n- Origins: Algorithms that try to mimic the brain.\n- WAs very widely used in 80s and early 90s; popularity diminished in late 90s.\n- Recent resurgence: State-of-the-art technique for many applications.\n\n**The \"one learning algorithm\" hypothesis**\n\n- Neuro-rewiring experiments\n- Sensor representations in the brain\n\n\n\n### 8.3 Model representation I\n\n- How we represent  Neural Networks (hypothesis or model).\n\n#### Neuron model: Logistic unit\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n**Sigmoid (logistic) activation function**\n\n> (activation function)$g(z)=\\frac1{1+e^{-z}}$\n\nSometimes we add an extra $x_0$ node (if necessary) called bias unit () or the bias neuron (). It's always equal to 1 so sometime we don't draw it.\n\nIn the neural networks literature, the parameters of model $\\theta$ is also called **weights of a model**.\n\n#### Neural Network\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\nLayer 1: Input layer\n\nLayer 2 : Hidden layer\n\nLayer 3: Output layer\n\n$a_i^{(j)}=$ \"activation\" of unit $i$ in layer $j$.\n\n$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$\n\n> $\\Theta^{(j)}$matrix of weights\n\n$$\na_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\\\\na_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\\\\na_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\\\\nh_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})\n$$\n\nIf networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.\n\n> The superscript $j$ in parentheses means that these values associated with layer $j$\n\n\n\n### 8.4 Model representation II\n\n- How to carry out computation efficiently and show a vectorized implementation.\n- Intuition about why these neural network representation\n\n#### Forward propagation: Vectorized implementation\n\n**Define:**\n\n$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$\n\nThus,\n\n$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$\n\nWe observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.\n\n**Define:**\n$$\nx=\\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)}\\end{bmatrix}\n$$\nFurther towards vectorization: \n$$\nz^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\\\\na^{(2)}=g(z^{(2)})\n$$\n\n>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.\n\nNext, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$\n\n$z^{(3)}=\\Theta^{(2)}a^{(2)}$\n\n$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$\n\n>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.\n\nThis process of computing $h(x)$ is also called **forward propagation**, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.\n\n#### Neural Network learning its own features\n\n> $a_1^{(2)},a_2^{(2)},a_3^{(2)}...$$x_1,x_2,x_3...$ \n\n#### Other network architectures\n\nThe way that neural networks are connected are called the architecture (). So the architecture refers to how different neurons are connected to each other.\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n### 8.5 Examples and intoitions (unfinished)\n\n- A detail example which shows how a neural network can compute a complex nonlinear function of the input\n\n- Why neural network can be used to learn complex nonlinear hypothesis\n\n\n\n## 9 Neural Networks: Learning (unfinished)\n\n\n\n## 10 Advice for applying machine learning (unfinished)\n\n10.1 Decide what to try next\n\n10.2 Evaluating a hypothesis\n\n10.3 Model selection and training/validation/test sets\n\n\n\n## 11 Machine Learning system design (unfinished)\n\n11.1 Prioritizing what to work on: Spam classification example\n\n\n\n## 12 Support Vector Machines (unfinished)\n\n12.1 Optimizaion object\n\n- Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions\n\n\n\n## 13 Clustering\n\n### 13.0 Notation\n\nTrain set: $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$  (without labels)\n\n\n\n### 13.1 K-means algorithm\n\n> K\n\nK-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.\n\n**Input**:\n\n- $K$ (number of clusters)\n- Training set $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$\n\n$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)\n\n#### K-means algorithm\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\t// <u>cluster assignment step</u>\n\n\tfor $i$ = 1 to $m$\n\n\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)\n\n\t// <u>move centroids step</u>\n\n\tfor $k$ = 1 to $K$\n\n\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n> $K$ means the number of centroids and the $k$ means the index of each centriod.\n>\n> If you have a cluster with no points assigned to it, the usual practice is to delete it.\n\n\n\n### 13.2 Optimization objective\n\n- How we can use it to help K-means algorithm find better clusters and avoid local optima.\n\n#### K-means optimization objective\n\n$c^{(i)}$ = index of cluster (1,2,,$K$) to which example $x^{(i)}$ is currently assigned\n\n$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)\n\n$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n\n#### Optimization objective:\n\n$$\nJ(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\\\\n\\min_{c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K} J(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)\n$$\n\n> The cost function $J$ is also called discotion function\n\n**Review K-means algorithm**\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\n\t//minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed\n\n\tfor $i$ = 1 to $m$\n\n\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$\n\n\t//minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed\n\n\tfor $k$ = 1 to $K$\n\n\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n\n\n### 13.3 Randomly initialization\n\n- How to initialize K-means\n- How to avoid local optima\n\n> Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\n**Randomly initialization**\n\nUsually, we have $K<m$\n\nRandomsly pick $K$ training examples.\n\nSet $\\mu_1,,\\mu_K$ equal to these $K$ examples.\n\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n> Local optima we should avoid, but randomly initialization may cause this situation.\n\n> \n\n\n\n### 13.4 Choosing the number of cluster\n\n- Manual\n- Elbow method\n- Later downstream purpose\n\n\n\n## 14 Dimensionaliy Reduction (unfinished)\n\n14.1 Motivation I: Data Compression\n\n14.2 Motivation II: Visualization\n\n14.3 Principle Component Analysis problem formulation (PCA)\n\n> \n\n- Compression algorithm\n\n14.4 Principle Component Analysis algorithm\n\nData preprocessing\n\n\n\n## 15 Anomaly Detection (unfinished)\n\n15.1 Problem motivation\n\n\n\n## 16 Recommeder Systems (unfinished)\n\n\n\n## 17 Large Scale Machine Learning (unfinished)\n\n### 17.1 Learning with large datasets\n\n$$\n\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x_j^{(i)}\n$$\n\n> \t\n\n\n\n### 17.2 Stochastic gradient descent\n\n#### Review linear regression with gradient descent\n\n<!-- unfinished -->\n\n#### Batch gradient descent\n\n#### Stochastic gradient descent\n\n\n\n### 17.3 Mini-batch gradient descent\n\n\n\n\n\n\n\n## Markdown\n\n\\begin{bmatrix}\\end{bmatrix}\n\n\\mathbb R\n\n[](https://www.jianshu.com/p/25f0139637b7)\n\n[2](https://www.jianshu.com/p/e74eb43960a1)\n\n[](https://www.jianshu.com/p/191d1e21f7ed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Machine Learning","published":1,"date":"2021-05-25T08:58:33.000Z","updated":"2021-06-13T01:25:59.000Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"ckqkno9980000y33t63j297j2","content":"<h1 id=\"Machine-Learning\"><a href=\"#Machine-Learning\" class=\"headerlink\" title=\"Machine Learning\"></a><a href=\"https://www.bilibili.com/video/BV164411b7dx\">Machine Learning</a></h1><h2 id=\"Index\"><a href=\"#Index\" class=\"headerlink\" title=\"Index\"></a>Index</h2><p>[toc]</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 Introduction\"></a>1 Introduction</h2><h3 id=\"1-1-Supervised-Learn\"><a href=\"#1-1-Supervised-Learn\" class=\"headerlink\" title=\"1.1 Supervised Learn\"></a>1.1 Supervised Learn</h3><p>A right answer given</p>\n<h4 id=\"Regression\"><a href=\"#Regression\" class=\"headerlink\" title=\"Regression\"></a>Regression</h4><p>Predict continuous valued output (e.g. housing price)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Linear regression</li>\n<li>Neural Networks</li>\n<li>Nearest Neighbor</li>\n</ul>\n<h4 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h4><p>Discrete valued output (0 or 1)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Logistic regression</li>\n<li>K-Nearest Neighbor (KNN)</li>\n<li>Support Vector Machines (SVM)</li>\n<li>Nave Bayes</li>\n<li>Decision Trees</li>\n<li>Neural Networks</li>\n</ul>\n<h3 id=\"1-2-Unsupervised-Learn\"><a href=\"#1-2-Unsupervised-Learn\" class=\"headerlink\" title=\"1.2 Unsupervised Learn\"></a>1.2 Unsupervised Learn</h3><h4 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a><a href=\"https://zhuanlan.zhihu.com/p/78382376\">Cluster</a></h4><p><strong>Applications</strong>:</p>\n<blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>Market segmentation</li>\n<li>Social network analysis</li>\n<li>Organize computing cluster</li>\n<li>Astronomical data analysis</li>\n</ul>\n<p><a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\"><strong>Related algorithms</strong></a>:</p>\n<ul>\n<li>K-Means Clustering</li>\n<li>Mean-Shift Clustering</li>\n<li>Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</li>\n<li>ExpectationMaximization (EM) Clustering using Gaussian Mixture Models (GMM)</li>\n</ul>\n<h2 id=\"2-Linear-Regression-with-One-Variable\"><a href=\"#2-Linear-Regression-with-One-Variable\" class=\"headerlink\" title=\"2 Linear Regression with One Variable\"></a>2 Linear Regression with One Variable</h2><h3 id=\"2-0-Model-Representation-Notation\"><a href=\"#2-0-Model-Representation-Notation\" class=\"headerlink\" title=\"2.0 Model Representation - Notation\"></a>2.0 Model Representation - Notation</h3><p>$m$: number of training examples</p>\n<p>$x$s: input variable/feature</p>\n<p>$y$s: output variable/feature</p>\n<p>$(x,y)$: a training example</p>\n<p>$(x^i,y^i)$: $i$ represents the $i^{th}$</p>\n<h3 id=\"2-1-Model-and-Cost-Function\"><a href=\"#2-1-Model-and-Cost-Function\" class=\"headerlink\" title=\"2.1 Model and Cost Function\"></a>2.1 Model and Cost Function</h3><p><strong>Hypothesis </strong>:<br>$$<br>h_(x)=_0+_1x<br>$$<br><strong>Parameters</strong>: $\\theta_0,\\theta_1$</p>\n<blockquote>\n<p>$\\theta_1$$\\theta_0$</p>\n</blockquote>\n<p><a href=\"https://www.cnblogs.com/geaozhang/p/11442343.html\"><strong>Cost Function </strong></a>:<br>$$<br>J(_0,_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2<br>$$</p>\n<blockquote>\n<p>minimize squared error cost function ()</p>\n</blockquote>\n<p><strong>Goal</strong>: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$</p>\n<h3 id=\"2-2-Parameter-Learning-Gradient-Descent\"><a href=\"#2-2-Parameter-Learning-Gradient-Descent\" class=\"headerlink\" title=\"2.2 Parameter Learning - Gradient Descent\"></a>2.2 Parameter Learning - Gradient Descent</h3><h4 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h4><ul>\n<li><p>Start with some $\\theta_0,\\theta_1$</p>\n</li>\n<li><p>Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum</p>\n</li>\n</ul>\n<h4 id=\"Gradient-descent-algorithm\"><a href=\"#Gradient-descent-algorithm\" class=\"headerlink\" title=\"Gradient descent algorithm\"></a>Gradient descent algorithm</h4><p>    Repeat until convergence {</p>\n<p>        $\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,\\theta_1)$    (for j=0 and j=1) </p>\n<p>}</p>\n<blockquote>\n<p>$:=$ colon equals, which used to denote assignment ()</p>\n<p>$\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent</p>\n<p>$\\frac{\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term ()</p>\n<p><strong>Assert</strong>: simultaneous update $\\theta_0,\\theta_1$ at the same time</p>\n</blockquote>\n<h4 id=\"Gradient-descent-intuition\"><a href=\"#Gradient-descent-intuition\" class=\"headerlink\" title=\"Gradient descent intuition\"></a>Gradient descent intuition</h4><p>$$<br>\\theta_1:=\\theta_1-\\alpha\\frac{\\theta_j}J(\\theta_1)<br>$$</p>\n<ol>\n<li><p>$\\alpha$</p>\n<p>if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.</p>\n<p>if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.</p>\n</li>\n<li><p>Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.</p>\n</li>\n<li><p>As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.</p>\n</li>\n</ol>\n<h4 id=\"Gradient-descent-for-linear-regression\"><a href=\"#Gradient-descent-for-linear-regression\" class=\"headerlink\" title=\"Gradient descent for linear regression\"></a>Gradient descent for linear regression</h4><blockquote>\n<p>Apply gradient descent to minimize squared error cost function </p>\n</blockquote>\n<p>$$\\frac{\\theta_j}J(\\theta_0,\\theta_1)=\\frac{\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$        <em>Expanding the formula</em></p>\n<p>Substituting 0 and 1 into $j$</p>\n<p>$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})$$</p>\n<p>$$\\theta_1:j=1:\\frac{\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}$$</p>\n<p>When work out the derivatives, which is the slope of the cost function <em>J</em>, plug them back in to gradient descent algorithm (remember to update simultaneously).</p>\n<p><strong>Convex function</strong></p>\n<blockquote>\n<p>It doesnt have any local optimum except for the global optimum</p>\n</blockquote>\n<p><strong>Batch Gradient Descent</strong></p>\n<blockquote>\n<p>Batch: Each step of gradient descent uses all the training examples (entire training set)</p>\n</blockquote>\n<h2 id=\"3-Linear-Algebra-review-optional\"><a href=\"#3-Linear-Algebra-review-optional\" class=\"headerlink\" title=\"3 Linear Algebra review (optional)\"></a>3 Linear Algebra review (optional)</h2><h3 id=\"3-1-Matrices-and-vectors\"><a href=\"#3-1-Matrices-and-vectors\" class=\"headerlink\" title=\"3.1 Matrices and vectors\"></a>3.1 Matrices and vectors</h3><h4 id=\"Matrix\"><a href=\"#Matrix\" class=\"headerlink\" title=\"Matrix\"></a>Matrix</h4><blockquote>\n<p>Rectangular array of numbers</p>\n</blockquote>\n<p>32 matrix: $\\begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\\ 5&amp;6\\end{bmatrix}$    23 matrix: $\\begin{bmatrix}1 &amp;2&amp;3 \\6&amp; 3 &amp; 4\\ \\end{bmatrix}$</p>\n<p><strong>Dimension of matrix</strong>: number of rows $\\times$ number of columns</p>\n<blockquote>\n<p>The above matrix can be also write as $\\mathbb R^{3\\times2}$ </p>\n</blockquote>\n<p>**Refer to specific elements of the matrix **(entries of matrix)</p>\n<p>$A=\\begin{bmatrix}1402&amp;191 \\ 1371 &amp;821\\ 949&amp;1437\\147&amp;1448\\end{bmatrix}$</p>\n<p>$A_{ij}=$ $i$,$j$ entry in the $i^{th}$ row, $j^{th}$ column.</p>\n<p>$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$</p>\n<h4 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h4><blockquote>\n<p>An $n\\times1$ matrix</p>\n</blockquote>\n<p>$y=\\begin{bmatrix}460\\232\\315\\178\\end{bmatrix}$</p>\n<p>$y_i=i^{th}$ element</p>\n<blockquote>\n<p>It is often customary to use uppercase letters for matrices and lowercase letters for vectors</p>\n</blockquote>\n<h3 id=\"3-2-Addition-and-Scalar-multiplication\"><a href=\"#3-2-Addition-and-Scalar-multiplication\" class=\"headerlink\" title=\"3.2 Addition and Scalar multiplication\"></a>3.2 Addition and Scalar multiplication</h3><h3 id=\"3-3-Matrix-vector-multiplication\"><a href=\"#3-3-Matrix-vector-multiplication\" class=\"headerlink\" title=\"3.3 Matrix-vector multiplication\"></a>3.3 Matrix-vector multiplication</h3><p><strong>Details</strong>:</p>\n<p>            $A$           $\\times$   $x$       $=$       $y$</p>\n<p>$\\begin{bmatrix}&amp;&amp;&amp;&amp;&amp;\\ \\ \\ \\end{bmatrix}\\times\\begin{bmatrix}\\ \\ \\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\ \\ \\ \\ \\end{bmatrix}$</p>\n<p>   m$\\times$n matrix        n$\\times$1    m-dimensional vector</p>\n<p>To get $y_i$, multiply $A$ $i^{th}$ row with elements of vector $x$, and add them up.</p>\n<p><strong>Calculation Tips</strong></p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Competing hypotheses: $h_\\theta(x)=-40+0.25x$</p>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\0.25\\end{bmatrix}$ </p>\n<h3 id=\"3-4-Matrix-matrix-multiplication\"><a href=\"#3-4-Matrix-matrix-multiplication\" class=\"headerlink\" title=\"3.4 Matrix-matrix multiplication\"></a>3.4 Matrix-matrix multiplication</h3><p><strong>Details</strong>:</p>\n<p>$A\\times B=C$</p>\n<p>[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o</p>\n<p>The $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,,0)</p>\n<p><strong>Example</strong>:</p>\n<p>$\\begin{bmatrix}1&amp;3\\2&amp;5\\end{bmatrix}\\begin{bmatrix}0&amp;1\\3&amp;2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&amp;1\\times1+3\\times2 \\2\\times0+5\\times3&amp;2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&amp;7\\15&amp;12\\end{bmatrix}$</p>\n<p><strong>Calculation Tips II</strong></p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Three competing hypotheses: </p>\n<ol>\n<li>$h_\\theta(x)=-40+0.25x$</li>\n<li>$h_\\theta(x)=200+0.1x$</li>\n<li>$h_\\theta(x)=-150+0.4x$</li>\n</ol>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40&amp;200&amp;-150 \\ 0.25&amp;0.1&amp;0.4 \\end{bmatrix}$</p>\n<h3 id=\"3-5-Matrix-multiplication-properties\"><a href=\"#3-5-Matrix-multiplication-properties\" class=\"headerlink\" title=\"3.5 Matrix multiplication properties\"></a>3.5 Matrix multiplication properties</h3><p>Let $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (<strong>Not commutative</strong>) </p>\n<h4 id=\"Identity-Matrix\"><a href=\"#Identity-Matrix\" class=\"headerlink\" title=\"Identity Matrix\"></a>Identity Matrix</h4><p>Denoted $I$ (or $I_{n\\times n}$).</p>\n<p>Example of identity matrices:</p>\n<p>2$\\times$2: $\\begin{bmatrix}1&amp;0 \\ 0&amp;1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&amp;0&amp;0 \\ 0&amp;1&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$      $\\cdots$</p>\n<p>For any matrix $A$,</p>\n<p>$$<br>A\\cdot I=I\\cdot A=A<br>$$</p>\n<blockquote>\n<p>Implicit conditions of the formula: </p>\n<p>$A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$</p>\n</blockquote>\n<h3 id=\"3-6-Inverse-and-Transpose\"><a href=\"#3-6-Inverse-and-Transpose\" class=\"headerlink\" title=\"3.6 Inverse and Transpose\"></a>3.6 Inverse and Transpose</h3><blockquote>\n<p></p>\n</blockquote>\n<p>Not all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&amp;0 \\ 0&amp;0\\end{bmatrix}$)</p>\n<blockquote>\n<p>Matrices that dont have an inverse are singular or degenerate.</p>\n</blockquote>\n<h4 id=\"Matrix-inverse\"><a href=\"#Matrix-inverse\" class=\"headerlink\" title=\"Matrix inverse\"></a>Matrix inverse</h4><p>If A is an m$\\times$m matrix, and if it has an inverse,<br>$$<br>AA^{-1}=A^{-1}A=I<br>$$</p>\n<blockquote>\n<p>An m$\\times$m matrix called a square matrix (), only square matrix has an inverse. </p>\n</blockquote>\n<h4 id=\"Matrix-transpose\"><a href=\"#Matrix-transpose\" class=\"headerlink\" title=\"Matrix transpose\"></a>Matrix transpose</h4><p>Example:<br>$$<br>A=\\begin{bmatrix}1&amp;2&amp;0 \\ 3&amp;5&amp;9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&amp;3 \\ 2&amp;5 \\ 0&amp;9 \\end{bmatrix}<br>$$<br>Let $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.</p>\n<h2 id=\"4-Linear-Regression-with-Multiple-Variables\"><a href=\"#4-Linear-Regression-with-Multiple-Variables\" class=\"headerlink\" title=\"4 Linear Regression with Multiple Variables\"></a>4 Linear Regression with Multiple Variables</h2><h3 id=\"4-1-Multiple-feature\"><a href=\"#4-1-Multiple-feature\" class=\"headerlink\" title=\"4.1 Multiple feature\"></a>4.1 Multiple feature</h3><h4 id=\"Notation\"><a href=\"#Notation\" class=\"headerlink\" title=\"Notation\"></a>Notation</h4><p>$n$ = number of features</p>\n<p>$x^{(i)}$ = input (features) of $i^{th}$ training example</p>\n<p>$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example</p>\n<h4 id=\"Multivariate-linear-regression\"><a href=\"#Multivariate-linear-regression\" class=\"headerlink\" title=\"Multivariate linear regression\"></a>Multivariate linear regression</h4><blockquote>\n<p></p>\n</blockquote>\n<p><strong>Hypothesis</strong>:</p>\n<p>$h_(x)=_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>For convenience of notation, define $x_0=1$. Then</p>\n<p>$h_(x)=_0x_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>          $=\\theta^Tx$</p>\n<p>          $=\\begin{bmatrix}\\theta_0&amp;\\theta_1&amp;\\cdots&amp;\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix}$</p>\n<h3 id=\"4-2-Gradient-descent-for-multiple-variables\"><a href=\"#4-2-Gradient-descent-for-multiple-variables\" class=\"headerlink\" title=\"4.2 Gradient descent for multiple variables\"></a>4.2 Gradient descent for multiple variables</h3><ul>\n<li><p></p>\n</li>\n<li><p></p>\n</li>\n</ul>\n<p><strong>Hypothesis</strong>: $h_(x)=\\theta^Tx=_0x_1+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p><strong>Parameters</strong>: $\\theta_0$,$\\theta_1$,,$\\theta_n$</p>\n<p><strong>Cost function</strong>: $J(\\theta_0$,$\\theta_1$,,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2$</p>\n<p><strong>Gradient descent</strong>:</p>\n<p>    Repeat {</p>\n<p>        $\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,,\\theta_n)$        </p>\n<p>}        (simultaneously update for every $j=0,,n$)</p>\n<blockquote>\n<p>$J(\\theta_0,,\\theta_n)$ can be instead by $J(\\theta)$</p>\n</blockquote>\n<p><strong>New algorithm</strong> for $n\\ge1$:</p>\n<p>    Repeat {</p>\n<p>        $\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>}        (simultaneously update for $\\theta_j$ for $j=0,,n$)</p>\n<blockquote>\n<p>Previously $(n=1)$ is in 2.2 </p>\n</blockquote>\n<h3 id=\"4-3-Gradient-descent-in-practice-I-Feature-Scaling\"><a href=\"#4-3-Gradient-descent-in-practice-I-Feature-Scaling\" class=\"headerlink\" title=\"4.3 Gradient descent in practice I: Feature Scaling\"></a>4.3 Gradient descent in practice I: Feature Scaling</h3><blockquote>\n<p></p>\n</blockquote>\n<h4 id=\"Feature-Scaling\"><a href=\"#Feature-Scaling\" class=\"headerlink\" title=\"Feature Scaling\"></a>Feature Scaling</h4><blockquote>\n<p><strong>Idea</strong>: Make sure flatten are on a similar scale.</p>\n</blockquote>\n<p>E.g. $x_1$= size(0-2000 $feet^2$)</p>\n<p>      $x_2$= number of bedrooms (1-5)</p>\n<p>It will be better to limit both $x_1$ and $x_2$ in [0,1]</p>\n<p>$x_1=\\frac{size(feet^2)}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom}{5}$</p>\n<blockquote>\n<p></p>\n</blockquote>\n<p>More general, get every feature into approximately a $-1\\le x_i\\le1$ range.</p>\n<blockquote>\n<p>-11</p>\n</blockquote>\n<h4 id=\"Mean-normalization\"><a href=\"#Mean-normalization\" class=\"headerlink\" title=\"Mean normalization\"></a>Mean normalization</h4><p>Replace $x_i$ with $x_i-_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)<br><strong>E.g.</strong> </p>\n<p>$x_1=\\frac{size(feet^2)-1000}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom-2}{5}$</p>\n<blockquote>\n<p>$_i$ (1000 and 2) is considered as the average value of $x_i$ in training set</p>\n</blockquote>\n<p>$$<br>x_i\\leftarrow \\frac{x_i-_i}{range(max-min)}<br>$$</p>\n<h3 id=\"4-4-Gradient-descent-in-practice-II-Learning-rate\"><a href=\"#4-4-Gradient-descent-in-practice-II-Learning-rate\" class=\"headerlink\" title=\"4.4 Gradient descent in practice II: Learning rate\"></a>4.4 Gradient descent in practice II: Learning rate</h3><ul>\n<li>The chapter will center around the learning rate $\\alpha$</li>\n</ul>\n<p><strong>Gradient descent</strong></p>\n<ul>\n<li>$\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta)$</li>\n<li>Debugging: How to make sure gradient descent is working correctly.</li>\n<li>How to choose learning rate $\\alpha$</li>\n</ul>\n<p>Declare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.</p>\n<blockquote>\n<p>$min_\\theta J(\\theta)$$y=|\\frac 1x|$$(\\epsilon)$threshold$\\epsilon$</p>\n</blockquote>\n<blockquote>\n<p>$\\alpha$0$\\alpha$</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<ul>\n<li>If $\\alpha$ is too small: slow convergence.</li>\n<li>If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.</li>\n</ul>\n<p>Recommended choices for $\\alpha$:</p>\n<p>, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, </p>\n<h3 id=\"4-5-Features-and-polynomial-regression\"><a href=\"#4-5-Features-and-polynomial-regression\" class=\"headerlink\" title=\"4.5 Features and polynomial regression\"></a>4.5 Features and polynomial regression</h3><blockquote>\n<p></p>\n</blockquote>\n<p>Housing prices prediction:</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth<br>$$</p>\n<blockquote>\n<p>Its better to use $area$ which is equal to $frontage\\times depth$ as new feature.</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times area<br>$$</p>\n<h4 id=\"Choice-of-feature\"><a href=\"#Choice-of-feature\" class=\"headerlink\" title=\"Choice of feature\"></a>Choice of feature</h4><p>Suppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2<br>$$</p>\n<blockquote>\n<p>-</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3<br>$$</p>\n<blockquote>\n<p>$(size,(size)^2,(size)^3)$</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}<br>$$</p>\n<blockquote>\n<p>xarea</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<p>You have a choice in what features to use to fix more complex functions to your data!</p>\n<h3 id=\"4-6-Normal-equation-unfinished\"><a href=\"#4-6-Normal-equation-unfinished\" class=\"headerlink\" title=\"4.6 Normal equation (unfinished)\"></a>4.6 Normal equation (unfinished)</h3><blockquote>\n<p>$\\theta$$\\theta$</p>\n</blockquote>\n<p><strong>Normal equation</strong>: Method to solve for $\\theta$ analytically.</p>\n<h4 id=\"Compare-to-Gradient-Descent\"><a href=\"#Compare-to-Gradient-Descent\" class=\"headerlink\" title=\"Compare to Gradient Descent\"></a>Compare to Gradient Descent</h4><p>$m$ training example, $n$ features.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Descent</th>\n<th>Normal Equation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Need to choice a $\\alpha$</td>\n<td>No need to choice a $\\alpha$</td>\n</tr>\n<tr>\n<td>Needs many iterations</td>\n<td>Dont need to iterate</td>\n</tr>\n<tr>\n<td>Work well even $n$ is large</td>\n<td>Need to compute $(X^TX)^{-1}$</td>\n</tr>\n<tr>\n<td></td>\n<td>Slow if $n$ is very large</td>\n</tr>\n</tbody></table>\n<p>$$<br>\\theta=(X^TX)^{-1}X^Ty<br>$$<br>$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$</p>\n<p><strong>Octave</strong>: <code>pinv(X&#39;*X)*X&#39;*y</code> </p>\n<blockquote>\n<p><code>X&#39;</code> is the transpose of $X$</p>\n<p><code>pinv</code> is a function  to compute the inverse of a matrix</p>\n</blockquote>\n<!--unfinished-->\n\n<h3 id=\"4-7-Normal-equation-and-non-invertibility-optional-unfinished\"><a href=\"#4-7-Normal-equation-and-non-invertibility-optional-unfinished\" class=\"headerlink\" title=\"4.7 Normal equation and non-invertibility (optional) (unfinished)\"></a>4.7 Normal equation and non-invertibility (optional) (unfinished)</h3><!--unfinished-->\n\n\n\n<h2 id=\"5-Octave-Tutorial-ignored\"><a href=\"#5-Octave-Tutorial-ignored\" class=\"headerlink\" title=\"5 Octave Tutorial (ignored)\"></a>5 Octave Tutorial (ignored)</h2><!--unfinished-->\n\n\n\n<h2 id=\"6-Logistic-Regression\"><a href=\"#6-Logistic-Regression\" class=\"headerlink\" title=\"6 Logistic Regression\"></a>6 Logistic Regression</h2><blockquote>\n<p></p>\n</blockquote>\n<h3 id=\"6-1-Classification\"><a href=\"#6-1-Classification\" class=\"headerlink\" title=\"6.1 Classification\"></a>6.1 Classification</h3><p><strong>Classification</strong></p>\n<p>$y\\in {0,1} $</p>\n<p>0: Negative Class (e.g., benign tumor)</p>\n<p>1: Positive Class (e.g., malignant tumor)</p>\n<blockquote>\n<p>There are multi-class problems as well that y can take value from 0, 1, 2, 3,</p>\n</blockquote>\n<p>Learning regression isnt fit the classification problem</p>\n<p>In the <a href=\"https://www.bilibili.com/video/BV164411b7dx?p=32&t=160\">video</a> there is an example to explain it. </p>\n<p>Another example:</p>\n<p>Classification: y = 0 or 1</p>\n<p>    $H_\\theta(x)$ can be $&gt;1$ or $&lt;0$ if we use the linear regression</p>\n<blockquote>\n<p>Obviously, the label is either 0 or 1.</p>\n</blockquote>\n<p>Logistic Regression: $0\\le h_\\theta(x)\\le1$</p>\n<blockquote>\n<p>This is a classification algorithm whose output always between 1 and 0. Besides, its a classification algorithm instead of linear regression algorithm though there is a regression in its name.</p>\n</blockquote>\n<h3 id=\"6-2-Hypothesis-Representation\"><a href=\"#6-2-Hypothesis-Representation\" class=\"headerlink\" title=\"6.2 Hypothesis Representation\"></a>6.2 Hypothesis Representation</h3><blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>What is the function were going to use to representation hypothesis when we have a classification problem.</li>\n</ul>\n<h4 id=\"Logistic-Regression-Model\"><a href=\"#Logistic-Regression-Model\" class=\"headerlink\" title=\"Logistic Regression Model\"></a>Logistic Regression Model</h4><p>    Want $0\\le h_\\theta(x)\\le1$</p>\n<p>$$<br>h_\\theta(x)=g(\\theta^Tx)<br>$$<br><strong>Sigmoid function (Logistic function)</strong>:<br>$$<br>g(z)=\\frac1{1+e^{-z}}<br>$$<br><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" /></p>\n<blockquote>\n<p>Its graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$</p>\n</blockquote>\n<p>Thus,<br>$$<br>h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}<br>$$</p>\n<blockquote>\n<p>$\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx&lt;0$ then  $h_\\theta(x)=0$</p>\n</blockquote>\n<p><strong>Interpretation of Hypothesis Output</strong></p>\n<p>$h_\\theta(x)=$ estimated probability that y = 1 on input x</p>\n<p>Example: if $x=\\begin{bmatrix}x_0 \\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\ tumorSize\\end{bmatrix}$</p>\n<p>                $h_\\theta(x)=0.7$</p>\n<p>Tell patient that 70% chance of tumor being malignant.</p>\n<h4 id=\"Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\"><a href=\"#Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\" class=\"headerlink\" title=\"Mathematical formula definition of the hypothesis for logistic regression\"></a>Mathematical formula definition of the hypothesis for logistic regression</h4><p>Probability that y=1, given x, parameterized by $\\theta$:<br>$$<br>P(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\<br>P(y=0|x;\\theta)=1-P(y=1|x;\\theta)<br>$$</p>\n<h3 id=\"6-3-Decision-boundary\"><a href=\"#6-3-Decision-boundary\" class=\"headerlink\" title=\"6.3 Decision boundary\"></a>6.3 Decision boundary</h3><blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>What logistic regression hypothesis function is computing?</li>\n</ul>\n<p>According to Logistic regression, </p>\n<p>suppose predict $y=1$ If $h_\\theta(x)\\ge0.5$</p>\n<p>predict $y=0$ If $h_\\theta(x)\\le0.5$</p>\n<h4 id=\"Decision-Boundary\"><a href=\"#Decision-Boundary\" class=\"headerlink\" title=\"Decision Boundary\"></a>Decision Boundary</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\n<p>Suppose the variable procedure to be specified. </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$</p>\n<p>And, $\\theta=\\begin{bmatrix}-3 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Predict $y=1$if $-3+x_1+x_2\\ge0$</p>\n<p>                                     $x_1+x_2\\ge3$</p>\n<p>The magenta line is called <strong>Decision Boundary</strong>.</p>\n<blockquote>\n<p>The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.</p>\n</blockquote>\n<h4 id=\"Non-linear-decision-boundaries\"><a href=\"#Non-linear-decision-boundaries\" class=\"headerlink\" title=\"Non-linear decision boundaries\"></a>Non-linear decision boundaries</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\n<p>Assuming hypothesis likes this: </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$</p>\n<p>And assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\ 0 \\ 0 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Then, predict $y=1$ if $-1+x_1^2+x_2^2\\ge0$</p>\n<p>                                               $x_1^2+x_2^2\\ge1$</p>\n<blockquote>\n<p>The training set used to fit the parameters $\\theta$</p>\n</blockquote>\n<h3 id=\"6-4-Cost-function\"><a href=\"#6-4-Cost-function\" class=\"headerlink\" title=\"6.4 Cost function\"></a>6.4 Cost function</h3><ul>\n<li><p>How to automatically choose the parameters $\\theta$ to a training set.</p>\n</li>\n<li><p>Define the optimization objective or the cost function that used to fit the parameters.</p>\n</li>\n</ul>\n<p>Here is to supervised learning problem of fitting a logistic regression model.</p>\n<p>Training set: ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})}$</p>\n<p>$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in{0,1}$</p>\n<p>$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<p>How to choose parameters $\\theta$ ? (The next sections will focus on this problem)</p>\n<h4 id=\"Cost-function-Logistic-regression-cost-function\"><a href=\"#Cost-function-Logistic-regression-cost-function\" class=\"headerlink\" title=\"Cost function - Logistic regression cost function\"></a>Cost function - Logistic regression cost function</h4><p>    Linear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_(x^{(i)})-y^{(i)})^2$</p>\n<p>    $Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$</p>\n<blockquote>\n<p>$\\theta$non-convex function$\\frac1{1+e^{-\\theta^T x}}$</p>\n</blockquote>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n<p><strong>Logistic regression cost function</strong><br>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases}<br>$$</p>\n<p>If y = 1</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\n<p>If y = 0</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n<h3 id=\"6-5-simplified-cost-function-and-gradient-descent\"><a href=\"#6-5-simplified-cost-function-and-gradient-descent\" class=\"headerlink\" title=\"6.5 simplified cost function and gradient descent\"></a>6.5 simplified cost function and gradient descent</h3><ul>\n<li><p>Figure out a simpler way to write the cost function</p>\n</li>\n<li><p>Also figure out how to apply gradient descent to fit the parameters of logistic regression</p>\n</li>\n</ul>\n<h4 id=\"Logistic-regression-cost-function\"><a href=\"#Logistic-regression-cost-function\" class=\"headerlink\" title=\"Logistic regression cost function\"></a>Logistic regression cost function</h4><p>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})<br>$$</p>\n<p>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases} \\ ;\\<br>Note:y=0;or;1;always<br>$$</p>\n<p>Compress them into one equation:</p>\n<p>$$<br>Cost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))<br>$$</p>\n<p><strong>Logistic regression cost function</strong><br>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})\\<br>\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad;;;<br>=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]<br>$$</p>\n<blockquote>\n<p>the principle maximum likelihood estimation</p>\n</blockquote>\n<p>To fit parameters $\\theta$:<br>$$<br>\\min_\\theta J(\\theta)<br>$$</p>\n<blockquote>\n<p>find the $\\theta$ which minimizes $J(\\theta)$</p>\n</blockquote>\n<p>To make prediction given new $x$:</p>\n<p>    Output $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<blockquote>\n<p>So how to minimize $J(\\theta)$, or how to choose parameter $\\theta$ ?</p>\n</blockquote>\n<h4 id=\"Implementation-of-logistic-regression\"><a href=\"#Implementation-of-logistic-regression\" class=\"headerlink\" title=\"Implementation of logistic regression\"></a>Implementation of logistic regression</h4><p><strong>Gradient Descent</strong></p>\n<p>$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$</p>\n<p>What $\\min_\\theta J(\\theta)$:    </p>\n<p>    Repeat {</p>\n<p>                $\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>        }        (simultaneously update for $\\theta_j$ for $j=0,,n$)</p>\n<blockquote>\n<p>Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.</p>\n</blockquote>\n<h3 id=\"6-6-Advanced-optimization\"><a href=\"#6-6-Advanced-optimization\" class=\"headerlink\" title=\"6.6 Advanced optimization\"></a>6.6 Advanced optimization</h3><ul>\n<li>Some advanced optimization algorithms </li>\n<li>Some advanced optimization concepts</li>\n</ul>\n<blockquote>\n<p>:see_no_evil:</p>\n</blockquote>\n<h4 id=\"Optimization-algorithm\"><a href=\"#Optimization-algorithm\" class=\"headerlink\" title=\"Optimization algorithm\"></a>Optimization algorithm</h4><p>Cost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.</p>\n<p>Given $\\theta$, we have code that compute</p>\n<ul>\n<li>$J(\\theta)$</li>\n<li>$\\frac{\\theta_j}J(\\theta)$      (For $j=0,1,,n$)</li>\n</ul>\n<p>Optimization algorithms:</p>\n<ul>\n<li>Gradient descent</li>\n<li>Conjugate gradient</li>\n<li>BFGS ()</li>\n<li>L-BFGS</li>\n</ul>\n<p>Others algorithm compare to gradient descent</p>\n<table>\n<thead>\n<tr>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No need to manually pick $\\alpha$</td>\n<td>More complex</td>\n</tr>\n<tr>\n<td>Often faster than gradient descent</td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>These complex algorithms have a clever inner-loop called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.</p>\n<p>:older_man:</p>\n</blockquote>\n<h3 id=\"6-7-Multi-class-classification-One-vs-all-unfinished\"><a href=\"#6-7-Multi-class-classification-One-vs-all-unfinished\" class=\"headerlink\" title=\"6.7 Multi-class classification: One-vs-all (unfinished)\"></a>6.7 Multi-class classification: One-vs-all (unfinished)</h3><ul>\n<li>How to get logistic regression to work for multi-class classification problems</li>\n<li>One-versus-all classification algorithm</li>\n</ul>\n<h4 id=\"Multiclass-classification\"><a href=\"#Multiclass-classification\" class=\"headerlink\" title=\"Multiclass classification\"></a>Multiclass classification</h4><!--unfinished-->\n\n\n\n<h2 id=\"7-Regularization-unfinished\"><a href=\"#7-Regularization-unfinished\" class=\"headerlink\" title=\"7 Regularization (unfinished)\"></a>7 Regularization (unfinished)</h2><blockquote>\n<p></p>\n</blockquote>\n<h3 id=\"7-1-The-problem-of-overfitting\"><a href=\"#7-1-The-problem-of-overfitting\" class=\"headerlink\" title=\"7.1 The problem of overfitting\"></a>7.1 The problem of overfitting</h3><ul>\n<li>Explain what is overfitting problem</li>\n</ul>\n<p>Example: Linear regression (housing prices)</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>UnderfitHigh bias,</p>\n</blockquote>\n<blockquote>\n<p>OverfitHigh variance,</p>\n</blockquote>\n<h4 id=\"Overfitting\"><a href=\"#Overfitting\" class=\"headerlink\" title=\"Overfitting\"></a>Overfitting</h4><p>If we have too many features, the learned hypothesis may fit the training set very well ($J()=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).</p>\n<p>Example: Logistic regression</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n<h4 id=\"Addressing-overfitting\"><a href=\"#Addressing-overfitting\" class=\"headerlink\" title=\"Addressing overfitting\"></a>Addressing overfitting</h4><p>If we have la lot of features, and very little training data, then overfitting can become a problem.</p>\n<p>Two main options:</p>\n<ol>\n<li>Reduce number of features<ul>\n<li>Manually select which features to keep (but some feature must be abandoned)</li>\n<li>Model selection algorithm (later will explain)</li>\n</ul>\n</li>\n<li>Regularization<ul>\n<li>Keep all the features, but reduce magnitude/values of parameters $\\theta_j$</li>\n<li>Works well when we have a lot of features, each of which contributes a bit to prediction $y$.</li>\n</ul>\n</li>\n</ol>\n<!--unfinished-->\n\n\n\n<h2 id=\"8-Neural-Networks-Representation\"><a href=\"#8-Neural-Networks-Representation\" class=\"headerlink\" title=\"8 Neural Networks: Representation\"></a>8 Neural Networks: Representation</h2><h3 id=\"8-1-Non-linear-hypothesis\"><a href=\"#8-1-Non-linear-hypothesis\" class=\"headerlink\" title=\"8.1 Non-linear hypothesis\"></a>8.1 Non-linear hypothesis</h3><p>Why we need Neural Networks?</p>\n<blockquote>\n<p></p>\n</blockquote>\n<p>The neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.</p>\n<h3 id=\"8-2-Neurons-and-the-brain\"><a href=\"#8-2-Neurons-and-the-brain\" class=\"headerlink\" title=\"8.2 Neurons and the brain\"></a>8.2 Neurons and the brain</h3><p><strong>History of  neural networks</strong></p>\n<ul>\n<li>Origins: Algorithms that try to mimic the brain.</li>\n<li>WAs very widely used in 80s and early 90s; popularity diminished in late 90s.</li>\n<li>Recent resurgence: State-of-the-art technique for many applications.</li>\n</ul>\n<p><strong>The one learning algorithm hypothesis</strong></p>\n<ul>\n<li>Neuro-rewiring experiments</li>\n<li>Sensor representations in the brain</li>\n</ul>\n<h3 id=\"8-3-Model-representation-I\"><a href=\"#8-3-Model-representation-I\" class=\"headerlink\" title=\"8.3 Model representation I\"></a>8.3 Model representation I</h3><ul>\n<li>How we represent  Neural Networks (hypothesis or model).</li>\n</ul>\n<h4 id=\"Neuron-model-Logistic-unit\"><a href=\"#Neuron-model-Logistic-unit\" class=\"headerlink\" title=\"Neuron model: Logistic unit\"></a>Neuron model: Logistic unit</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n<p><strong>Sigmoid (logistic) activation function</strong></p>\n<blockquote>\n<p>(activation function)$g(z)=\\frac1{1+e^{-z}}$</p>\n</blockquote>\n<p>Sometimes we add an extra $x_0$ node (if necessary) called bias unit () or the bias neuron (). Its always equal to 1 so sometime we dont draw it.</p>\n<p>In the neural networks literature, the parameters of model $\\theta$ is also called <strong>weights of a model</strong>.</p>\n<h4 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\n<p>Layer 1: Input layer</p>\n<p>Layer 2 : Hidden layer</p>\n<p>Layer 3: Output layer</p>\n<p>$a_i^{(j)}=$ activation of unit $i$ in layer $j$.</p>\n<p>$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$</p>\n<blockquote>\n<p>$\\Theta^{(j)}$matrix of weights</p>\n</blockquote>\n<p>$$<br>a_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\<br>a_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\<br>a_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\<br>h_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})<br>$$</p>\n<p>If networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.</p>\n<blockquote>\n<p>The superscript $j$ in parentheses means that these values associated with layer $j$</p>\n</blockquote>\n<h3 id=\"8-4-Model-representation-II\"><a href=\"#8-4-Model-representation-II\" class=\"headerlink\" title=\"8.4 Model representation II\"></a>8.4 Model representation II</h3><ul>\n<li>How to carry out computation efficiently and show a vectorized implementation.</li>\n<li>Intuition about why these neural network representation</li>\n</ul>\n<h4 id=\"Forward-propagation-Vectorized-implementation\"><a href=\"#Forward-propagation-Vectorized-implementation\" class=\"headerlink\" title=\"Forward propagation: Vectorized implementation\"></a>Forward propagation: Vectorized implementation</h4><p><strong>Define:</strong></p>\n<p>$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$</p>\n<p>Thus,</p>\n<p>$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$</p>\n<p>We observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.</p>\n<p><strong>Define:</strong><br>$$<br>x=\\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)}\\end{bmatrix}<br>$$<br>Further towards vectorization:<br>$$<br>z^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\<br>a^{(2)}=g(z^{(2)})<br>$$</p>\n<blockquote>\n<p>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.</p>\n</blockquote>\n<p>Next, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$</p>\n<p>$z^{(3)}=\\Theta^{(2)}a^{(2)}$</p>\n<p>$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$</p>\n<blockquote>\n<p>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.</p>\n</blockquote>\n<p>This process of computing $h(x)$ is also called <strong>forward propagation</strong>, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.</p>\n<h4 id=\"Neural-Network-learning-its-own-features\"><a href=\"#Neural-Network-learning-its-own-features\" class=\"headerlink\" title=\"Neural Network learning its own features\"></a>Neural Network learning its own features</h4><blockquote>\n<p>$a_1^{(2)},a_2^{(2)},a_3^{(2)}$$x_1,x_2,x_3$ </p>\n</blockquote>\n<h4 id=\"Other-network-architectures\"><a href=\"#Other-network-architectures\" class=\"headerlink\" title=\"Other network architectures\"></a>Other network architectures</h4><p>The way that neural networks are connected are called the architecture (). So the architecture refers to how different neurons are connected to each other.</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n<h3 id=\"8-5-Examples-and-intoitions-unfinished\"><a href=\"#8-5-Examples-and-intoitions-unfinished\" class=\"headerlink\" title=\"8.5 Examples and intoitions (unfinished)\"></a>8.5 Examples and intoitions (unfinished)</h3><ul>\n<li><p>A detail example which shows how a neural network can compute a complex nonlinear function of the input</p>\n</li>\n<li><p>Why neural network can be used to learn complex nonlinear hypothesis</p>\n</li>\n</ul>\n<h2 id=\"9-Neural-Networks-Learning-unfinished\"><a href=\"#9-Neural-Networks-Learning-unfinished\" class=\"headerlink\" title=\"9 Neural Networks: Learning (unfinished)\"></a>9 Neural Networks: Learning (unfinished)</h2><h2 id=\"10-Advice-for-applying-machine-learning-unfinished\"><a href=\"#10-Advice-for-applying-machine-learning-unfinished\" class=\"headerlink\" title=\"10 Advice for applying machine learning (unfinished)\"></a>10 Advice for applying machine learning (unfinished)</h2><p>10.1 Decide what to try next</p>\n<p>10.2 Evaluating a hypothesis</p>\n<p>10.3 Model selection and training/validation/test sets</p>\n<h2 id=\"11-Machine-Learning-system-design-unfinished\"><a href=\"#11-Machine-Learning-system-design-unfinished\" class=\"headerlink\" title=\"11 Machine Learning system design (unfinished)\"></a>11 Machine Learning system design (unfinished)</h2><p>11.1 Prioritizing what to work on: Spam classification example</p>\n<h2 id=\"12-Support-Vector-Machines-unfinished\"><a href=\"#12-Support-Vector-Machines-unfinished\" class=\"headerlink\" title=\"12 Support Vector Machines (unfinished)\"></a>12 Support Vector Machines (unfinished)</h2><p>12.1 Optimizaion object</p>\n<ul>\n<li>Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions</li>\n</ul>\n<h2 id=\"13-Clustering\"><a href=\"#13-Clustering\" class=\"headerlink\" title=\"13 Clustering\"></a>13 Clustering</h2><h3 id=\"13-0-Notation\"><a href=\"#13-0-Notation\" class=\"headerlink\" title=\"13.0 Notation\"></a>13.0 Notation</h3><p>Train set: ${x^{(1)},x^{(2)},x^{(3)},x^{(m)}}$  (without labels)</p>\n<h3 id=\"13-1-K-means-algorithm\"><a href=\"#13-1-K-means-algorithm\" class=\"headerlink\" title=\"13.1 K-means algorithm\"></a>13.1 K-means algorithm</h3><blockquote>\n<p>K</p>\n</blockquote>\n<p>K-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.</p>\n<p><strong>Input</strong>:</p>\n<ul>\n<li>$K$ (number of clusters)</li>\n<li>Training set ${x^{(1)},x^{(2)},x^{(3)},x^{(m)}}$</li>\n</ul>\n<p>$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)</p>\n<h4 id=\"K-means-algorithm\"><a href=\"#K-means-algorithm\" class=\"headerlink\" title=\"K-means algorithm\"></a>K-means algorithm</h4><p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {<br>    // <u>cluster assignment step</u></p>\n<p>    for $i$ = 1 to $m$</p>\n<p>        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)</p>\n<p>    // <u>move centroids step</u></p>\n<p>    for $k$ = 1 to $K$</p>\n<p>        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<blockquote>\n<p>$K$ means the number of centroids and the $k$ means the index of each centriod.</p>\n<p>If you have a cluster with no points assigned to it, the usual practice is to delete it.</p>\n</blockquote>\n<h3 id=\"13-2-Optimization-objective\"><a href=\"#13-2-Optimization-objective\" class=\"headerlink\" title=\"13.2 Optimization objective\"></a>13.2 Optimization objective</h3><ul>\n<li>How we can use it to help K-means algorithm find better clusters and avoid local optima.</li>\n</ul>\n<h4 id=\"K-means-optimization-objective\"><a href=\"#K-means-optimization-objective\" class=\"headerlink\" title=\"K-means optimization objective\"></a>K-means optimization objective</h4><p>$c^{(i)}$ = index of cluster (1,2,,$K$) to which example $x^{(i)}$ is currently assigned</p>\n<p>$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)</p>\n<p>$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p>\n<h4 id=\"Optimization-objective\"><a href=\"#Optimization-objective\" class=\"headerlink\" title=\"Optimization objective:\"></a>Optimization objective:</h4><p>$$<br>J(c^{(1)}, c^{(m)},\\mu_1,,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\<br>\\min_{c^{(1)}, c^{(m)},\\mu_1,,\\mu_K} J(c^{(1)}, c^{(m)},\\mu_1,,\\mu_K)<br>$$</p>\n<blockquote>\n<p>The cost function $J$ is also called discotion function</p>\n</blockquote>\n<p><strong>Review K-means algorithm</strong></p>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {</p>\n<p>    //minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed</p>\n<p>    for $i$ = 1 to $m$</p>\n<p>        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$</p>\n<p>    //minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed</p>\n<p>    for $k$ = 1 to $K$</p>\n<p>        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<h3 id=\"13-3-Randomly-initialization\"><a href=\"#13-3-Randomly-initialization\" class=\"headerlink\" title=\"13.3 Randomly initialization\"></a>13.3 Randomly initialization</h3><ul>\n<li>How to initialize K-means</li>\n<li>How to avoid local optima</li>\n</ul>\n<blockquote>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,,\\mu_K \\in \\mathbb R^n$</p>\n</blockquote>\n<p><strong>Randomly initialization</strong></p>\n<p>Usually, we have $K&lt;m$</p>\n<p>Randomsly pick $K$ training examples.</p>\n<p>Set $\\mu_1,,\\mu_K$ equal to these $K$ examples.</p>\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>Local optima we should avoid, but randomly initialization may cause this situation.</p>\n</blockquote>\n<blockquote>\n<p></p>\n</blockquote>\n<h3 id=\"13-4-Choosing-the-number-of-cluster\"><a href=\"#13-4-Choosing-the-number-of-cluster\" class=\"headerlink\" title=\"13.4 Choosing the number of cluster\"></a>13.4 Choosing the number of cluster</h3><ul>\n<li>Manual</li>\n<li>Elbow method</li>\n<li>Later downstream purpose</li>\n</ul>\n<h2 id=\"14-Dimensionaliy-Reduction-unfinished\"><a href=\"#14-Dimensionaliy-Reduction-unfinished\" class=\"headerlink\" title=\"14 Dimensionaliy Reduction (unfinished)\"></a>14 Dimensionaliy Reduction (unfinished)</h2><p>14.1 Motivation I: Data Compression</p>\n<p>14.2 Motivation II: Visualization</p>\n<p>14.3 Principle Component Analysis problem formulation (PCA)</p>\n<blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>Compression algorithm</li>\n</ul>\n<p>14.4 Principle Component Analysis algorithm</p>\n<p>Data preprocessing</p>\n<h2 id=\"15-Anomaly-Detection-unfinished\"><a href=\"#15-Anomaly-Detection-unfinished\" class=\"headerlink\" title=\"15 Anomaly Detection (unfinished)\"></a>15 Anomaly Detection (unfinished)</h2><p>15.1 Problem motivation</p>\n<h2 id=\"16-Recommeder-Systems-unfinished\"><a href=\"#16-Recommeder-Systems-unfinished\" class=\"headerlink\" title=\"16 Recommeder Systems (unfinished)\"></a>16 Recommeder Systems (unfinished)</h2><h2 id=\"17-Large-Scale-Machine-Learning-unfinished\"><a href=\"#17-Large-Scale-Machine-Learning-unfinished\" class=\"headerlink\" title=\"17 Large Scale Machine Learning (unfinished)\"></a>17 Large Scale Machine Learning (unfinished)</h2><h3 id=\"17-1-Learning-with-large-datasets\"><a href=\"#17-1-Learning-with-large-datasets\" class=\"headerlink\" title=\"17.1 Learning with large datasets\"></a>17.1 Learning with large datasets</h3><p>$$<br>\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p>\n<blockquote>\n<p>    </p>\n</blockquote>\n<h3 id=\"17-2-Stochastic-gradient-descent\"><a href=\"#17-2-Stochastic-gradient-descent\" class=\"headerlink\" title=\"17.2 Stochastic gradient descent\"></a>17.2 Stochastic gradient descent</h3><h4 id=\"Review-linear-regression-with-gradient-descent\"><a href=\"#Review-linear-regression-with-gradient-descent\" class=\"headerlink\" title=\"Review linear regression with gradient descent\"></a>Review linear regression with gradient descent</h4><!-- unfinished -->\n\n<h4 id=\"Batch-gradient-descent\"><a href=\"#Batch-gradient-descent\" class=\"headerlink\" title=\"Batch gradient descent\"></a>Batch gradient descent</h4><h4 id=\"Stochastic-gradient-descent\"><a href=\"#Stochastic-gradient-descent\" class=\"headerlink\" title=\"Stochastic gradient descent\"></a>Stochastic gradient descent</h4><h3 id=\"17-3-Mini-batch-gradient-descent\"><a href=\"#17-3-Mini-batch-gradient-descent\" class=\"headerlink\" title=\"17.3 Mini-batch gradient descent\"></a>17.3 Mini-batch gradient descent</h3><h2 id=\"Markdown\"><a href=\"#Markdown\" class=\"headerlink\" title=\"Markdown\"></a>Markdown</h2><p>\\begin{bmatrix}\\end{bmatrix}</p>\n<p>\\mathbb R</p>\n<p><a href=\"https://www.jianshu.com/p/25f0139637b7\"></a></p>\n<p><a href=\"https://www.jianshu.com/p/e74eb43960a1\">2</a></p>\n<p><a href=\"https://www.jianshu.com/p/191d1e21f7ed\"></a></p>\n","site":{"data":{}},"length":24179,"excerpt":"","more":"<h1 id=\"Machine-Learning\"><a href=\"#Machine-Learning\" class=\"headerlink\" title=\"Machine Learning\"></a><a href=\"https://www.bilibili.com/video/BV164411b7dx\">Machine Learning</a></h1><h2 id=\"Index\"><a href=\"#Index\" class=\"headerlink\" title=\"Index\"></a>Index</h2><p>[toc]</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 Introduction\"></a>1 Introduction</h2><h3 id=\"1-1-Supervised-Learn\"><a href=\"#1-1-Supervised-Learn\" class=\"headerlink\" title=\"1.1 Supervised Learn\"></a>1.1 Supervised Learn</h3><p>A right answer given</p>\n<h4 id=\"Regression\"><a href=\"#Regression\" class=\"headerlink\" title=\"Regression\"></a>Regression</h4><p>Predict continuous valued output (e.g. housing price)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Linear regression</li>\n<li>Neural Networks</li>\n<li>Nearest Neighbor</li>\n</ul>\n<h4 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h4><p>Discrete valued output (0 or 1)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Logistic regression</li>\n<li>K-Nearest Neighbor (KNN)</li>\n<li>Support Vector Machines (SVM)</li>\n<li>Nave Bayes</li>\n<li>Decision Trees</li>\n<li>Neural Networks</li>\n</ul>\n<h3 id=\"1-2-Unsupervised-Learn\"><a href=\"#1-2-Unsupervised-Learn\" class=\"headerlink\" title=\"1.2 Unsupervised Learn\"></a>1.2 Unsupervised Learn</h3><h4 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a><a href=\"https://zhuanlan.zhihu.com/p/78382376\">Cluster</a></h4><p><strong>Applications</strong>:</p>\n<blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>Market segmentation</li>\n<li>Social network analysis</li>\n<li>Organize computing cluster</li>\n<li>Astronomical data analysis</li>\n</ul>\n<p><a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\"><strong>Related algorithms</strong></a>:</p>\n<ul>\n<li>K-Means Clustering</li>\n<li>Mean-Shift Clustering</li>\n<li>Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</li>\n<li>ExpectationMaximization (EM) Clustering using Gaussian Mixture Models (GMM)</li>\n</ul>\n<h2 id=\"2-Linear-Regression-with-One-Variable\"><a href=\"#2-Linear-Regression-with-One-Variable\" class=\"headerlink\" title=\"2 Linear Regression with One Variable\"></a>2 Linear Regression with One Variable</h2><h3 id=\"2-0-Model-Representation-Notation\"><a href=\"#2-0-Model-Representation-Notation\" class=\"headerlink\" title=\"2.0 Model Representation - Notation\"></a>2.0 Model Representation - Notation</h3><p>$m$: number of training examples</p>\n<p>$x$s: input variable/feature</p>\n<p>$y$s: output variable/feature</p>\n<p>$(x,y)$: a training example</p>\n<p>$(x^i,y^i)$: $i$ represents the $i^{th}$</p>\n<h3 id=\"2-1-Model-and-Cost-Function\"><a href=\"#2-1-Model-and-Cost-Function\" class=\"headerlink\" title=\"2.1 Model and Cost Function\"></a>2.1 Model and Cost Function</h3><p><strong>Hypothesis </strong>:<br>$$<br>h_(x)=_0+_1x<br>$$<br><strong>Parameters</strong>: $\\theta_0,\\theta_1$</p>\n<blockquote>\n<p>$\\theta_1$$\\theta_0$</p>\n</blockquote>\n<p><a href=\"https://www.cnblogs.com/geaozhang/p/11442343.html\"><strong>Cost Function </strong></a>:<br>$$<br>J(_0,_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2<br>$$</p>\n<blockquote>\n<p>minimize squared error cost function ()</p>\n</blockquote>\n<p><strong>Goal</strong>: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$</p>\n<h3 id=\"2-2-Parameter-Learning-Gradient-Descent\"><a href=\"#2-2-Parameter-Learning-Gradient-Descent\" class=\"headerlink\" title=\"2.2 Parameter Learning - Gradient Descent\"></a>2.2 Parameter Learning - Gradient Descent</h3><h4 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h4><ul>\n<li><p>Start with some $\\theta_0,\\theta_1$</p>\n</li>\n<li><p>Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum</p>\n</li>\n</ul>\n<h4 id=\"Gradient-descent-algorithm\"><a href=\"#Gradient-descent-algorithm\" class=\"headerlink\" title=\"Gradient descent algorithm\"></a>Gradient descent algorithm</h4><p>    Repeat until convergence {</p>\n<p>        $\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,\\theta_1)$    (for j=0 and j=1) </p>\n<p>}</p>\n<blockquote>\n<p>$:=$ colon equals, which used to denote assignment ()</p>\n<p>$\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent</p>\n<p>$\\frac{\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term ()</p>\n<p><strong>Assert</strong>: simultaneous update $\\theta_0,\\theta_1$ at the same time</p>\n</blockquote>\n<h4 id=\"Gradient-descent-intuition\"><a href=\"#Gradient-descent-intuition\" class=\"headerlink\" title=\"Gradient descent intuition\"></a>Gradient descent intuition</h4><p>$$<br>\\theta_1:=\\theta_1-\\alpha\\frac{\\theta_j}J(\\theta_1)<br>$$</p>\n<ol>\n<li><p>$\\alpha$</p>\n<p>if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.</p>\n<p>if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.</p>\n</li>\n<li><p>Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.</p>\n</li>\n<li><p>As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.</p>\n</li>\n</ol>\n<h4 id=\"Gradient-descent-for-linear-regression\"><a href=\"#Gradient-descent-for-linear-regression\" class=\"headerlink\" title=\"Gradient descent for linear regression\"></a>Gradient descent for linear regression</h4><blockquote>\n<p>Apply gradient descent to minimize squared error cost function </p>\n</blockquote>\n<p>$$\\frac{\\theta_j}J(\\theta_0,\\theta_1)=\\frac{\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$        <em>Expanding the formula</em></p>\n<p>Substituting 0 and 1 into $j$</p>\n<p>$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})$$</p>\n<p>$$\\theta_1:j=1:\\frac{\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}$$</p>\n<p>When work out the derivatives, which is the slope of the cost function <em>J</em>, plug them back in to gradient descent algorithm (remember to update simultaneously).</p>\n<p><strong>Convex function</strong></p>\n<blockquote>\n<p>It doesnt have any local optimum except for the global optimum</p>\n</blockquote>\n<p><strong>Batch Gradient Descent</strong></p>\n<blockquote>\n<p>Batch: Each step of gradient descent uses all the training examples (entire training set)</p>\n</blockquote>\n<h2 id=\"3-Linear-Algebra-review-optional\"><a href=\"#3-Linear-Algebra-review-optional\" class=\"headerlink\" title=\"3 Linear Algebra review (optional)\"></a>3 Linear Algebra review (optional)</h2><h3 id=\"3-1-Matrices-and-vectors\"><a href=\"#3-1-Matrices-and-vectors\" class=\"headerlink\" title=\"3.1 Matrices and vectors\"></a>3.1 Matrices and vectors</h3><h4 id=\"Matrix\"><a href=\"#Matrix\" class=\"headerlink\" title=\"Matrix\"></a>Matrix</h4><blockquote>\n<p>Rectangular array of numbers</p>\n</blockquote>\n<p>32 matrix: $\\begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\\ 5&amp;6\\end{bmatrix}$    23 matrix: $\\begin{bmatrix}1 &amp;2&amp;3 \\6&amp; 3 &amp; 4\\ \\end{bmatrix}$</p>\n<p><strong>Dimension of matrix</strong>: number of rows $\\times$ number of columns</p>\n<blockquote>\n<p>The above matrix can be also write as $\\mathbb R^{3\\times2}$ </p>\n</blockquote>\n<p>**Refer to specific elements of the matrix **(entries of matrix)</p>\n<p>$A=\\begin{bmatrix}1402&amp;191 \\ 1371 &amp;821\\ 949&amp;1437\\147&amp;1448\\end{bmatrix}$</p>\n<p>$A_{ij}=$ $i$,$j$ entry in the $i^{th}$ row, $j^{th}$ column.</p>\n<p>$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$</p>\n<h4 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h4><blockquote>\n<p>An $n\\times1$ matrix</p>\n</blockquote>\n<p>$y=\\begin{bmatrix}460\\232\\315\\178\\end{bmatrix}$</p>\n<p>$y_i=i^{th}$ element</p>\n<blockquote>\n<p>It is often customary to use uppercase letters for matrices and lowercase letters for vectors</p>\n</blockquote>\n<h3 id=\"3-2-Addition-and-Scalar-multiplication\"><a href=\"#3-2-Addition-and-Scalar-multiplication\" class=\"headerlink\" title=\"3.2 Addition and Scalar multiplication\"></a>3.2 Addition and Scalar multiplication</h3><h3 id=\"3-3-Matrix-vector-multiplication\"><a href=\"#3-3-Matrix-vector-multiplication\" class=\"headerlink\" title=\"3.3 Matrix-vector multiplication\"></a>3.3 Matrix-vector multiplication</h3><p><strong>Details</strong>:</p>\n<p>            $A$           $\\times$   $x$       $=$       $y$</p>\n<p>$\\begin{bmatrix}&amp;&amp;&amp;&amp;&amp;\\ \\ \\ \\end{bmatrix}\\times\\begin{bmatrix}\\ \\ \\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\ \\ \\ \\ \\end{bmatrix}$</p>\n<p>   m$\\times$n matrix        n$\\times$1    m-dimensional vector</p>\n<p>To get $y_i$, multiply $A$ $i^{th}$ row with elements of vector $x$, and add them up.</p>\n<p><strong>Calculation Tips</strong></p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Competing hypotheses: $h_\\theta(x)=-40+0.25x$</p>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\0.25\\end{bmatrix}$ </p>\n<h3 id=\"3-4-Matrix-matrix-multiplication\"><a href=\"#3-4-Matrix-matrix-multiplication\" class=\"headerlink\" title=\"3.4 Matrix-matrix multiplication\"></a>3.4 Matrix-matrix multiplication</h3><p><strong>Details</strong>:</p>\n<p>$A\\times B=C$</p>\n<p>[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o</p>\n<p>The $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,,0)</p>\n<p><strong>Example</strong>:</p>\n<p>$\\begin{bmatrix}1&amp;3\\2&amp;5\\end{bmatrix}\\begin{bmatrix}0&amp;1\\3&amp;2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&amp;1\\times1+3\\times2 \\2\\times0+5\\times3&amp;2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&amp;7\\15&amp;12\\end{bmatrix}$</p>\n<p><strong>Calculation Tips II</strong></p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Three competing hypotheses: </p>\n<ol>\n<li>$h_\\theta(x)=-40+0.25x$</li>\n<li>$h_\\theta(x)=200+0.1x$</li>\n<li>$h_\\theta(x)=-150+0.4x$</li>\n</ol>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40&amp;200&amp;-150 \\ 0.25&amp;0.1&amp;0.4 \\end{bmatrix}$</p>\n<h3 id=\"3-5-Matrix-multiplication-properties\"><a href=\"#3-5-Matrix-multiplication-properties\" class=\"headerlink\" title=\"3.5 Matrix multiplication properties\"></a>3.5 Matrix multiplication properties</h3><p>Let $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (<strong>Not commutative</strong>) </p>\n<h4 id=\"Identity-Matrix\"><a href=\"#Identity-Matrix\" class=\"headerlink\" title=\"Identity Matrix\"></a>Identity Matrix</h4><p>Denoted $I$ (or $I_{n\\times n}$).</p>\n<p>Example of identity matrices:</p>\n<p>2$\\times$2: $\\begin{bmatrix}1&amp;0 \\ 0&amp;1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&amp;0&amp;0 \\ 0&amp;1&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$      $\\cdots$</p>\n<p>For any matrix $A$,</p>\n<p>$$<br>A\\cdot I=I\\cdot A=A<br>$$</p>\n<blockquote>\n<p>Implicit conditions of the formula: </p>\n<p>$A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$</p>\n</blockquote>\n<h3 id=\"3-6-Inverse-and-Transpose\"><a href=\"#3-6-Inverse-and-Transpose\" class=\"headerlink\" title=\"3.6 Inverse and Transpose\"></a>3.6 Inverse and Transpose</h3><blockquote>\n<p></p>\n</blockquote>\n<p>Not all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&amp;0 \\ 0&amp;0\\end{bmatrix}$)</p>\n<blockquote>\n<p>Matrices that dont have an inverse are singular or degenerate.</p>\n</blockquote>\n<h4 id=\"Matrix-inverse\"><a href=\"#Matrix-inverse\" class=\"headerlink\" title=\"Matrix inverse\"></a>Matrix inverse</h4><p>If A is an m$\\times$m matrix, and if it has an inverse,<br>$$<br>AA^{-1}=A^{-1}A=I<br>$$</p>\n<blockquote>\n<p>An m$\\times$m matrix called a square matrix (), only square matrix has an inverse. </p>\n</blockquote>\n<h4 id=\"Matrix-transpose\"><a href=\"#Matrix-transpose\" class=\"headerlink\" title=\"Matrix transpose\"></a>Matrix transpose</h4><p>Example:<br>$$<br>A=\\begin{bmatrix}1&amp;2&amp;0 \\ 3&amp;5&amp;9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&amp;3 \\ 2&amp;5 \\ 0&amp;9 \\end{bmatrix}<br>$$<br>Let $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.</p>\n<h2 id=\"4-Linear-Regression-with-Multiple-Variables\"><a href=\"#4-Linear-Regression-with-Multiple-Variables\" class=\"headerlink\" title=\"4 Linear Regression with Multiple Variables\"></a>4 Linear Regression with Multiple Variables</h2><h3 id=\"4-1-Multiple-feature\"><a href=\"#4-1-Multiple-feature\" class=\"headerlink\" title=\"4.1 Multiple feature\"></a>4.1 Multiple feature</h3><h4 id=\"Notation\"><a href=\"#Notation\" class=\"headerlink\" title=\"Notation\"></a>Notation</h4><p>$n$ = number of features</p>\n<p>$x^{(i)}$ = input (features) of $i^{th}$ training example</p>\n<p>$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example</p>\n<h4 id=\"Multivariate-linear-regression\"><a href=\"#Multivariate-linear-regression\" class=\"headerlink\" title=\"Multivariate linear regression\"></a>Multivariate linear regression</h4><blockquote>\n<p></p>\n</blockquote>\n<p><strong>Hypothesis</strong>:</p>\n<p>$h_(x)=_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>For convenience of notation, define $x_0=1$. Then</p>\n<p>$h_(x)=_0x_0+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>          $=\\theta^Tx$</p>\n<p>          $=\\begin{bmatrix}\\theta_0&amp;\\theta_1&amp;\\cdots&amp;\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix}$</p>\n<h3 id=\"4-2-Gradient-descent-for-multiple-variables\"><a href=\"#4-2-Gradient-descent-for-multiple-variables\" class=\"headerlink\" title=\"4.2 Gradient descent for multiple variables\"></a>4.2 Gradient descent for multiple variables</h3><ul>\n<li><p></p>\n</li>\n<li><p></p>\n</li>\n</ul>\n<p><strong>Hypothesis</strong>: $h_(x)=\\theta^Tx=_0x_1+_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p><strong>Parameters</strong>: $\\theta_0$,$\\theta_1$,,$\\theta_n$</p>\n<p><strong>Cost function</strong>: $J(\\theta_0$,$\\theta_1$,,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2$</p>\n<p><strong>Gradient descent</strong>:</p>\n<p>    Repeat {</p>\n<p>        $\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta_0,,\\theta_n)$        </p>\n<p>}        (simultaneously update for every $j=0,,n$)</p>\n<blockquote>\n<p>$J(\\theta_0,,\\theta_n)$ can be instead by $J(\\theta)$</p>\n</blockquote>\n<p><strong>New algorithm</strong> for $n\\ge1$:</p>\n<p>    Repeat {</p>\n<p>        $\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>}        (simultaneously update for $\\theta_j$ for $j=0,,n$)</p>\n<blockquote>\n<p>Previously $(n=1)$ is in 2.2 </p>\n</blockquote>\n<h3 id=\"4-3-Gradient-descent-in-practice-I-Feature-Scaling\"><a href=\"#4-3-Gradient-descent-in-practice-I-Feature-Scaling\" class=\"headerlink\" title=\"4.3 Gradient descent in practice I: Feature Scaling\"></a>4.3 Gradient descent in practice I: Feature Scaling</h3><blockquote>\n<p></p>\n</blockquote>\n<h4 id=\"Feature-Scaling\"><a href=\"#Feature-Scaling\" class=\"headerlink\" title=\"Feature Scaling\"></a>Feature Scaling</h4><blockquote>\n<p><strong>Idea</strong>: Make sure flatten are on a similar scale.</p>\n</blockquote>\n<p>E.g. $x_1$= size(0-2000 $feet^2$)</p>\n<p>      $x_2$= number of bedrooms (1-5)</p>\n<p>It will be better to limit both $x_1$ and $x_2$ in [0,1]</p>\n<p>$x_1=\\frac{size(feet^2)}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom}{5}$</p>\n<blockquote>\n<p></p>\n</blockquote>\n<p>More general, get every feature into approximately a $-1\\le x_i\\le1$ range.</p>\n<blockquote>\n<p>-11</p>\n</blockquote>\n<h4 id=\"Mean-normalization\"><a href=\"#Mean-normalization\" class=\"headerlink\" title=\"Mean normalization\"></a>Mean normalization</h4><p>Replace $x_i$ with $x_i-_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)<br><strong>E.g.</strong> </p>\n<p>$x_1=\\frac{size(feet^2)-1000}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom-2}{5}$</p>\n<blockquote>\n<p>$_i$ (1000 and 2) is considered as the average value of $x_i$ in training set</p>\n</blockquote>\n<p>$$<br>x_i\\leftarrow \\frac{x_i-_i}{range(max-min)}<br>$$</p>\n<h3 id=\"4-4-Gradient-descent-in-practice-II-Learning-rate\"><a href=\"#4-4-Gradient-descent-in-practice-II-Learning-rate\" class=\"headerlink\" title=\"4.4 Gradient descent in practice II: Learning rate\"></a>4.4 Gradient descent in practice II: Learning rate</h3><ul>\n<li>The chapter will center around the learning rate $\\alpha$</li>\n</ul>\n<p><strong>Gradient descent</strong></p>\n<ul>\n<li>$\\theta_j:=\\theta_j-\\alpha \\frac{\\theta_j}J(\\theta)$</li>\n<li>Debugging: How to make sure gradient descent is working correctly.</li>\n<li>How to choose learning rate $\\alpha$</li>\n</ul>\n<p>Declare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.</p>\n<blockquote>\n<p>$min_\\theta J(\\theta)$$y=|\\frac 1x|$$(\\epsilon)$threshold$\\epsilon$</p>\n</blockquote>\n<blockquote>\n<p>$\\alpha$0$\\alpha$</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<ul>\n<li>If $\\alpha$ is too small: slow convergence.</li>\n<li>If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.</li>\n</ul>\n<p>Recommended choices for $\\alpha$:</p>\n<p>, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, </p>\n<h3 id=\"4-5-Features-and-polynomial-regression\"><a href=\"#4-5-Features-and-polynomial-regression\" class=\"headerlink\" title=\"4.5 Features and polynomial regression\"></a>4.5 Features and polynomial regression</h3><blockquote>\n<p></p>\n</blockquote>\n<p>Housing prices prediction:</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth<br>$$</p>\n<blockquote>\n<p>Its better to use $area$ which is equal to $frontage\\times depth$ as new feature.</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times area<br>$$</p>\n<h4 id=\"Choice-of-feature\"><a href=\"#Choice-of-feature\" class=\"headerlink\" title=\"Choice of feature\"></a>Choice of feature</h4><p>Suppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2<br>$$</p>\n<blockquote>\n<p>-</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3<br>$$</p>\n<blockquote>\n<p>$(size,(size)^2,(size)^3)$</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}<br>$$</p>\n<blockquote>\n<p>xarea</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<p>You have a choice in what features to use to fix more complex functions to your data!</p>\n<h3 id=\"4-6-Normal-equation-unfinished\"><a href=\"#4-6-Normal-equation-unfinished\" class=\"headerlink\" title=\"4.6 Normal equation (unfinished)\"></a>4.6 Normal equation (unfinished)</h3><blockquote>\n<p>$\\theta$$\\theta$</p>\n</blockquote>\n<p><strong>Normal equation</strong>: Method to solve for $\\theta$ analytically.</p>\n<h4 id=\"Compare-to-Gradient-Descent\"><a href=\"#Compare-to-Gradient-Descent\" class=\"headerlink\" title=\"Compare to Gradient Descent\"></a>Compare to Gradient Descent</h4><p>$m$ training example, $n$ features.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Descent</th>\n<th>Normal Equation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Need to choice a $\\alpha$</td>\n<td>No need to choice a $\\alpha$</td>\n</tr>\n<tr>\n<td>Needs many iterations</td>\n<td>Dont need to iterate</td>\n</tr>\n<tr>\n<td>Work well even $n$ is large</td>\n<td>Need to compute $(X^TX)^{-1}$</td>\n</tr>\n<tr>\n<td></td>\n<td>Slow if $n$ is very large</td>\n</tr>\n</tbody></table>\n<p>$$<br>\\theta=(X^TX)^{-1}X^Ty<br>$$<br>$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$</p>\n<p><strong>Octave</strong>: <code>pinv(X&#39;*X)*X&#39;*y</code> </p>\n<blockquote>\n<p><code>X&#39;</code> is the transpose of $X$</p>\n<p><code>pinv</code> is a function  to compute the inverse of a matrix</p>\n</blockquote>\n<!--unfinished-->\n\n<h3 id=\"4-7-Normal-equation-and-non-invertibility-optional-unfinished\"><a href=\"#4-7-Normal-equation-and-non-invertibility-optional-unfinished\" class=\"headerlink\" title=\"4.7 Normal equation and non-invertibility (optional) (unfinished)\"></a>4.7 Normal equation and non-invertibility (optional) (unfinished)</h3><!--unfinished-->\n\n\n\n<h2 id=\"5-Octave-Tutorial-ignored\"><a href=\"#5-Octave-Tutorial-ignored\" class=\"headerlink\" title=\"5 Octave Tutorial (ignored)\"></a>5 Octave Tutorial (ignored)</h2><!--unfinished-->\n\n\n\n<h2 id=\"6-Logistic-Regression\"><a href=\"#6-Logistic-Regression\" class=\"headerlink\" title=\"6 Logistic Regression\"></a>6 Logistic Regression</h2><blockquote>\n<p></p>\n</blockquote>\n<h3 id=\"6-1-Classification\"><a href=\"#6-1-Classification\" class=\"headerlink\" title=\"6.1 Classification\"></a>6.1 Classification</h3><p><strong>Classification</strong></p>\n<p>$y\\in {0,1} $</p>\n<p>0: Negative Class (e.g., benign tumor)</p>\n<p>1: Positive Class (e.g., malignant tumor)</p>\n<blockquote>\n<p>There are multi-class problems as well that y can take value from 0, 1, 2, 3,</p>\n</blockquote>\n<p>Learning regression isnt fit the classification problem</p>\n<p>In the <a href=\"https://www.bilibili.com/video/BV164411b7dx?p=32&t=160\">video</a> there is an example to explain it. </p>\n<p>Another example:</p>\n<p>Classification: y = 0 or 1</p>\n<p>    $H_\\theta(x)$ can be $&gt;1$ or $&lt;0$ if we use the linear regression</p>\n<blockquote>\n<p>Obviously, the label is either 0 or 1.</p>\n</blockquote>\n<p>Logistic Regression: $0\\le h_\\theta(x)\\le1$</p>\n<blockquote>\n<p>This is a classification algorithm whose output always between 1 and 0. Besides, its a classification algorithm instead of linear regression algorithm though there is a regression in its name.</p>\n</blockquote>\n<h3 id=\"6-2-Hypothesis-Representation\"><a href=\"#6-2-Hypothesis-Representation\" class=\"headerlink\" title=\"6.2 Hypothesis Representation\"></a>6.2 Hypothesis Representation</h3><blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>What is the function were going to use to representation hypothesis when we have a classification problem.</li>\n</ul>\n<h4 id=\"Logistic-Regression-Model\"><a href=\"#Logistic-Regression-Model\" class=\"headerlink\" title=\"Logistic Regression Model\"></a>Logistic Regression Model</h4><p>    Want $0\\le h_\\theta(x)\\le1$</p>\n<p>$$<br>h_\\theta(x)=g(\\theta^Tx)<br>$$<br><strong>Sigmoid function (Logistic function)</strong>:<br>$$<br>g(z)=\\frac1{1+e^{-z}}<br>$$<br><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" /></p>\n<blockquote>\n<p>Its graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$</p>\n</blockquote>\n<p>Thus,<br>$$<br>h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}<br>$$</p>\n<blockquote>\n<p>$\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx&lt;0$ then  $h_\\theta(x)=0$</p>\n</blockquote>\n<p><strong>Interpretation of Hypothesis Output</strong></p>\n<p>$h_\\theta(x)=$ estimated probability that y = 1 on input x</p>\n<p>Example: if $x=\\begin{bmatrix}x_0 \\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\ tumorSize\\end{bmatrix}$</p>\n<p>                $h_\\theta(x)=0.7$</p>\n<p>Tell patient that 70% chance of tumor being malignant.</p>\n<h4 id=\"Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\"><a href=\"#Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\" class=\"headerlink\" title=\"Mathematical formula definition of the hypothesis for logistic regression\"></a>Mathematical formula definition of the hypothesis for logistic regression</h4><p>Probability that y=1, given x, parameterized by $\\theta$:<br>$$<br>P(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\<br>P(y=0|x;\\theta)=1-P(y=1|x;\\theta)<br>$$</p>\n<h3 id=\"6-3-Decision-boundary\"><a href=\"#6-3-Decision-boundary\" class=\"headerlink\" title=\"6.3 Decision boundary\"></a>6.3 Decision boundary</h3><blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>What logistic regression hypothesis function is computing?</li>\n</ul>\n<p>According to Logistic regression, </p>\n<p>suppose predict $y=1$ If $h_\\theta(x)\\ge0.5$</p>\n<p>predict $y=0$ If $h_\\theta(x)\\le0.5$</p>\n<h4 id=\"Decision-Boundary\"><a href=\"#Decision-Boundary\" class=\"headerlink\" title=\"Decision Boundary\"></a>Decision Boundary</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\n<p>Suppose the variable procedure to be specified. </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$</p>\n<p>And, $\\theta=\\begin{bmatrix}-3 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Predict $y=1$if $-3+x_1+x_2\\ge0$</p>\n<p>                                     $x_1+x_2\\ge3$</p>\n<p>The magenta line is called <strong>Decision Boundary</strong>.</p>\n<blockquote>\n<p>The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.</p>\n</blockquote>\n<h4 id=\"Non-linear-decision-boundaries\"><a href=\"#Non-linear-decision-boundaries\" class=\"headerlink\" title=\"Non-linear decision boundaries\"></a>Non-linear decision boundaries</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\n<p>Assuming hypothesis likes this: </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$</p>\n<p>And assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\ 0 \\ 0 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Then, predict $y=1$ if $-1+x_1^2+x_2^2\\ge0$</p>\n<p>                                               $x_1^2+x_2^2\\ge1$</p>\n<blockquote>\n<p>The training set used to fit the parameters $\\theta$</p>\n</blockquote>\n<h3 id=\"6-4-Cost-function\"><a href=\"#6-4-Cost-function\" class=\"headerlink\" title=\"6.4 Cost function\"></a>6.4 Cost function</h3><ul>\n<li><p>How to automatically choose the parameters $\\theta$ to a training set.</p>\n</li>\n<li><p>Define the optimization objective or the cost function that used to fit the parameters.</p>\n</li>\n</ul>\n<p>Here is to supervised learning problem of fitting a logistic regression model.</p>\n<p>Training set: ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})}$</p>\n<p>$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in{0,1}$</p>\n<p>$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<p>How to choose parameters $\\theta$ ? (The next sections will focus on this problem)</p>\n<h4 id=\"Cost-function-Logistic-regression-cost-function\"><a href=\"#Cost-function-Logistic-regression-cost-function\" class=\"headerlink\" title=\"Cost function - Logistic regression cost function\"></a>Cost function - Logistic regression cost function</h4><p>    Linear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_(x^{(i)})-y^{(i)})^2$</p>\n<p>    $Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$</p>\n<blockquote>\n<p>$\\theta$non-convex function$\\frac1{1+e^{-\\theta^T x}}$</p>\n</blockquote>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n<p><strong>Logistic regression cost function</strong><br>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases}<br>$$</p>\n<p>If y = 1</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\n<p>If y = 0</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n<h3 id=\"6-5-simplified-cost-function-and-gradient-descent\"><a href=\"#6-5-simplified-cost-function-and-gradient-descent\" class=\"headerlink\" title=\"6.5 simplified cost function and gradient descent\"></a>6.5 simplified cost function and gradient descent</h3><ul>\n<li><p>Figure out a simpler way to write the cost function</p>\n</li>\n<li><p>Also figure out how to apply gradient descent to fit the parameters of logistic regression</p>\n</li>\n</ul>\n<h4 id=\"Logistic-regression-cost-function\"><a href=\"#Logistic-regression-cost-function\" class=\"headerlink\" title=\"Logistic regression cost function\"></a>Logistic regression cost function</h4><p>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})<br>$$</p>\n<p>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases} \\ ;\\<br>Note:y=0;or;1;always<br>$$</p>\n<p>Compress them into one equation:</p>\n<p>$$<br>Cost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))<br>$$</p>\n<p><strong>Logistic regression cost function</strong><br>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_(x^{(i)})-y^{(i)})\\<br>\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad;;;<br>=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]<br>$$</p>\n<blockquote>\n<p>the principle maximum likelihood estimation</p>\n</blockquote>\n<p>To fit parameters $\\theta$:<br>$$<br>\\min_\\theta J(\\theta)<br>$$</p>\n<blockquote>\n<p>find the $\\theta$ which minimizes $J(\\theta)$</p>\n</blockquote>\n<p>To make prediction given new $x$:</p>\n<p>    Output $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<blockquote>\n<p>So how to minimize $J(\\theta)$, or how to choose parameter $\\theta$ ?</p>\n</blockquote>\n<h4 id=\"Implementation-of-logistic-regression\"><a href=\"#Implementation-of-logistic-regression\" class=\"headerlink\" title=\"Implementation of logistic regression\"></a>Implementation of logistic regression</h4><p><strong>Gradient Descent</strong></p>\n<p>$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$</p>\n<p>What $\\min_\\theta J(\\theta)$:    </p>\n<p>    Repeat {</p>\n<p>                $\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>        }        (simultaneously update for $\\theta_j$ for $j=0,,n$)</p>\n<blockquote>\n<p>Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.</p>\n</blockquote>\n<h3 id=\"6-6-Advanced-optimization\"><a href=\"#6-6-Advanced-optimization\" class=\"headerlink\" title=\"6.6 Advanced optimization\"></a>6.6 Advanced optimization</h3><ul>\n<li>Some advanced optimization algorithms </li>\n<li>Some advanced optimization concepts</li>\n</ul>\n<blockquote>\n<p>:see_no_evil:</p>\n</blockquote>\n<h4 id=\"Optimization-algorithm\"><a href=\"#Optimization-algorithm\" class=\"headerlink\" title=\"Optimization algorithm\"></a>Optimization algorithm</h4><p>Cost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.</p>\n<p>Given $\\theta$, we have code that compute</p>\n<ul>\n<li>$J(\\theta)$</li>\n<li>$\\frac{\\theta_j}J(\\theta)$      (For $j=0,1,,n$)</li>\n</ul>\n<p>Optimization algorithms:</p>\n<ul>\n<li>Gradient descent</li>\n<li>Conjugate gradient</li>\n<li>BFGS ()</li>\n<li>L-BFGS</li>\n</ul>\n<p>Others algorithm compare to gradient descent</p>\n<table>\n<thead>\n<tr>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No need to manually pick $\\alpha$</td>\n<td>More complex</td>\n</tr>\n<tr>\n<td>Often faster than gradient descent</td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>These complex algorithms have a clever inner-loop called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.</p>\n<p>:older_man:</p>\n</blockquote>\n<h3 id=\"6-7-Multi-class-classification-One-vs-all-unfinished\"><a href=\"#6-7-Multi-class-classification-One-vs-all-unfinished\" class=\"headerlink\" title=\"6.7 Multi-class classification: One-vs-all (unfinished)\"></a>6.7 Multi-class classification: One-vs-all (unfinished)</h3><ul>\n<li>How to get logistic regression to work for multi-class classification problems</li>\n<li>One-versus-all classification algorithm</li>\n</ul>\n<h4 id=\"Multiclass-classification\"><a href=\"#Multiclass-classification\" class=\"headerlink\" title=\"Multiclass classification\"></a>Multiclass classification</h4><!--unfinished-->\n\n\n\n<h2 id=\"7-Regularization-unfinished\"><a href=\"#7-Regularization-unfinished\" class=\"headerlink\" title=\"7 Regularization (unfinished)\"></a>7 Regularization (unfinished)</h2><blockquote>\n<p></p>\n</blockquote>\n<h3 id=\"7-1-The-problem-of-overfitting\"><a href=\"#7-1-The-problem-of-overfitting\" class=\"headerlink\" title=\"7.1 The problem of overfitting\"></a>7.1 The problem of overfitting</h3><ul>\n<li>Explain what is overfitting problem</li>\n</ul>\n<p>Example: Linear regression (housing prices)</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>UnderfitHigh bias,</p>\n</blockquote>\n<blockquote>\n<p>OverfitHigh variance,</p>\n</blockquote>\n<h4 id=\"Overfitting\"><a href=\"#Overfitting\" class=\"headerlink\" title=\"Overfitting\"></a>Overfitting</h4><p>If we have too many features, the learned hypothesis may fit the training set very well ($J()=\\frac {1}{2m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).</p>\n<p>Example: Logistic regression</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n<h4 id=\"Addressing-overfitting\"><a href=\"#Addressing-overfitting\" class=\"headerlink\" title=\"Addressing overfitting\"></a>Addressing overfitting</h4><p>If we have la lot of features, and very little training data, then overfitting can become a problem.</p>\n<p>Two main options:</p>\n<ol>\n<li>Reduce number of features<ul>\n<li>Manually select which features to keep (but some feature must be abandoned)</li>\n<li>Model selection algorithm (later will explain)</li>\n</ul>\n</li>\n<li>Regularization<ul>\n<li>Keep all the features, but reduce magnitude/values of parameters $\\theta_j$</li>\n<li>Works well when we have a lot of features, each of which contributes a bit to prediction $y$.</li>\n</ul>\n</li>\n</ol>\n<!--unfinished-->\n\n\n\n<h2 id=\"8-Neural-Networks-Representation\"><a href=\"#8-Neural-Networks-Representation\" class=\"headerlink\" title=\"8 Neural Networks: Representation\"></a>8 Neural Networks: Representation</h2><h3 id=\"8-1-Non-linear-hypothesis\"><a href=\"#8-1-Non-linear-hypothesis\" class=\"headerlink\" title=\"8.1 Non-linear hypothesis\"></a>8.1 Non-linear hypothesis</h3><p>Why we need Neural Networks?</p>\n<blockquote>\n<p></p>\n</blockquote>\n<p>The neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.</p>\n<h3 id=\"8-2-Neurons-and-the-brain\"><a href=\"#8-2-Neurons-and-the-brain\" class=\"headerlink\" title=\"8.2 Neurons and the brain\"></a>8.2 Neurons and the brain</h3><p><strong>History of  neural networks</strong></p>\n<ul>\n<li>Origins: Algorithms that try to mimic the brain.</li>\n<li>WAs very widely used in 80s and early 90s; popularity diminished in late 90s.</li>\n<li>Recent resurgence: State-of-the-art technique for many applications.</li>\n</ul>\n<p><strong>The one learning algorithm hypothesis</strong></p>\n<ul>\n<li>Neuro-rewiring experiments</li>\n<li>Sensor representations in the brain</li>\n</ul>\n<h3 id=\"8-3-Model-representation-I\"><a href=\"#8-3-Model-representation-I\" class=\"headerlink\" title=\"8.3 Model representation I\"></a>8.3 Model representation I</h3><ul>\n<li>How we represent  Neural Networks (hypothesis or model).</li>\n</ul>\n<h4 id=\"Neuron-model-Logistic-unit\"><a href=\"#Neuron-model-Logistic-unit\" class=\"headerlink\" title=\"Neuron model: Logistic unit\"></a>Neuron model: Logistic unit</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n<p><strong>Sigmoid (logistic) activation function</strong></p>\n<blockquote>\n<p>(activation function)$g(z)=\\frac1{1+e^{-z}}$</p>\n</blockquote>\n<p>Sometimes we add an extra $x_0$ node (if necessary) called bias unit () or the bias neuron (). Its always equal to 1 so sometime we dont draw it.</p>\n<p>In the neural networks literature, the parameters of model $\\theta$ is also called <strong>weights of a model</strong>.</p>\n<h4 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\n<p>Layer 1: Input layer</p>\n<p>Layer 2 : Hidden layer</p>\n<p>Layer 3: Output layer</p>\n<p>$a_i^{(j)}=$ activation of unit $i$ in layer $j$.</p>\n<p>$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$</p>\n<blockquote>\n<p>$\\Theta^{(j)}$matrix of weights</p>\n</blockquote>\n<p>$$<br>a_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\<br>a_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\<br>a_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\<br>h_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})<br>$$</p>\n<p>If networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.</p>\n<blockquote>\n<p>The superscript $j$ in parentheses means that these values associated with layer $j$</p>\n</blockquote>\n<h3 id=\"8-4-Model-representation-II\"><a href=\"#8-4-Model-representation-II\" class=\"headerlink\" title=\"8.4 Model representation II\"></a>8.4 Model representation II</h3><ul>\n<li>How to carry out computation efficiently and show a vectorized implementation.</li>\n<li>Intuition about why these neural network representation</li>\n</ul>\n<h4 id=\"Forward-propagation-Vectorized-implementation\"><a href=\"#Forward-propagation-Vectorized-implementation\" class=\"headerlink\" title=\"Forward propagation: Vectorized implementation\"></a>Forward propagation: Vectorized implementation</h4><p><strong>Define:</strong></p>\n<p>$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$</p>\n<p>Thus,</p>\n<p>$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$</p>\n<p>We observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.</p>\n<p><strong>Define:</strong><br>$$<br>x=\\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)}\\end{bmatrix}<br>$$<br>Further towards vectorization:<br>$$<br>z^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\<br>a^{(2)}=g(z^{(2)})<br>$$</p>\n<blockquote>\n<p>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.</p>\n</blockquote>\n<p>Next, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$</p>\n<p>$z^{(3)}=\\Theta^{(2)}a^{(2)}$</p>\n<p>$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$</p>\n<blockquote>\n<p>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.</p>\n</blockquote>\n<p>This process of computing $h(x)$ is also called <strong>forward propagation</strong>, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.</p>\n<h4 id=\"Neural-Network-learning-its-own-features\"><a href=\"#Neural-Network-learning-its-own-features\" class=\"headerlink\" title=\"Neural Network learning its own features\"></a>Neural Network learning its own features</h4><blockquote>\n<p>$a_1^{(2)},a_2^{(2)},a_3^{(2)}$$x_1,x_2,x_3$ </p>\n</blockquote>\n<h4 id=\"Other-network-architectures\"><a href=\"#Other-network-architectures\" class=\"headerlink\" title=\"Other network architectures\"></a>Other network architectures</h4><p>The way that neural networks are connected are called the architecture (). So the architecture refers to how different neurons are connected to each other.</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n<h3 id=\"8-5-Examples-and-intoitions-unfinished\"><a href=\"#8-5-Examples-and-intoitions-unfinished\" class=\"headerlink\" title=\"8.5 Examples and intoitions (unfinished)\"></a>8.5 Examples and intoitions (unfinished)</h3><ul>\n<li><p>A detail example which shows how a neural network can compute a complex nonlinear function of the input</p>\n</li>\n<li><p>Why neural network can be used to learn complex nonlinear hypothesis</p>\n</li>\n</ul>\n<h2 id=\"9-Neural-Networks-Learning-unfinished\"><a href=\"#9-Neural-Networks-Learning-unfinished\" class=\"headerlink\" title=\"9 Neural Networks: Learning (unfinished)\"></a>9 Neural Networks: Learning (unfinished)</h2><h2 id=\"10-Advice-for-applying-machine-learning-unfinished\"><a href=\"#10-Advice-for-applying-machine-learning-unfinished\" class=\"headerlink\" title=\"10 Advice for applying machine learning (unfinished)\"></a>10 Advice for applying machine learning (unfinished)</h2><p>10.1 Decide what to try next</p>\n<p>10.2 Evaluating a hypothesis</p>\n<p>10.3 Model selection and training/validation/test sets</p>\n<h2 id=\"11-Machine-Learning-system-design-unfinished\"><a href=\"#11-Machine-Learning-system-design-unfinished\" class=\"headerlink\" title=\"11 Machine Learning system design (unfinished)\"></a>11 Machine Learning system design (unfinished)</h2><p>11.1 Prioritizing what to work on: Spam classification example</p>\n<h2 id=\"12-Support-Vector-Machines-unfinished\"><a href=\"#12-Support-Vector-Machines-unfinished\" class=\"headerlink\" title=\"12 Support Vector Machines (unfinished)\"></a>12 Support Vector Machines (unfinished)</h2><p>12.1 Optimizaion object</p>\n<ul>\n<li>Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions</li>\n</ul>\n<h2 id=\"13-Clustering\"><a href=\"#13-Clustering\" class=\"headerlink\" title=\"13 Clustering\"></a>13 Clustering</h2><h3 id=\"13-0-Notation\"><a href=\"#13-0-Notation\" class=\"headerlink\" title=\"13.0 Notation\"></a>13.0 Notation</h3><p>Train set: ${x^{(1)},x^{(2)},x^{(3)},x^{(m)}}$  (without labels)</p>\n<h3 id=\"13-1-K-means-algorithm\"><a href=\"#13-1-K-means-algorithm\" class=\"headerlink\" title=\"13.1 K-means algorithm\"></a>13.1 K-means algorithm</h3><blockquote>\n<p>K</p>\n</blockquote>\n<p>K-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.</p>\n<p><strong>Input</strong>:</p>\n<ul>\n<li>$K$ (number of clusters)</li>\n<li>Training set ${x^{(1)},x^{(2)},x^{(3)},x^{(m)}}$</li>\n</ul>\n<p>$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)</p>\n<h4 id=\"K-means-algorithm\"><a href=\"#K-means-algorithm\" class=\"headerlink\" title=\"K-means algorithm\"></a>K-means algorithm</h4><p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {<br>    // <u>cluster assignment step</u></p>\n<p>    for $i$ = 1 to $m$</p>\n<p>        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)</p>\n<p>    // <u>move centroids step</u></p>\n<p>    for $k$ = 1 to $K$</p>\n<p>        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<blockquote>\n<p>$K$ means the number of centroids and the $k$ means the index of each centriod.</p>\n<p>If you have a cluster with no points assigned to it, the usual practice is to delete it.</p>\n</blockquote>\n<h3 id=\"13-2-Optimization-objective\"><a href=\"#13-2-Optimization-objective\" class=\"headerlink\" title=\"13.2 Optimization objective\"></a>13.2 Optimization objective</h3><ul>\n<li>How we can use it to help K-means algorithm find better clusters and avoid local optima.</li>\n</ul>\n<h4 id=\"K-means-optimization-objective\"><a href=\"#K-means-optimization-objective\" class=\"headerlink\" title=\"K-means optimization objective\"></a>K-means optimization objective</h4><p>$c^{(i)}$ = index of cluster (1,2,,$K$) to which example $x^{(i)}$ is currently assigned</p>\n<p>$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)</p>\n<p>$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p>\n<h4 id=\"Optimization-objective\"><a href=\"#Optimization-objective\" class=\"headerlink\" title=\"Optimization objective:\"></a>Optimization objective:</h4><p>$$<br>J(c^{(1)}, c^{(m)},\\mu_1,,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\<br>\\min_{c^{(1)}, c^{(m)},\\mu_1,,\\mu_K} J(c^{(1)}, c^{(m)},\\mu_1,,\\mu_K)<br>$$</p>\n<blockquote>\n<p>The cost function $J$ is also called discotion function</p>\n</blockquote>\n<p><strong>Review K-means algorithm</strong></p>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {</p>\n<p>    //minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed</p>\n<p>    for $i$ = 1 to $m$</p>\n<p>        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$</p>\n<p>    //minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed</p>\n<p>    for $k$ = 1 to $K$</p>\n<p>        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<h3 id=\"13-3-Randomly-initialization\"><a href=\"#13-3-Randomly-initialization\" class=\"headerlink\" title=\"13.3 Randomly initialization\"></a>13.3 Randomly initialization</h3><ul>\n<li>How to initialize K-means</li>\n<li>How to avoid local optima</li>\n</ul>\n<blockquote>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,,\\mu_K \\in \\mathbb R^n$</p>\n</blockquote>\n<p><strong>Randomly initialization</strong></p>\n<p>Usually, we have $K&lt;m$</p>\n<p>Randomsly pick $K$ training examples.</p>\n<p>Set $\\mu_1,,\\mu_K$ equal to these $K$ examples.</p>\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>Local optima we should avoid, but randomly initialization may cause this situation.</p>\n</blockquote>\n<blockquote>\n<p></p>\n</blockquote>\n<h3 id=\"13-4-Choosing-the-number-of-cluster\"><a href=\"#13-4-Choosing-the-number-of-cluster\" class=\"headerlink\" title=\"13.4 Choosing the number of cluster\"></a>13.4 Choosing the number of cluster</h3><ul>\n<li>Manual</li>\n<li>Elbow method</li>\n<li>Later downstream purpose</li>\n</ul>\n<h2 id=\"14-Dimensionaliy-Reduction-unfinished\"><a href=\"#14-Dimensionaliy-Reduction-unfinished\" class=\"headerlink\" title=\"14 Dimensionaliy Reduction (unfinished)\"></a>14 Dimensionaliy Reduction (unfinished)</h2><p>14.1 Motivation I: Data Compression</p>\n<p>14.2 Motivation II: Visualization</p>\n<p>14.3 Principle Component Analysis problem formulation (PCA)</p>\n<blockquote>\n<p></p>\n</blockquote>\n<ul>\n<li>Compression algorithm</li>\n</ul>\n<p>14.4 Principle Component Analysis algorithm</p>\n<p>Data preprocessing</p>\n<h2 id=\"15-Anomaly-Detection-unfinished\"><a href=\"#15-Anomaly-Detection-unfinished\" class=\"headerlink\" title=\"15 Anomaly Detection (unfinished)\"></a>15 Anomaly Detection (unfinished)</h2><p>15.1 Problem motivation</p>\n<h2 id=\"16-Recommeder-Systems-unfinished\"><a href=\"#16-Recommeder-Systems-unfinished\" class=\"headerlink\" title=\"16 Recommeder Systems (unfinished)\"></a>16 Recommeder Systems (unfinished)</h2><h2 id=\"17-Large-Scale-Machine-Learning-unfinished\"><a href=\"#17-Large-Scale-Machine-Learning-unfinished\" class=\"headerlink\" title=\"17 Large Scale Machine Learning (unfinished)\"></a>17 Large Scale Machine Learning (unfinished)</h2><h3 id=\"17-1-Learning-with-large-datasets\"><a href=\"#17-1-Learning-with-large-datasets\" class=\"headerlink\" title=\"17.1 Learning with large datasets\"></a>17.1 Learning with large datasets</h3><p>$$<br>\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p>\n<blockquote>\n<p>    </p>\n</blockquote>\n<h3 id=\"17-2-Stochastic-gradient-descent\"><a href=\"#17-2-Stochastic-gradient-descent\" class=\"headerlink\" title=\"17.2 Stochastic gradient descent\"></a>17.2 Stochastic gradient descent</h3><h4 id=\"Review-linear-regression-with-gradient-descent\"><a href=\"#Review-linear-regression-with-gradient-descent\" class=\"headerlink\" title=\"Review linear regression with gradient descent\"></a>Review linear regression with gradient descent</h4><!-- unfinished -->\n\n<h4 id=\"Batch-gradient-descent\"><a href=\"#Batch-gradient-descent\" class=\"headerlink\" title=\"Batch gradient descent\"></a>Batch gradient descent</h4><h4 id=\"Stochastic-gradient-descent\"><a href=\"#Stochastic-gradient-descent\" class=\"headerlink\" title=\"Stochastic gradient descent\"></a>Stochastic gradient descent</h4><h3 id=\"17-3-Mini-batch-gradient-descent\"><a href=\"#17-3-Mini-batch-gradient-descent\" class=\"headerlink\" title=\"17.3 Mini-batch gradient descent\"></a>17.3 Mini-batch gradient descent</h3><h2 id=\"Markdown\"><a href=\"#Markdown\" class=\"headerlink\" title=\"Markdown\"></a>Markdown</h2><p>\\begin{bmatrix}\\end{bmatrix}</p>\n<p>\\mathbb R</p>\n<p><a href=\"https://www.jianshu.com/p/25f0139637b7\"></a></p>\n<p><a href=\"https://www.jianshu.com/p/e74eb43960a1\">2</a></p>\n<p><a href=\"https://www.jianshu.com/p/191d1e21f7ed\"></a></p>\n"},{"title":"first test","date":"2021-07-01T06:49:39.000Z","_content":"","source":"_posts/first-test.md","raw":"---\ntitle: first test\ndate: 2021-07-01 14:49:39\ntags:\n---\n","slug":"first-test","published":1,"updated":"2021-07-01T06:49:39.866Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckqkno99b0001y33ta8gg5far","content":"","site":{"data":{}},"length":0,"excerpt":"","more":""},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2021-07-01T06:33:06.646Z","updated":"2021-07-01T06:33:06.646Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckqkno99c0002y33ta6vs3hmw","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"length":367,"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}