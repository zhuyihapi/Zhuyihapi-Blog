{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"0bae0877624796503feec562c67e2d798a0d0ad3","modified":1625127331709},{"_id":"source/_posts/first-test.md","hash":"a5787c78815510a431c6ee0c3e539a1bfef20407","modified":1625122179866},{"_id":"source/_posts/Machine Learning.md","hash":"565493805e442d6b6446a48504dd4bbf11520a82","modified":1623547559000},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1625121186646},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1625129289289},{"_id":"themes/next/.gitignore","hash":"56f3470755c20311ddd30d421b377697a6e5e68b","modified":1625129289290},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1625129289289},{"_id":"themes/next/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1625129289289},{"_id":"themes/next/.travis.yml","hash":"ecca3b919a5b15886e3eca58aa84aafc395590da","modified":1625129289290},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1625129289290},{"_id":"themes/next/package.json","hash":"62fad6de02adbbba9fb096cbe2dcc15fe25f2435","modified":1625129289301},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"e554931b98f251fd49ff1d2443006d9ea2c20461","modified":1625129289289},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1625129289291},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1625129289290},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"aa4cb7aff595ca628cb58160ee1eee117989ec4e","modified":1625129289289},{"_id":"themes/next/gulpfile.js","hash":"1b4fc262b89948937b9e3794de812a7c1f2f3592","modified":1625129289293},{"_id":"themes/next/_config.yml","hash":"a85021b34477e02b0dd0839e18a5d61256207edc","modified":1625130283394},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1a435c20ae8fa183d49bbf96ac956f7c6c25c8af","modified":1625129289290},{"_id":"themes/next/.github/config.yml","hash":"1d3f4e8794986817c0fead095c74f756d45f91ed","modified":1625129289290},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"fca600ddef6f80c5e61aeed21722d191e5606e5b","modified":1625129289290},{"_id":"themes/next/README.md","hash":"9b4b7d66aca47f9c65d6321b14eef48d95c4dff1","modified":1625129289291},{"_id":"themes/next/.github/lock.yml","hash":"61173b9522ebac13db2c544e138808295624f7fd","modified":1625129289290},{"_id":"themes/next/.github/stale.yml","hash":"fdf82de9284f8bc8e0b0712b4cc1cb081a94de59","modified":1625129289290},{"_id":"themes/next/.github/release-drafter.yml","hash":"3cc10ce75ecc03a5ce86b00363e2a17eb65d15ea","modified":1625129289290},{"_id":"themes/next/.github/issue-close-app.yml","hash":"7cba457eec47dbfcfd4086acd1c69eaafca2f0cd","modified":1625129289290},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1625129289290},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"c7a994b9542040317d8f99affa1405c143a94a38","modified":1625129289291},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1625129289291},{"_id":"themes/next/docs/AUTHORS.md","hash":"10135a2f78ac40e9f46b3add3e360c025400752f","modified":1625129289291},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1625129289290},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"94dc3404ccb0e5f663af2aa883c1af1d6eae553d","modified":1625129289292},{"_id":"themes/next/docs/INSTALLATION.md","hash":"af88bcce035780aaa061261ed9d0d6c697678618","modified":1625129289292},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"8b6e4b2c9cfcb969833092bdeaed78534082e3e6","modified":1625129289292},{"_id":"themes/next/docs/MATH.md","hash":"d645b025ec7fb9fbf799b9bb76af33b9f5b9ed93","modified":1625129289292},{"_id":"themes/next/docs/DATA-FILES.md","hash":"cddbdc91ee9e65c37a50bec12194f93d36161616","modified":1625129289292},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1625129289292},{"_id":"themes/next/languages/de.yml","hash":"74c59f2744217003b717b59d96e275b54635abf5","modified":1625129289293},{"_id":"themes/next/languages/en.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1625129289294},{"_id":"themes/next/languages/default.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1625129289294},{"_id":"themes/next/languages/ar.yml","hash":"9815e84e53d750c8bcbd9193c2d44d8d910e3444","modified":1625129289293},{"_id":"themes/next/languages/fr.yml","hash":"752bf309f46a2cd43890b82300b342d7218d625f","modified":1625129289294},{"_id":"themes/next/languages/fa.yml","hash":"3676b32fda37e122f3c1a655085a1868fb6ad66b","modified":1625129289294},{"_id":"themes/next/languages/id.yml","hash":"572ed855d47aafe26f58c73b1394530754881ec2","modified":1625129289294},{"_id":"themes/next/languages/ja.yml","hash":"0cf0baa663d530f22ff380a051881216d6adcdd8","modified":1625129289294},{"_id":"themes/next/languages/hu.yml","hash":"b1ebb77a5fd101195b79f94de293bcf9001d996f","modified":1625129289294},{"_id":"themes/next/languages/es.yml","hash":"c64cf05f356096f1464b4b1439da3c6c9b941062","modified":1625129289294},{"_id":"themes/next/languages/ko.yml","hash":"0feea9e43cd399f3610b94d755a39fff1d371e97","modified":1625129289294},{"_id":"themes/next/languages/pt.yml","hash":"718d131f42f214842337776e1eaddd1e9a584054","modified":1625129289294},{"_id":"themes/next/languages/it.yml","hash":"44759f779ce9c260b895532de1d209ad4bd144bf","modified":1625129289294},{"_id":"themes/next/languages/ru.yml","hash":"e993d5ca072f7f6887e30fc0c19b4da791ca7a88","modified":1625129289295},{"_id":"themes/next/languages/nl.yml","hash":"5af3473d9f22897204afabc08bb984b247493330","modified":1625129289294},{"_id":"themes/next/languages/tr.yml","hash":"fe793f4c2608e3f85f0b872fd0ac1fb93e6155e2","modified":1625129289295},{"_id":"themes/next/languages/uk.yml","hash":"3a6d635b1035423b22fc86d9455dba9003724de9","modified":1625129289295},{"_id":"themes/next/languages/vi.yml","hash":"93393b01df148dcbf0863f6eee8e404e2d94ef9e","modified":1625129289295},{"_id":"themes/next/languages/pt-BR.yml","hash":"67555b1ba31a0242b12fc6ce3add28531160e35b","modified":1625129289294},{"_id":"themes/next/layout/archive.swig","hash":"e4e31317a8df68f23156cfc49e9b1aa9a12ad2ed","modified":1625129289300},{"_id":"themes/next/languages/zh-CN.yml","hash":"a1f15571ee7e1e84e3cc0985c3ec4ba1a113f6f8","modified":1625129289295},{"_id":"themes/next/layout/_layout.swig","hash":"6a6e92a4664cdb981890a27ac11fd057f44de1d5","modified":1625129289295},{"_id":"themes/next/layout/category.swig","hash":"1bde61cf4d2d171647311a0ac2c5c7933f6a53b0","modified":1625129289300},{"_id":"themes/next/layout/index.swig","hash":"7f403a18a68e6d662ae3e154b2c1d3bbe0801a23","modified":1625129289300},{"_id":"themes/next/layout/page.swig","hash":"db581bdeac5c75fabb0f17d7c5e746e47f2a9168","modified":1625129289301},{"_id":"themes/next/languages/zh-HK.yml","hash":"3789f94010f948e9f23e21235ef422a191753c65","modified":1625129289295},{"_id":"themes/next/layout/post.swig","hash":"2f6d992ced7e067521fdce05ffe4fd75481f41c5","modified":1625129289301},{"_id":"themes/next/layout/tag.swig","hash":"0dfb653bd5de980426d55a0606d1ab122bd8c017","modified":1625129289301},{"_id":"themes/next/languages/zh-TW.yml","hash":"8c09da7c4ec3fca2c6ee897b2eea260596a2baa1","modified":1625129289295},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"d3efc0df0275c98440e69476f733097916a2d579","modified":1625129289290},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"c3e6b8196c983c40fd140bdeca012d03e6e86967","modified":1625129289289},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"12d99fb8b62bd9e34d9672f306c9ae4ace7e053e","modified":1625129289289},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"9c4fe2873123bf9ceacab5c50d17d8a0f1baef27","modified":1625129289292},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"0bd2d696f62a997a11a7d84fec0130122234174e","modified":1625129289292},{"_id":"themes/next/docs/ru/README.md","hash":"85dd68ed1250897a8e4a444a53a68c1d49eb7e11","modified":1625129289292},{"_id":"themes/next/scripts/renderer.js","hash":"49a65df2028a1bc24814dc72fa50d52231ca4f05","modified":1625129289302},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"5237a368ab99123749d724b6c379415f2c142a96","modified":1625129289292},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"34b88784ec120dfdc20fa82aadeb5f64ef614d14","modified":1625129289292},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"ca1030efdfca5e20f9db2e7a428998e66a24c0d0","modified":1625129289293},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"579c7bd8341873fb8be4732476d412814f1a3df7","modified":1625129289293},{"_id":"themes/next/docs/zh-CN/README.md","hash":"c038629ff8f3f24e8593c4c8ecf0bef3a35c750d","modified":1625129289293},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"8b18f84503a361fc712b0fe4d4568e2f086ca97d","modified":1625129289293},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"53df7d537e26aaf062d70d86835c5fd8f81412f3","modified":1625129289290},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"d3f03be036b75dc71cf3c366cd75aee7c127c874","modified":1625129289293},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"b92585d251f1f9ebe401abb5d932cb920f9b8b10","modified":1625129289293},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"d9ce7331c1236bbe0a551d56cef2405e47e65325","modified":1625129289293},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"fb23b85db6f7d8279d73ae1f41631f92f64fc864","modified":1625129289293},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"9c8dc0b8170679cdc1ee9ee8dbcbaebf3f42897b","modified":1625129289295},{"_id":"themes/next/layout/_macro/post.swig","hash":"090b5a9b6fca8e968178004cbd6cff205b7eba57","modified":1625129289295},{"_id":"themes/next/layout/_partials/footer.swig","hash":"4369b313cbbeae742cb35f86d23d99d4285f7359","modified":1625129289296},{"_id":"themes/next/layout/_partials/languages.swig","hash":"ba9e272f1065b8f0e8848648caa7dea3f02c6be1","modified":1625129289296},{"_id":"themes/next/layout/_scripts/index.swig","hash":"cea942b450bcb0f352da78d76dc6d6f1d23d5029","modified":1625129289297},{"_id":"themes/next/layout/_partials/comments.swig","hash":"db6ab5421b5f4b7cb32ac73ad0e053fdf065f83e","modified":1625129289296},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"83a40ce83dfd5cada417444fb2d6f5470aae6bb0","modified":1625129289297},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1625129289296},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"71655ca21907e9061b6e8ac52d0d8fbf54d0062b","modified":1625129289295},{"_id":"themes/next/layout/_scripts/three.swig","hash":"a4f42f2301866bd25a784a2281069d8b66836d0b","modified":1625129289298},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"4d2c93c66e069852bb0e3ea2e268d213d07bfa3f","modified":1625129289298},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"b782eb2e34c0c15440837040b5d65b093ab6ec04","modified":1625129289299},{"_id":"themes/next/scripts/events/index.js","hash":"5743cde07f3d2aa11532a168a652e52ec28514fd","modified":1625129289301},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"d1f2bfde6f1da51a2b35a7ab9e7e8eb6eefd1c6b","modified":1625129289297},{"_id":"themes/next/layout/_third-party/index.swig","hash":"70c3c01dd181de81270c57f3d99b6d8f4c723404","modified":1625129289299},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"aec50ed57b9d5d3faf2db3c88374f107203617e0","modified":1625129289302},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"ef38c213679e7b6d2a4116f56c9e55d678446069","modified":1625129289298},{"_id":"themes/next/scripts/filters/locals.js","hash":"b193a936ee63451f09f8886343dcfdca577c0141","modified":1625129289302},{"_id":"themes/next/scripts/filters/post.js","hash":"44ba9b1c0bdda57590b53141306bb90adf0678db","modified":1625129289302},{"_id":"themes/next/scripts/filters/minify.js","hash":"19985723b9f677ff775f3b17dcebf314819a76ac","modified":1625129289302},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"5e11f30ddb5093a88a687446617a46b048fa02e5","modified":1625129289302},{"_id":"themes/next/scripts/helpers/font.js","hash":"40cf00e9f2b7aa6e5f33d412e03ed10304b15fd7","modified":1625129289302},{"_id":"themes/next/scripts/helpers/engine.js","hash":"bdb424c3cc0d145bd0c6015bb1d2443c8a9c6cda","modified":1625129289302},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"958e86b2bd24e4fdfcbf9ce73e998efe3491a71f","modified":1625129289302},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"94e0bbc7999b359baa42fa3731bdcf89c79ae2b3","modified":1625129289302},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"703bdd142a671b4b67d3d9dfb4a19d1dd7e7e8f7","modified":1625129289302},{"_id":"themes/next/scripts/tags/button.js","hash":"8c6b45f36e324820c919a822674703769e6da32c","modified":1625129289302},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"d902fd313e8d35c3cc36f237607c2a0536c9edf1","modified":1625129289303},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1625129289303},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1625129289303},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1625129289303},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"311e5eceec9e949f1ea8d623b083cec0b8700ff2","modified":1625129289299},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"2731e262a6b88eaee2a3ca61e6a3583a7f594702","modified":1625129289300},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f1826ade2d135e2f60e2d95cb035383685b3370c","modified":1625129289303},{"_id":"themes/next/scripts/tags/tabs.js","hash":"93d8a734a3035c1d3f04933167b500517557ba3e","modified":1625129289303},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1625129289303},{"_id":"themes/next/scripts/tags/pdf.js","hash":"8c613b39e7bff735473e35244b5629d02ee20618","modified":1625129289303},{"_id":"themes/next/source/css/main.styl","hash":"a3a3bbb5a973052f0186b3523911cb2539ff7b88","modified":1625129289310},{"_id":"themes/next/source/css/_mixins.styl","hash":"e31a557f8879c2f4d8d5567ee1800b3e03f91f6e","modified":1625129289308},{"_id":"themes/next/source/css/_colors.styl","hash":"a8442520f719d3d7a19811cb3b85bcfd4a596e1f","modified":1625129289303},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1625129289310},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1625129289310},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1625129289310},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1625129289310},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1625129289310},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1625129289310},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1625129289311},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1625129289311},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1625129289311},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1625129289311},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1625129289311},{"_id":"themes/next/source/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1625129289311},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1625129289311},{"_id":"themes/next/source/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1625129289311},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1625129289311},{"_id":"themes/next/source/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1625129289311},{"_id":"themes/next/source/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1625129289311},{"_id":"themes/next/source/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1625129289311},{"_id":"themes/next/source/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1625129289311},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"810d544019e4a8651b756dd23e5592ee851eda71","modified":1625129289296},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"000bad572d76ee95d9c0a78f9ccdc8d97cc7d4b4","modified":1625129289296},{"_id":"themes/next/source/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1625129289312},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"7dbe93b8297b746afb89700b4d29289556e85267","modified":1625129289296},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"d31f896680a6c2f2c3f5128b4d4dd46c87ce2130","modified":1625129289296},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"9440d8a3a181698b80e1fa47f5104f4565d8cdf3","modified":1625129289296},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"c70f8e71e026e878a4e9d5ab3bbbf9b0b23c240c","modified":1625129289296},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"ae2261bea836581918a1c2b0d1028a78718434e0","modified":1625129289296},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"9b7a66791d7822c52117fe167612265356512477","modified":1625129289296},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"954ad71536b6eb08bd1f30ac6e2f5493b69d1c04","modified":1625129289297},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"ceba16b9bd3a0c5c8811af7e7e49d0f9dcb2f41e","modified":1625129289297},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"c851717497ca64789f2176c9ecd1dedab237b752","modified":1625129289296},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":1625129289297},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"2b1a73556595c37951e39574df5a3f20b2edeaef","modified":1625129289297},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"48430bd03b8f19c9b8cdb2642005ed67d56c6e0b","modified":1625129289297},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"f48a6a8eba04eb962470ce76dd731e13074d4c45","modified":1625129289297},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"c46849e0af8f8fb78baccd40d2af14df04a074af","modified":1625129289297},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f79c44692451db26efce704813f7a8872b7e63a0","modified":1625129289297},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"2be50f9bfb1c56b85b3b6910a7df27f51143632c","modified":1625129289297},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1625129289298},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1625129289298},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"077b5d66f6309f2e7dcf08645058ff2e03143e6c","modified":1625129289298},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1625129289298},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1625129289298},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1625129289298},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"2fa2b51d56bfac6a1ea76d651c93b9c20b01c09b","modified":1625129289298},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"5adea065641e8c55994dd2328ddae53215604928","modified":1625129289298},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"1472cabb0181f60a6a0b7fec8899a4d03dfb2040","modified":1625129289298},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1625129289299},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"f39a5bf3ce9ee9adad282501235e0c588e4356ec","modified":1625129289299},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b14908644225d78c864cd0a9b60c52407de56183","modified":1625129289299},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"82f5b6822aa5ec958aa987b101ef860494c6cf1f","modified":1625129289299},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"f910618292c63871ca2e6c6e66c491f344fa7b1f","modified":1625129289299},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"6c5976621efd5db5f7c4c6b4f11bc79d6554885f","modified":1625129289299},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"d6ceb70648555338a80ae5724b778c8c58d7060d","modified":1625129289299},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"f7a9eca599a682479e8ca863db59be7c9c7508c8","modified":1625129289299},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"d35a999d67f4c302f76fdf13744ceef3c6506481","modified":1625129289300},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"ecf751321e799f0fb3bf94d049e535130e2547aa","modified":1625129289299},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"be0a8eccf1f6dc21154af297fc79555343031277","modified":1625129289299},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1625129289300},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1625129289300},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"4b1986e43d6abce13450d2b41a736dd6a5620a10","modified":1625129289300},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"5f6a966c509680dbfa70433f9d658cee59c304d7","modified":1625129289300},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"4791c977a730f29c846efcf6c9c15131b9400ead","modified":1625129289299},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"d56d5af427cdfecc33a0f62ee62c056b4e33d095","modified":1625129289300},{"_id":"themes/next/scripts/events/lib/config.js","hash":"d34c6040b13649714939f59be5175e137de65ede","modified":1625129289301},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"b26ac2bfbe91dd88267f8b96aee6bb222b265b7a","modified":1625129289300},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"d30b0e255a8092043bac46441243f943ed6fb09b","modified":1625129289300},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1625129289301},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"f3c43664a071ff3c0b28bd7e59b5523446829576","modified":1625129289300},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"f233d8d0103ae7f9b861344aa65c1a3c1de8a845","modified":1625129289301},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"2486f3e0150c753e5f3af1a3665d074704b8ee2c","modified":1625129289301},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"7f2d93af012c1e14b8596fecbfc7febb43d9b7f5","modified":1625129289301},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"7f8b92913d21070b489457fa5ed996d2a55f2c32","modified":1625129289301},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"e51dc3072c1ba0ea3008f09ecae8b46242ec6021","modified":1625129289302},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"6cbd85f9433c06bae22225ccf75ac55e04f2d106","modified":1625129289302},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"d5fefc31fba4ab0188305b1af1feb61da49fdeb0","modified":1625129289302},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"a54708fd9309b4357c423a3730eb67f395344a5e","modified":1625129289301},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1625129289300},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"4c0c99c7e0f00849003dfce02a131104fb671137","modified":1625129289301},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"f70be8e229da7e1715c11dd0e975a2e71e453ac8","modified":1625129289310},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"62df49459d552bbf73841753da8011a1f5e875c8","modified":1625129289310},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"f4e694e5db81e57442c7e34505a416d818b3044a","modified":1625129289310},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"612ec843372dae709acb17112c1145a53450cc59","modified":1625129289310},{"_id":"themes/next/source/css/_variables/base.styl","hash":"818508748b7a62e02035e87fe58e75b603ed56dc","modified":1625129289310},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1625129289311},{"_id":"themes/next/source/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1625129289311},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"ca5e70662dcfb261c25191cc5db5084dcf661c76","modified":1625129289303},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"8e7b57a72e757cf95278239641726bb2d5b869d1","modified":1625129289303},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"a47725574e1bee3bc3b63b0ff2039cc982b17eff","modified":1625129289303},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"2e3bf7baf383c9073ec5e67f157d3cb3823c0957","modified":1625129289305},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1625129289313},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1625129289313},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"681d33e3bc85bdca407d93b134c089264837378c","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"a1690e035b505d28bdef2b4424c13fc6312ab049","modified":1625129289306},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"0b2c4b78eead410020d7c4ded59c75592a648df8","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"a2e9e00962e43e98ec2614d6d248ef1773bb9b78","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"b1f0fab7344a20ed6748b04065b141ad423cf4d9","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"8f58570a1bbc34c4989a47a1b7d42a8030f38b06","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"179e33b8ac7f4d8a8e76736a7e4f965fe9ab8b42","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1625129289307},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"7785bd756e0c4acede3a47fec1ed7b55988385a5","modified":1625129289308},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"bb7ace23345364eb14983e860a7172e1683a4c94","modified":1625129289308},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"7104b9cef90ca3b140d7a7afcf15540a250218fc","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"f6516d0f7d89dc7b6c6e143a5af54b926f585d82","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"18ce72d90459c9aa66910ac64eae115f2dde3767","modified":1625129289307},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a717969829fa6ef88225095737df3f8ee86c286b","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"f0131db6275ceaecae7e1a6a3798b8f89f6c850d","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"6136da4bbb7e70cec99f5c7ae8c7e74f5e7c261a","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"93db5dafe9294542a6b5f647643cb9deaced8e06","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"2b2e7b5cea7783c9c8bb92655e26a67c266886f0","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"e282df938bd029f391c466168d0e68389978f120","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"70a4324b70501132855b5e59029acfc5d3da1ebd","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"85da2f3006f4bef9a2199416ecfab4d288f848c4","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"44f47c88c06d89d06f220f102649057118715828","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"4d1c17345d2d39ef7698f7acf82dfc0f59308c34","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"e740deadcfc4f29c5cb01e40f9df6277262ba4e3","modified":1625129289309},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1625129289309},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fafc96c86926b22afba8bb9418c05e6afbc05a57","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1625129289304},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1625129289312},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"e771dcb0b4673e063c0f3e2d73e7336ac05bcd57","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"d21d4ac1982c13d02f125a67c065412085a92ff2","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"e75693f33dbc92afc55489438267869ae2f3db54","modified":1625129289304},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1625129289312},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f49ca072b5a800f735e8f01fc3518f885951dd8e","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"902569a9dea90548bec21a823dd3efd94ff7c133","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"1e4190c10c9e0c9ce92653b0dbcec21754b0b69d","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ded41fd9d20a5e8db66aaff7cc50f105f5ef2952","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"72d495a88f7d6515af425c12cbc67308a57d88ea","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"65cb6edb69e94e70e3291e9132408361148d41d5","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"6a97bcfa635d637dc59005be3b931109e0d1ead5","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"d114b2a531129e739a27ba6271cfe6857aa9a865","modified":1625129289304},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"99e12c9ce3d14d4837e3d3f12fc867ba9c565317","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"a760ee83ba6216871a9f14c5e56dc9bd0d9e2103","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"e2992846b39bf3857b5104675af02ba73e72eed5","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9a878d0119785a2316f42aebcceaa05a120b9a7a","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"9f0b93d109c9aec79450c8a0cf4a4eab717d674d","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":1625129289305},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"b49e9fbd3c182b8fc066b8c2caf248e3eb748619","modified":1625129289305},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e2d606f1ac343e9be4f15dbbaf3464bc4df8bf81","modified":1625129289305},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"454a4aebfabb4469b92a8cbb49f46c49ac9bf165","modified":1625129289305},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"e7a9fdb6478b8674b1cdf94de4f8052843fb71d9","modified":1625129289305},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"5f432a6ed9ca80a413c68b00e93d4a411abf280a","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"a793cfff86ad4af818faef04c18013077873f8f0","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"45a239edca44acecf971d99b04f30a1aafbf6906","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"fa0222197b5eee47e18ac864cdc6eac75678b8fe","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2cb1876e9e0c9ac32160888af27b1178dbcb0616","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"44487d9ab290dc97871fa8dd4487016deb56e123","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"a05a4031e799bc864a4536f9ef61fe643cd421af","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"1f0e7fbe80956f47087c2458ea880acf7a83078b","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"b3220db827e1adbca7880c2bb23e78fa7cbe95cb","modified":1625129289307},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b2fc519828fe89a1f8f03ff7b809ad68cd46f3d7","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"9b479c2f9a9bfed77885e5093b8245cc5d768ec7","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"a960a2dd587b15d3b3fe1b59525d6fa971c6a6ec","modified":1625129289306},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"2a47f8a6bb589c2fb635e6c1e4a2563c7f63c407","modified":1625129289307},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"a9cd93c36bae5af9223e7804963096274e8a4f03","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"3b3acc5caa0b95a2598bef4eeacb21bab21bea56","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"35c871a809afa8306c8cde13651010e282548bc6","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f71a3e86c05ea668b008cf05a81f67d92b6d65e4","modified":1625129289307},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"1d2778ca5aeeeafaa690dc2766b01b352ab76a02","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"709d10f763e357e1472d6471f8be384ec9e2d983","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"e4d9a77ffe98e851c1202676940097ba28253313","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"d7fce4b51b5f4b7c31d93a9edb6c6ce740aa0d6b","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b49c64f8e9a6ca1c45c0ba98febf1974fdd03616","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"f23670f1d8e749f3e83766d446790d8fd9620278","modified":1625129289308},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1625129289308},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1625129289312},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1625129289312},{"_id":"public/search.xml","hash":"8bb8f4dd866b0142e5f1f1de92836aab98a8f23f","modified":1625129879529},{"_id":"public/2021/07/01/first-test/index.html","hash":"7562e17fb03fd1fd7c0e9ef16e7b4220ec561630","modified":1625130296582},{"_id":"public/2021/07/01/hello-world/index.html","hash":"9a6df44b7b7efc63702dd89cdcf32ca8ed4b4f46","modified":1625130296582},{"_id":"public/archives/2021/index.html","hash":"14f791f24b60a1503071e1b030006db3443fa5da","modified":1625130296582},{"_id":"public/archives/2021/05/index.html","hash":"9e0f427164688393ce596a961d9b7727c64a1aa9","modified":1625130296582},{"_id":"public/archives/index.html","hash":"88c13e70b68e924f30fc3aa20a7eff27dc41def7","modified":1625130296582},{"_id":"public/2021/05/25/Machine Learning/index.html","hash":"063b8b1fb0fa6a52b6f659edb1597e22dbc6514d","modified":1625130296582},{"_id":"public/archives/2021/07/index.html","hash":"6b9f8dc9f1da46b1161511ce2375ba404966376f","modified":1625130296582},{"_id":"public/index.html","hash":"ffa951dad5ba9eb593c53064ebff5926201f5353","modified":1625130296582},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1625129879529},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1625129879529},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1625129879529},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1625129879529},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1625129879529},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1625129879529},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1625129879529},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1625129879529},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1625129879529},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1625129879529},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1625129879529},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1625129879529},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1625129879529},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1625129879529},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1625129879529},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1625129879529},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1625129879529},{"_id":"public/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1625129879529},{"_id":"public/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1625129879529},{"_id":"public/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1625129879529},{"_id":"public/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1625129879529},{"_id":"public/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1625129879529},{"_id":"public/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1625129879529},{"_id":"public/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1625129879529},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1625129879529},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1625129879529},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1625129879529},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1625129879529},{"_id":"public/css/main.css","hash":"36c0c12fdcac6d075a2779ac1eff365f1080724a","modified":1625129879529}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"first test","date":"2021-07-01T06:49:39.000Z","_content":"","source":"_posts/first-test.md","raw":"---\ntitle: first test\ndate: 2021-07-01 14:49:39\ntags:\n---\n","slug":"first-test","published":1,"updated":"2021-07-01T06:49:39.866Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckqkodclc0000033tg5ip50xe","content":"","site":{"data":{}},"length":0,"excerpt":"","more":""},{"_content":"\n\n# [Machine Learning](https://www.bilibili.com/video/BV164411b7dx)\n\n\n\n## Index\n\n[toc]\n\n\n\n## 1 Introduction\n\n### 1.1 Supervised Learn\n\nA \"right answer\" given\n\n#### Regression\n\nPredict continuous valued output (e.g. housing price)\n\n**Related algorithms**:\n\n- Linear regression\n- Neural Networks\n- Nearest Neighbor\n\n#### Classification\n\nDiscrete valued output (0 or 1)\n\n**Related algorithms**:\n\n- Logistic regression\n- K-Nearest Neighbor (KNN)\n- Support Vector Machines (SVM)\n- Naïve Bayes\n- Decision Trees\n- Neural Networks\n\n\n\n### 1.2 Unsupervised Learn\n\n#### [Cluster](https://zhuanlan.zhihu.com/p/78382376)\n\n**Applications**:\n\n> 聚类：将相似的对象归到同一个簇中，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。\n\n- Market segmentation\n- Social network analysis\n- Organize computing cluster\n- Astronomical data analysis\n\n[**Related algorithms**](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68):\n\n- K-Means Clustering\n- Mean-Shift Clustering\n- Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n- Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)\n\n\n\n## 2 Linear Regression with One Variable\n\n### 2.0 Model Representation - Notation\n\n$m$: number of training examples\n\n$x$'s: input variable/feature\n\n$y$'s: output variable/feature\n\n$(x,y)$: a training example\n\n$(x^i,y^i)$: $i$ represents the $i^{th}$\n\n\n\n### 2.1 Model and Cost Function\n\n**Hypothesis 假设函数**: \n$$\nh_θ(x)=θ_0+θ_1x\n$$\n**Parameters**: $\\theta_0,\\theta_1$\n\n> $\\theta_1$代表斜率而$\\theta_0$则代表由代价函数计算出的差值\n\n[**Cost Function 代价函数**](https://www.cnblogs.com/geaozhang/p/11442343.html):\n$$\nJ(θ_0,θ_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2\n$$\n\n> minimize squared error cost function (最小化平方差代价函数)\n\n**Goal**: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$\n\n\n\n### 2.2 Parameter Learning - Gradient Descent\n\n#### Outline\n\n- Start with some $\\theta_0,\\theta_1$\n\n- Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum\n\n#### Gradient descent algorithm\n\n​\tRepeat until convergence {\n\n​\t\t$\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$\t(for j=0 and j=1) \n\n}\n\n> $:=$ colon equals, which used to denote assignment (赋值运算符)\n>\n> $\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent\n>\n> $\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term (导数项)\n>\n> **Assert**: simultaneous update $\\theta_0,\\theta_1$ \"at the same time\"\n\n#### Gradient descent intuition\n\n$$\n\\theta_1:=\\theta_1-\\alpha\\frac∂{∂\\theta_j}J(\\theta_1)\n$$\n\n1. $\\alpha$\n\n   if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.\n\n   if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.\n\n2. Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.\n\n3. As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.\n\n#### Gradient descent for linear regression\n\n> Apply gradient descent to minimize squared error cost function \n\n$$\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)=\\frac∂{∂\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$\t\t*Expanding the formula*\n\nSubstituting 0 and 1 into $j$\n\n$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$\n\n$$\\theta_1:j=1:\\frac∂{∂\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$\n\nWhen work out the derivatives, which is the slope of the cost function *J*, plug them back in to gradient descent algorithm (remember to update simultaneously).\n\n**Convex function**\n\n> It doesn't have any local optimum except for the global optimum\n\n**\"Batch\" Gradient Descent**\n\n> \"Batch\": Each step of gradient descent uses all the training examples (entire training set)\n\n\n\n## 3 Linear Algebra review (optional)\n\n### 3.1 Matrices and vectors\n\n#### Matrix\n\n> Rectangular array of numbers\n\n3×2 matrix: $\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\\\ 5&6\\end{bmatrix}$\t2×3 matrix: $\\begin{bmatrix}1 &2&3 \\\\6& 3 & 4\\\\ \\end{bmatrix}$\n\n**Dimension of matrix**: number of rows $\\times$ number of columns\n\n> The above matrix can be also write as $\\mathbb R^{3\\times2}$ \n\n**Refer to specific elements of the matrix **(entries of matrix)\n\n$A=\\begin{bmatrix}1402&191 \\\\ 1371 &821\\\\ 949&1437\\\\147&1448\\end{bmatrix}$\n\n$A_{ij}=$ \"$i$,$j$ entry\" in the $i^{th}$ row, $j^{th}$ column.\n\n$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$\n\n#### Vector\n\n> An $n\\times1$ matrix\n\n$y=\\begin{bmatrix}460\\\\232\\\\315\\\\178\\end{bmatrix}$\n\n$y_i=i^{th}$ element\n\n> It is often customary to use uppercase letters for matrices and lowercase letters for vectors\n\n\n\n### 3.2 Addition and Scalar multiplication\n\n\n\n### 3.3 Matrix-vector multiplication\n\n**Details**:\n\n​            $A$           $\\times$   $x$       $=$       $y$\n\n$\\begin{bmatrix}&&&&&\\\\ \\\\ \\\\ \\end{bmatrix}\\times\\begin{bmatrix}\\\\ \\\\ \\\\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\\\ \\\\ \\\\ \\\\ \\end{bmatrix}$\n\n   m$\\times$n matrix        n$\\times$1    m-dimensional vector\n\nTo get $y_i$, multiply $A$' $i^{th}$ row with elements of vector $x$, and add them up.\n\n**Calculation Tips**：\n\nHouse sizes: 2104,1216, 1534, 852\n\nCompeting hypotheses: $h_\\theta(x)=-40+0.25x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\\\0.25\\end{bmatrix}$ \n\n\n\n### 3.4 Matrix-matrix multiplication\n\n**Details**:\n\n$A\\times B=C$\n\n[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o\n\nThe $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,...,0)\n\n**Example**:\n\n$\\begin{bmatrix}1&3\\\\2&5\\end{bmatrix}\\begin{bmatrix}0&1\\\\3&2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&1\\times1+3\\times2 \\\\2\\times0+5\\times3&2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&7\\\\15&12\\end{bmatrix}$\n\n**Calculation Tips II**：\n\nHouse sizes: 2104,1216, 1534, 852\n\nThree competing hypotheses: \n\n1. $h_\\theta(x)=-40+0.25x$\n2. $h_\\theta(x)=200+0.1x$\n3. $h_\\theta(x)=-150+0.4x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40&200&-150 \\\\ 0.25&0.1&0.4 \\end{bmatrix}$\n\n\n\n### 3.5 Matrix multiplication properties\n\nLet $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (**Not commutative**) \n\n#### Identity Matrix\n\nDenoted $I$ (or $I_{n\\times n}$).\n\nExample of identity matrices:\n\n2$\\times$2: $\\begin{bmatrix}1&0 \\\\ 0&1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&0&0 \\\\ 0&1&0 \\\\ 0&0&1\\end{bmatrix}$      $\\cdots$\n\nFor any matrix $A$,\n\n$$\nA\\cdot I=I\\cdot A=A\n$$\n\n> Implicit conditions of the formula: \n>\n> $A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$\n\n\n\n### 3.6 Inverse and Transpose\n\n> 矩阵的逆和矩阵的转置\n\nNot all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&0 \\\\ 0&0\\end{bmatrix}$)\n\n> Matrices that don't have an inverse are \"singular\" or \"degenerate\".\n\n#### Matrix inverse\n\nIf A is an m$\\times$m matrix, and if it has an inverse,\n$$\nAA^{-1}=A^{-1}A=I\n$$\n\n> An m$\\times$m matrix called a square matrix (方阵), only square matrix has an inverse. \n\n#### Matrix transpose\n\nExample:\n$$\nA=\\begin{bmatrix}1&2&0 \\\\ 3&5&9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&3 \\\\ 2&5 \\\\ 0&9 \\end{bmatrix}\n$$\nLet $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.\n\n\n\n## 4 Linear Regression with Multiple Variables\n\n### 4.1 Multiple feature\n\n#### Notation\n\n$n$ = number of features\n\n$x^{(i)}$ = input (features) of $i^{th}$ training example\n\n$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example\n\n#### Multivariate linear regression\n\n> 多元线性回归\n\n**Hypothesis**:\n\n$h_θ(x)=θ_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\nFor convenience of notation, define $x_0=1$. Then\n\n$h_θ(x)=θ_0x_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n​          $=\\theta^Tx$\n\n​          $=\\begin{bmatrix}\\theta_0&\\theta_1&\\cdots&\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix}$\n\n\n\n### 4.2 Gradient descent for multiple variables\n\n- 如何设定假设的参数\n\n- 如何使用梯度下降法来处理多元线性回归\n\n**Hypothesis**: $h_θ(x)=\\theta^Tx=θ_0x_1+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n**Parameters**: $\\theta_0$,$\\theta_1$,...,$\\theta_n$\n\n**Cost function**: $J(\\theta_0$,$\\theta_1$,...,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$\n\n**Gradient descent**:\n\n​\tRepeat {\n\n​\t\t$\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,...,\\theta_n)$\t\t\n\n}\t\t(simultaneously update for every $j=0,...,n$)\n\n> $J(\\theta_0,...,\\theta_n)$ can be instead by $J(\\theta)$\n\n**New algorithm** for $n\\ge1$:\n\n​\tRepeat {\n\n​\t\t$\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Previously $(n=1)$ is in 2.2 \n\n\n\n### 4.3 Gradient descent in practice I: Feature Scaling\n\n> 梯度下降运算中的实用技巧——特征缩放\n\n#### Feature Scaling\n\n> **Idea**: Make sure flatten are on a similar scale.\n\nE.g. $x_1$= size(0-2000 $feet^2$)\n\n​      $x_2$= number of bedrooms (1-5)\n\nIt will be better to limit both $x_1$ and $x_2$ in [0,1]\n\n$x_1=\\frac{size(feet^2)}{2000}$\n\n$x_2=\\frac{numberOfBedroom}{5}$\n\n> 椭圆相较于正圆需要更多的时间来计算梯度下降\n\nMore general, get every feature into approximately a $-1\\le x_i\\le1$ range.\n\n> 范围的上下限并不是被严格限制的，不过越接近-1和1越好。过大和过小都是不合适的。\n\n#### Mean normalization\n\nReplace $x_i$ with $x_i-µ_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)\n**E.g.** \n\n$x_1=\\frac{size(feet^2)-1000}{2000}$\n\n$x_2=\\frac{numberOfBedroom-2}{5}$\n\n> $µ_i$ (1000 and 2) is considered as the average value of $x_i$ in training set\n\n$$\nx_i\\leftarrow \\frac{x_i-µ_i}{range(max-min)}\n$$\n\n\n\n### 4.4 Gradient descent in practice II: Learning rate\n\n- The chapter will center around the learning rate $\\alpha$\n\n**Gradient descent**\n\n* $\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta)$\n* \"Debugging\": How to make sure gradient descent is working correctly.\n* How to choose learning rate $\\alpha$\n\nDeclare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.\n\n> 这是因为若将$min_\\theta J(\\theta)$作为纵轴，将迭代次数作为横轴，那么得到的是是一个近似$y=|\\frac 1x|$的图像。当迭代次数达到一定量$(\\epsilon)$后，梯度下降的量就几乎可以忽略不计了，所以该测试就判断函数已经收敛。不过要选择一个合适的阈值（threshold，$\\epsilon$）并不容易。\n\n> 如果你的图像并不是上述的样子，那么说明你的$\\alpha$值选取的不恰当。例如图像在0点附近形如指数函数图像，或者呈波浪形，就意味着你的$\\alpha$值过大了，函数无法收敛。\n\n**Summary**:\n\n* If $\\alpha$ is too small: slow convergence.\n* If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.\n\nRecommended choices for $\\alpha$:\n\n..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1,... \n\n\n\n### 4.5 Features and polynomial regression\n\n> 根据特征选择算法以提高效率；使用多项式回归来拟合复杂函数\n\nHousing prices prediction:\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth\n$$\n\n> It's better to use $area$ which is equal to $frontage\\times depth$ as new feature.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times area\n$$\n\n#### Choice of feature\n\nSuppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2\n$$\n\n> 使用二次函数来拟合房价-面积曲线可能在一定范围内是合适的，但是二次函数曲线一定会在达到顶点后下降，房价却不会。所以这并不是一个好的选择。\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3\n$$\n\n> 也许是可行的，但要注意应用特征缩放，从而使三个特征值$(size,(size)^2,(size)^3)$都在大致相同的范围内\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}\n$$\n\n> 上升的曲线，斜率随着x（area）变大逐渐变小，增长趋于平缓，似乎也不错\n\n**Summary**:\n\nYou have a choice in what features to use to fix more complex functions to your data!\n\n\n\n### 4.6 Normal equation (unfinished)\n\n> 对于某些线性回归问题，求取参数$\\theta$最优值的方法。不同与以往的迭代算法（梯度下降的多次迭代来收敛到全局最小值），正规方程提供了一种解析解法，一次性求解$\\theta$的最优值。\n\n**Normal equation**: Method to solve for $\\theta$ analytically.\n\n\n\n#### Compare to Gradient Descent\n\n$m$ training example, $n$ features.\n\n| Gradient Descent            | Normal Equation               |\n| --------------------------- | ----------------------------- |\n| Need to choice a $\\alpha$   | No need to choice a $\\alpha$  |\n| Needs many iterations       | Don't need to iterate         |\n| Work well even $n$ is large | Need to compute $(X^TX)^{-1}$ |\n|                             | Slow if $n$ is very large     |\n\n\n$$\n\\theta=(X^TX)^{-1}X^Ty\n$$\n$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$\n\n**Octave**: `pinv(X'*X)*X'*y` \n\n> `X'` is the transpose of $X$\n>\n> `pinv` is a function  to compute the inverse of a matrix\n\n\n\n<!--unfinished-->\n\n### 4.7 Normal equation and non-invertibility (optional) (unfinished)\n\n<!--unfinished-->\n\n\n\n## 5 Octave Tutorial (ignored)\n\n<!--unfinished-->\n\n\n\n## 6 Logistic Regression\n\n> 回归算法\n\n### 6.1 Classification\n\n**Classification**\n\n$y\\in \\{0,1\\} $\n\n0: \"Negative Class\" (e.g., benign tumor)\n\n1: \"Positive Class\" (e.g., malignant tumor)\n\n> There are multi-class problems as well that y can take value from 0, 1, 2, 3,...\n\nLearning regression isn't fit the classification problem\n\nIn the [video](https://www.bilibili.com/video/BV164411b7dx?p=32&t=160) there is an example to explain it. \n\nAnother example:\n\nClassification: y = 0 or 1\n\n​\t$H_\\theta(x)$ can be $>1$ or $<0$ if we use the linear regression\n\n> Obviously, the label is either 0 or 1.\n\nLogistic Regression: $0\\le h_\\theta(x)\\le1$\n\n> This is a classification algorithm whose output always between 1 and 0. Besides, it's a classification algorithm instead of linear regression algorithm though there is a \"regression\" in its name.\n\n\n\n### 6.2 Hypothesis Representation\n\n> 假设陈述\n\n- What is the function we're going to use to representation hypothesis when we have a classification problem.\n\n#### Logistic Regression Model\n\n​\tWant $0\\le h_\\theta(x)\\le1$\n\n$$\nh_\\theta(x)=g(\\theta^Tx)\n$$\n**Sigmoid function (Logistic function)**: \n$$\ng(z)=\\frac1{1+e^{-z}}\n$$\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" />\n\n> It's graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$\n\nThus,\n$$\nh_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}\n$$\n\n\n> $\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx<0$ then  $h_\\theta(x)=0$\n\n**Interpretation of Hypothesis Output**\n\n$h_\\theta(x)=$ estimated probability that y = 1 on input x\n\nExample: if $x=\\begin{bmatrix}x_0 \\\\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\\\ tumorSize\\end{bmatrix}$\n\n​\t\t\t\t$h_\\theta(x)=0.7$\n\nTell patient that 70% chance of tumor being malignant.\n\n#### Mathematical formula definition of the hypothesis for logistic regression \n\n\"Probability that y=1, given x, parameterized by $\\theta$\": \n$$\nP(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\\\\nP(y=0|x;\\theta)=1-P(y=1|x;\\theta)\n$$\n\n\n### 6.3 Decision boundary\n\n> 决策界限\n\n- What logistic regression hypothesis function is computing?\n\n  \n\nAccording to Logistic regression, \n\nsuppose predict \"$y=1$\" If $h_\\theta(x)\\ge0.5$\n\npredict \"$y=0$\" If $h_\\theta(x)\\le0.5$\n\n#### Decision Boundary\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\nSuppose the variable procedure to be specified. \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$\n\nAnd, $\\theta=\\begin{bmatrix}-3 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nPredict \"$y=1$\"if $-3+x_1+x_2\\ge0$\n\n​\t\t\t         \t\t\t\t$x_1+x_2\\ge3$\n\nThe magenta line is called **Decision Boundary**.\n\n> The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.\n\n#### Non-linear decision boundaries\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\nAssuming hypothesis likes this: \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$\n\nAnd assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nThen, predict \"$y=1$\" if $-1+x_1^2+x_2^2\\ge0$\n\n​                                               $x_1^2+x_2^2\\ge1$\n\n> The training set used to fit the parameters $\\theta$\n\n\n\n### 6.4 Cost function\n\n- How to automatically choose the parameters $\\theta$ to a training set.\n\n- Define the optimization objective or the cost function that used to fit the parameters.\n\n\n\nHere is to supervised learning problem of fitting a logistic regression model.\n\nTraining set: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})\\}$\n\n$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in\\{0,1\\}$\n\n$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\nHow to choose parameters $\\theta$ ? (The next sections will focus on this problem)\n\n\n\n#### Cost function - Logistic regression cost function\n\n​\tLinear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_θ(x^{(i)})-y^{(i)})^2$\n\n​\t$Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$\n\n> 我们先尝试直接将线性回归函数转化为逻辑回归函数，事实上后者将会是一个参数为$\\theta$的非凸函数（non-convex function），这是因为这一部分（$\\frac1{1+e^{-\\theta^T x}}$）是很复杂的非线性函数。\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n**Logistic regression cost function**\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases}\n$$\n\n\nIf y = 1\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\nIf y = 0\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n### 6.5 simplified cost function and gradient descent\n\n- Figure out a simpler way to write the cost function\n\n- Also figure out how to apply gradient descent to fit the parameters of logistic regression\n\n#### Logistic regression cost function\n\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})\n$$\n\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases} \\\\ \\;\\\\\nNote:y=0\\;or\\;1\\;always\n$$\n\nCompress them into one equation:\n\n$$\nCost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))\n$$\n\n\n**Logistic regression cost function**\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})\\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\;\\;\\; \n=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]\n$$\n\n> 为什么用这个函数作为逻辑回归的代价函数：这个式子是从统计学中的极大似然法（the principle maximum likelihood estimation）得来的，它是统计学中为不同模型快速寻找参数的方法。并且它还拥有一个良好的性质：它是凸函数。\n\nTo fit parameters $\\theta$: \n$$\n\\min_\\theta J(\\theta)\n$$\n\n> find the $\\theta$ which minimizes $J(\\theta)$\n\nTo make prediction given new $x$:\n\n​\tOutput $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\n> So how to minimize $J(\\theta)$, or how to choose parameter $ \\theta$ ?\n\n\n\n#### Implementation of logistic regression\n\n**Gradient Descent**\n\n$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$\n\nWhat $\\min_\\theta J(\\theta)$:\t\n\n​\tRepeat {\n\n​\t\t\t\t$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n​\t\t}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.\n\n\n\n### 6.6 Advanced optimization\n\n- Some advanced optimization algorithms \n- Some advanced optimization concepts\n\n> 大大提高逻辑回归的计算速度。:see_no_evil:\n\n#### Optimization algorithm\n\nCost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.\n\nGiven $\\theta$, we have code that compute\n\n- $J(\\theta)$\n- $\\frac∂{∂\\theta_j}J(\\theta)$      (For $j=0,1,…,n$)\n\nOptimization algorithms:\n\n- Gradient descent\n- Conjugate gradient\n- BFGS (共轭梯度法)\n- L-BFGS\n\nOthers algorithm compare to gradient descent\n\n| Advantages                         | Disadvantages |\n| ---------------------------------- | ------------- |\n| No need to manually pick $\\alpha$  | More complex  |\n| Often faster than gradient descent |               |\n\n> These complex algorithms have a \"clever inner-loop\" called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.\n>\n> :older_man:建议不要试图实现这些算法，除非你是数值计算方面的专家，甚至你不必完全理解这些算法就能很好的使用它们。\n\n\n\n### 6.7 Multi-class classification: One-vs-all (unfinished)\n\n- How to get logistic regression to work for multi-class classification problems\n- One-versus-all classification algorithm\n\n#### Multiclass classification\n\n<!--unfinished-->\n\n\n\n## 7 Regularization (unfinished)\n\n> 正则化，是解决（改善）过拟合问题的手段之一\n\n### 7.1 The problem of overfitting\n\n- Explain what is overfitting problem\n\nExample: Linear regression (housing prices)\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n> \"Underfit\"\"High bias\",高偏差：强行用直线拟合曲线分布的数据，就像“持有偏见，固执认为房价变化就是线性的”，导致拟合结果偏差很大\n\n> \"Overfit\"\"High variance\",高方差\n\n#### Overfitting\n\nIf we have too many features, the learned hypothesis may fit the training set very well ($J(θ)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).\n\nExample: Logistic regression\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n#### Addressing overfitting\n\nIf we have la lot of features, and very little training data, then overfitting can become a problem.\n\nTwo main options:\n\n1. Reduce number of features\n   - Manually select which features to keep (but some feature must be abandoned)\n   - Model selection algorithm (later will explain)\n2. Regularization\n   - Keep all the features, but reduce magnitude/values of parameters $\\theta_j$\n   - Works well when we have a lot of features, each of which contributes a bit to prediction $y$.\n\n<!--unfinished-->\n\n\n\n## 8 Neural Networks: Representation\n\n### 8.1 Non-linear hypothesis\n\nWhy we need Neural Networks?\n\n> 当特征很多，线性回归和逻辑回归就不那么好用了。即使他们得出了能够拟合当前样本的结论，该结果也很有可能是过拟合的。\n\nThe neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.\n\n### 8.2 Neurons and the brain\n\n**History of  neural networks**\n\n- Origins: Algorithms that try to mimic the brain.\n- WAs very widely used in 80s and early 90s; popularity diminished in late 90s.\n- Recent resurgence: State-of-the-art technique for many applications.\n\n**The \"one learning algorithm\" hypothesis**\n\n- Neuro-rewiring experiments\n- Sensor representations in the brain\n\n\n\n### 8.3 Model representation I\n\n- How we represent  Neural Networks (hypothesis or model).\n\n#### Neuron model: Logistic unit\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n**Sigmoid (logistic) activation function**\n\n> 这里的激活函数(activation function)是指代非线性函数$g(z)=\\frac1{1+e^{-z}}$的另一个术语\n\nSometimes we add an extra $x_0$ node (if necessary) called bias unit (偏置单元) or the bias neuron (偏置神经元). It's always equal to 1 so sometime we don't draw it.\n\nIn the neural networks literature, the parameters of model $\\theta$ is also called **weights of a model**.\n\n#### Neural Network\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\nLayer 1: Input layer\n\nLayer 2 : Hidden layer\n\nLayer 3: Output layer\n\n$a_i^{(j)}=$ \"activation\" of unit $i$ in layer $j$.\n\n$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$\n\n> 可以形象的看作神经网络被这些矩阵（$\\Theta^{(j)}$）参数化，因而这些矩阵也被称作权重矩阵（matrix of weights）。权重矩阵控制着从某一层到下一层的映射。\n\n$$\na_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\\\\na_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\\\\na_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\\\\nh_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})\n$$\n\nIf networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.\n\n> The superscript $j$ in parentheses means that these values associated with layer $j$\n\n\n\n### 8.4 Model representation II\n\n- How to carry out computation efficiently and show a vectorized implementation.\n- Intuition about why these neural network representation\n\n#### Forward propagation: Vectorized implementation\n\n**Define:**\n\n$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$\n\nThus,\n\n$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$\n\nWe observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.\n\n**Define:**\n$$\nx=\\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)}\\end{bmatrix}\n$$\nFurther towards vectorization: \n$$\nz^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\\\\na^{(2)}=g(z^{(2)})\n$$\n\n>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.\n\nNext, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$\n\n$z^{(3)}=\\Theta^{(2)}a^{(2)}$\n\n$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$\n\n>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.\n\nThis process of computing $h(x)$ is also called **forward propagation**, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.\n\n#### Neural Network learning its own features\n\n> 如果只关注第二层和第三层，那么神经网络的行为类似于逻辑回归。然而神经网络输出层的输入是隐藏层计算或是说学习的结果（$a_1^{(2)},a_2^{(2)},a_3^{(2)}...$），而不是逻辑回归中初始的特征项$x_1,x_2,x_3...$ 。前者较后者更适合作为假设参数，因此神经网络算法具备灵活快速尝试学习任意特征项，处理更多复杂特征的能力。\n\n#### Other network architectures\n\nThe way that neural networks are connected are called the architecture (神经网络的架构). So the architecture refers to how different neurons are connected to each other.\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n### 8.5 Examples and intoitions (unfinished)\n\n- A detail example which shows how a neural network can compute a complex nonlinear function of the input\n\n- Why neural network can be used to learn complex nonlinear hypothesis\n\n\n\n## 9 Neural Networks: Learning (unfinished)\n\n\n\n## 10 Advice for applying machine learning (unfinished)\n\n10.1 Decide what to try next\n\n10.2 Evaluating a hypothesis\n\n10.3 Model selection and training/validation/test sets\n\n\n\n## 11 Machine Learning system design (unfinished)\n\n11.1 Prioritizing what to work on: Spam classification example\n\n\n\n## 12 Support Vector Machines (unfinished)\n\n12.1 Optimizaion object\n\n- Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions\n\n\n\n## 13 Clustering\n\n### 13.0 Notation\n\nTrain set: $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$  (without labels)\n\n\n\n### 13.1 K-means algorithm\n\n> K均值算法\n\nK-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (聚类中心). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.\n\n**Input**:\n\n- $K$ (number of clusters)\n- Training set $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$\n\n$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)\n\n#### K-means algorithm\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\t// <u>cluster assignment step</u>\n\n​\tfor $i$ = 1 to $m$\n\n​\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)\n\n​\t// <u>move centroids step</u>\n\n​\tfor $k$ = 1 to $K$\n\n​\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n> $K$ means the number of centroids and the $k$ means the index of each centriod.\n>\n> If you have a cluster with no points assigned to it, the usual practice is to delete it.\n\n\n\n### 13.2 Optimization objective\n\n- How we can use it to help K-means algorithm find better clusters and avoid local optima.\n\n#### K-means optimization objective\n\n$c^{(i)}$ = index of cluster (1,2,…,$K$) to which example $x^{(i)}$ is currently assigned\n\n$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)\n\n$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n\n#### Optimization objective:\n\n$$\nJ(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\\\\n\\min_{c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K} J(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)\n$$\n\n> The cost function $J$ is also called discotion function\n\n**Review K-means algorithm**\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\n​\t//minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed\n\n​\tfor $i$ = 1 to $m$\n\n​\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$\n\n​\t//minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed\n\n​\tfor $k$ = 1 to $K$\n\n​\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n\n\n### 13.3 Randomly initialization\n\n- How to initialize K-means\n- How to avoid local optima\n\n> Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\n**Randomly initialization**\n\nUsually, we have $K<m$\n\nRandomsly pick $K$ training examples.\n\nSet $\\mu_1,…,\\mu_K$ equal to these $K$ examples.\n\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n> Local optima we should avoid, but randomly initialization may cause this situation.\n\n> 简而言之，为了避免这种情况的发生，我们可以先初始化一千遍，然后选择畸变最小的那一种情况，也就是最有潜力，最不可能陷入局部最优的情况来进行接下来的运算。\n\n\n\n### 13.4 Choosing the number of cluster\n\n- Manual\n- Elbow method\n- Later downstream purpose\n\n\n\n## 14 Dimensionaliy Reduction (unfinished)\n\n14.1 Motivation I: Data Compression\n\n14.2 Motivation II: Visualization\n\n14.3 Principle Component Analysis problem formulation (PCA)\n\n> 主成分分析法\n\n- Compression algorithm\n\n14.4 Principle Component Analysis algorithm\n\nData preprocessing\n\n\n\n## 15 Anomaly Detection (unfinished)\n\n15.1 Problem motivation\n\n\n\n## 16 Recommeder Systems (unfinished)\n\n\n\n## 17 Large Scale Machine Learning (unfinished)\n\n### 17.1 Learning with large datasets\n\n$$\n\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}\n$$\n\n> ​\t对于上亿的数据规模来说，计算梯度下降中的求和函数是难以承受的负担\n\n\n\n### 17.2 Stochastic gradient descent\n\n#### Review linear regression with gradient descent\n\n<!-- unfinished -->\n\n#### Batch gradient descent\n\n#### Stochastic gradient descent\n\n\n\n### 17.3 Mini-batch gradient descent\n\n\n\n\n\n\n\n## Markdown\n\n\\begin{bmatrix}\\end{bmatrix}\n\n\\mathbb R\n\n[公式](https://www.jianshu.com/p/25f0139637b7)\n\n[公式2](https://www.jianshu.com/p/e74eb43960a1)\n\n[语法](https://www.jianshu.com/p/191d1e21f7ed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Machine Learning.md","raw":"\n\n# [Machine Learning](https://www.bilibili.com/video/BV164411b7dx)\n\n\n\n## Index\n\n[toc]\n\n\n\n## 1 Introduction\n\n### 1.1 Supervised Learn\n\nA \"right answer\" given\n\n#### Regression\n\nPredict continuous valued output (e.g. housing price)\n\n**Related algorithms**:\n\n- Linear regression\n- Neural Networks\n- Nearest Neighbor\n\n#### Classification\n\nDiscrete valued output (0 or 1)\n\n**Related algorithms**:\n\n- Logistic regression\n- K-Nearest Neighbor (KNN)\n- Support Vector Machines (SVM)\n- Naïve Bayes\n- Decision Trees\n- Neural Networks\n\n\n\n### 1.2 Unsupervised Learn\n\n#### [Cluster](https://zhuanlan.zhihu.com/p/78382376)\n\n**Applications**:\n\n> 聚类：将相似的对象归到同一个簇中，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。\n\n- Market segmentation\n- Social network analysis\n- Organize computing cluster\n- Astronomical data analysis\n\n[**Related algorithms**](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68):\n\n- K-Means Clustering\n- Mean-Shift Clustering\n- Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n- Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)\n\n\n\n## 2 Linear Regression with One Variable\n\n### 2.0 Model Representation - Notation\n\n$m$: number of training examples\n\n$x$'s: input variable/feature\n\n$y$'s: output variable/feature\n\n$(x,y)$: a training example\n\n$(x^i,y^i)$: $i$ represents the $i^{th}$\n\n\n\n### 2.1 Model and Cost Function\n\n**Hypothesis 假设函数**: \n$$\nh_θ(x)=θ_0+θ_1x\n$$\n**Parameters**: $\\theta_0,\\theta_1$\n\n> $\\theta_1$代表斜率而$\\theta_0$则代表由代价函数计算出的差值\n\n[**Cost Function 代价函数**](https://www.cnblogs.com/geaozhang/p/11442343.html):\n$$\nJ(θ_0,θ_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2\n$$\n\n> minimize squared error cost function (最小化平方差代价函数)\n\n**Goal**: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$\n\n\n\n### 2.2 Parameter Learning - Gradient Descent\n\n#### Outline\n\n- Start with some $\\theta_0,\\theta_1$\n\n- Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum\n\n#### Gradient descent algorithm\n\n​\tRepeat until convergence {\n\n​\t\t$\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$\t(for j=0 and j=1) \n\n}\n\n> $:=$ colon equals, which used to denote assignment (赋值运算符)\n>\n> $\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent\n>\n> $\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term (导数项)\n>\n> **Assert**: simultaneous update $\\theta_0,\\theta_1$ \"at the same time\"\n\n#### Gradient descent intuition\n\n$$\n\\theta_1:=\\theta_1-\\alpha\\frac∂{∂\\theta_j}J(\\theta_1)\n$$\n\n1. $\\alpha$\n\n   if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.\n\n   if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.\n\n2. Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.\n\n3. As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.\n\n#### Gradient descent for linear regression\n\n> Apply gradient descent to minimize squared error cost function \n\n$$\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)=\\frac∂{∂\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$\t\t*Expanding the formula*\n\nSubstituting 0 and 1 into $j$\n\n$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$\n\n$$\\theta_1:j=1:\\frac∂{∂\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$\n\nWhen work out the derivatives, which is the slope of the cost function *J*, plug them back in to gradient descent algorithm (remember to update simultaneously).\n\n**Convex function**\n\n> It doesn't have any local optimum except for the global optimum\n\n**\"Batch\" Gradient Descent**\n\n> \"Batch\": Each step of gradient descent uses all the training examples (entire training set)\n\n\n\n## 3 Linear Algebra review (optional)\n\n### 3.1 Matrices and vectors\n\n#### Matrix\n\n> Rectangular array of numbers\n\n3×2 matrix: $\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\\\ 5&6\\end{bmatrix}$\t2×3 matrix: $\\begin{bmatrix}1 &2&3 \\\\6& 3 & 4\\\\ \\end{bmatrix}$\n\n**Dimension of matrix**: number of rows $\\times$ number of columns\n\n> The above matrix can be also write as $\\mathbb R^{3\\times2}$ \n\n**Refer to specific elements of the matrix **(entries of matrix)\n\n$A=\\begin{bmatrix}1402&191 \\\\ 1371 &821\\\\ 949&1437\\\\147&1448\\end{bmatrix}$\n\n$A_{ij}=$ \"$i$,$j$ entry\" in the $i^{th}$ row, $j^{th}$ column.\n\n$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$\n\n#### Vector\n\n> An $n\\times1$ matrix\n\n$y=\\begin{bmatrix}460\\\\232\\\\315\\\\178\\end{bmatrix}$\n\n$y_i=i^{th}$ element\n\n> It is often customary to use uppercase letters for matrices and lowercase letters for vectors\n\n\n\n### 3.2 Addition and Scalar multiplication\n\n\n\n### 3.3 Matrix-vector multiplication\n\n**Details**:\n\n​            $A$           $\\times$   $x$       $=$       $y$\n\n$\\begin{bmatrix}&&&&&\\\\ \\\\ \\\\ \\end{bmatrix}\\times\\begin{bmatrix}\\\\ \\\\ \\\\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\\\ \\\\ \\\\ \\\\ \\end{bmatrix}$\n\n   m$\\times$n matrix        n$\\times$1    m-dimensional vector\n\nTo get $y_i$, multiply $A$' $i^{th}$ row with elements of vector $x$, and add them up.\n\n**Calculation Tips**：\n\nHouse sizes: 2104,1216, 1534, 852\n\nCompeting hypotheses: $h_\\theta(x)=-40+0.25x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\\\0.25\\end{bmatrix}$ \n\n\n\n### 3.4 Matrix-matrix multiplication\n\n**Details**:\n\n$A\\times B=C$\n\n[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o\n\nThe $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,...,0)\n\n**Example**:\n\n$\\begin{bmatrix}1&3\\\\2&5\\end{bmatrix}\\begin{bmatrix}0&1\\\\3&2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&1\\times1+3\\times2 \\\\2\\times0+5\\times3&2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&7\\\\15&12\\end{bmatrix}$\n\n**Calculation Tips II**：\n\nHouse sizes: 2104,1216, 1534, 852\n\nThree competing hypotheses: \n\n1. $h_\\theta(x)=-40+0.25x$\n2. $h_\\theta(x)=200+0.1x$\n3. $h_\\theta(x)=-150+0.4x$\n\nIt can be calculated as $\\begin{bmatrix}1&2140\\\\1&1416\\\\1&1534\\\\1&852 \\end{bmatrix}\\times\\begin{bmatrix}-40&200&-150 \\\\ 0.25&0.1&0.4 \\end{bmatrix}$\n\n\n\n### 3.5 Matrix multiplication properties\n\nLet $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (**Not commutative**) \n\n#### Identity Matrix\n\nDenoted $I$ (or $I_{n\\times n}$).\n\nExample of identity matrices:\n\n2$\\times$2: $\\begin{bmatrix}1&0 \\\\ 0&1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&0&0 \\\\ 0&1&0 \\\\ 0&0&1\\end{bmatrix}$      $\\cdots$\n\nFor any matrix $A$,\n\n$$\nA\\cdot I=I\\cdot A=A\n$$\n\n> Implicit conditions of the formula: \n>\n> $A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$\n\n\n\n### 3.6 Inverse and Transpose\n\n> 矩阵的逆和矩阵的转置\n\nNot all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&0 \\\\ 0&0\\end{bmatrix}$)\n\n> Matrices that don't have an inverse are \"singular\" or \"degenerate\".\n\n#### Matrix inverse\n\nIf A is an m$\\times$m matrix, and if it has an inverse,\n$$\nAA^{-1}=A^{-1}A=I\n$$\n\n> An m$\\times$m matrix called a square matrix (方阵), only square matrix has an inverse. \n\n#### Matrix transpose\n\nExample:\n$$\nA=\\begin{bmatrix}1&2&0 \\\\ 3&5&9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&3 \\\\ 2&5 \\\\ 0&9 \\end{bmatrix}\n$$\nLet $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.\n\n\n\n## 4 Linear Regression with Multiple Variables\n\n### 4.1 Multiple feature\n\n#### Notation\n\n$n$ = number of features\n\n$x^{(i)}$ = input (features) of $i^{th}$ training example\n\n$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example\n\n#### Multivariate linear regression\n\n> 多元线性回归\n\n**Hypothesis**:\n\n$h_θ(x)=θ_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\nFor convenience of notation, define $x_0=1$. Then\n\n$h_θ(x)=θ_0x_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n​          $=\\theta^Tx$\n\n​          $=\\begin{bmatrix}\\theta_0&\\theta_1&\\cdots&\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix}$\n\n\n\n### 4.2 Gradient descent for multiple variables\n\n- 如何设定假设的参数\n\n- 如何使用梯度下降法来处理多元线性回归\n\n**Hypothesis**: $h_θ(x)=\\theta^Tx=θ_0x_1+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$\n\n**Parameters**: $\\theta_0$,$\\theta_1$,...,$\\theta_n$\n\n**Cost function**: $J(\\theta_0$,$\\theta_1$,...,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$\n\n**Gradient descent**:\n\n​\tRepeat {\n\n​\t\t$\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,...,\\theta_n)$\t\t\n\n}\t\t(simultaneously update for every $j=0,...,n$)\n\n> $J(\\theta_0,...,\\theta_n)$ can be instead by $J(\\theta)$\n\n**New algorithm** for $n\\ge1$:\n\n​\tRepeat {\n\n​\t\t$\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Previously $(n=1)$ is in 2.2 \n\n\n\n### 4.3 Gradient descent in practice I: Feature Scaling\n\n> 梯度下降运算中的实用技巧——特征缩放\n\n#### Feature Scaling\n\n> **Idea**: Make sure flatten are on a similar scale.\n\nE.g. $x_1$= size(0-2000 $feet^2$)\n\n​      $x_2$= number of bedrooms (1-5)\n\nIt will be better to limit both $x_1$ and $x_2$ in [0,1]\n\n$x_1=\\frac{size(feet^2)}{2000}$\n\n$x_2=\\frac{numberOfBedroom}{5}$\n\n> 椭圆相较于正圆需要更多的时间来计算梯度下降\n\nMore general, get every feature into approximately a $-1\\le x_i\\le1$ range.\n\n> 范围的上下限并不是被严格限制的，不过越接近-1和1越好。过大和过小都是不合适的。\n\n#### Mean normalization\n\nReplace $x_i$ with $x_i-µ_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)\n**E.g.** \n\n$x_1=\\frac{size(feet^2)-1000}{2000}$\n\n$x_2=\\frac{numberOfBedroom-2}{5}$\n\n> $µ_i$ (1000 and 2) is considered as the average value of $x_i$ in training set\n\n$$\nx_i\\leftarrow \\frac{x_i-µ_i}{range(max-min)}\n$$\n\n\n\n### 4.4 Gradient descent in practice II: Learning rate\n\n- The chapter will center around the learning rate $\\alpha$\n\n**Gradient descent**\n\n* $\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta)$\n* \"Debugging\": How to make sure gradient descent is working correctly.\n* How to choose learning rate $\\alpha$\n\nDeclare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.\n\n> 这是因为若将$min_\\theta J(\\theta)$作为纵轴，将迭代次数作为横轴，那么得到的是是一个近似$y=|\\frac 1x|$的图像。当迭代次数达到一定量$(\\epsilon)$后，梯度下降的量就几乎可以忽略不计了，所以该测试就判断函数已经收敛。不过要选择一个合适的阈值（threshold，$\\epsilon$）并不容易。\n\n> 如果你的图像并不是上述的样子，那么说明你的$\\alpha$值选取的不恰当。例如图像在0点附近形如指数函数图像，或者呈波浪形，就意味着你的$\\alpha$值过大了，函数无法收敛。\n\n**Summary**:\n\n* If $\\alpha$ is too small: slow convergence.\n* If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.\n\nRecommended choices for $\\alpha$:\n\n..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1,... \n\n\n\n### 4.5 Features and polynomial regression\n\n> 根据特征选择算法以提高效率；使用多项式回归来拟合复杂函数\n\nHousing prices prediction:\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth\n$$\n\n> It's better to use $area$ which is equal to $frontage\\times depth$ as new feature.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1\\times area\n$$\n\n#### Choice of feature\n\nSuppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2\n$$\n\n> 使用二次函数来拟合房价-面积曲线可能在一定范围内是合适的，但是二次函数曲线一定会在达到顶点后下降，房价却不会。所以这并不是一个好的选择。\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3\n$$\n\n> 也许是可行的，但要注意应用特征缩放，从而使三个特征值$(size,(size)^2,(size)^3)$都在大致相同的范围内\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}\n$$\n\n> 上升的曲线，斜率随着x（area）变大逐渐变小，增长趋于平缓，似乎也不错\n\n**Summary**:\n\nYou have a choice in what features to use to fix more complex functions to your data!\n\n\n\n### 4.6 Normal equation (unfinished)\n\n> 对于某些线性回归问题，求取参数$\\theta$最优值的方法。不同与以往的迭代算法（梯度下降的多次迭代来收敛到全局最小值），正规方程提供了一种解析解法，一次性求解$\\theta$的最优值。\n\n**Normal equation**: Method to solve for $\\theta$ analytically.\n\n\n\n#### Compare to Gradient Descent\n\n$m$ training example, $n$ features.\n\n| Gradient Descent            | Normal Equation               |\n| --------------------------- | ----------------------------- |\n| Need to choice a $\\alpha$   | No need to choice a $\\alpha$  |\n| Needs many iterations       | Don't need to iterate         |\n| Work well even $n$ is large | Need to compute $(X^TX)^{-1}$ |\n|                             | Slow if $n$ is very large     |\n\n\n$$\n\\theta=(X^TX)^{-1}X^Ty\n$$\n$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$\n\n**Octave**: `pinv(X'*X)*X'*y` \n\n> `X'` is the transpose of $X$\n>\n> `pinv` is a function  to compute the inverse of a matrix\n\n\n\n<!--unfinished-->\n\n### 4.7 Normal equation and non-invertibility (optional) (unfinished)\n\n<!--unfinished-->\n\n\n\n## 5 Octave Tutorial (ignored)\n\n<!--unfinished-->\n\n\n\n## 6 Logistic Regression\n\n> 回归算法\n\n### 6.1 Classification\n\n**Classification**\n\n$y\\in \\{0,1\\} $\n\n0: \"Negative Class\" (e.g., benign tumor)\n\n1: \"Positive Class\" (e.g., malignant tumor)\n\n> There are multi-class problems as well that y can take value from 0, 1, 2, 3,...\n\nLearning regression isn't fit the classification problem\n\nIn the [video](https://www.bilibili.com/video/BV164411b7dx?p=32&t=160) there is an example to explain it. \n\nAnother example:\n\nClassification: y = 0 or 1\n\n​\t$H_\\theta(x)$ can be $>1$ or $<0$ if we use the linear regression\n\n> Obviously, the label is either 0 or 1.\n\nLogistic Regression: $0\\le h_\\theta(x)\\le1$\n\n> This is a classification algorithm whose output always between 1 and 0. Besides, it's a classification algorithm instead of linear regression algorithm though there is a \"regression\" in its name.\n\n\n\n### 6.2 Hypothesis Representation\n\n> 假设陈述\n\n- What is the function we're going to use to representation hypothesis when we have a classification problem.\n\n#### Logistic Regression Model\n\n​\tWant $0\\le h_\\theta(x)\\le1$\n\n$$\nh_\\theta(x)=g(\\theta^Tx)\n$$\n**Sigmoid function (Logistic function)**: \n$$\ng(z)=\\frac1{1+e^{-z}}\n$$\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" />\n\n> It's graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$\n\nThus,\n$$\nh_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}\n$$\n\n\n> $\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx<0$ then  $h_\\theta(x)=0$\n\n**Interpretation of Hypothesis Output**\n\n$h_\\theta(x)=$ estimated probability that y = 1 on input x\n\nExample: if $x=\\begin{bmatrix}x_0 \\\\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\\\ tumorSize\\end{bmatrix}$\n\n​\t\t\t\t$h_\\theta(x)=0.7$\n\nTell patient that 70% chance of tumor being malignant.\n\n#### Mathematical formula definition of the hypothesis for logistic regression \n\n\"Probability that y=1, given x, parameterized by $\\theta$\": \n$$\nP(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\\\\nP(y=0|x;\\theta)=1-P(y=1|x;\\theta)\n$$\n\n\n### 6.3 Decision boundary\n\n> 决策界限\n\n- What logistic regression hypothesis function is computing?\n\n  \n\nAccording to Logistic regression, \n\nsuppose predict \"$y=1$\" If $h_\\theta(x)\\ge0.5$\n\npredict \"$y=0$\" If $h_\\theta(x)\\le0.5$\n\n#### Decision Boundary\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\nSuppose the variable procedure to be specified. \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$\n\nAnd, $\\theta=\\begin{bmatrix}-3 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nPredict \"$y=1$\"if $-3+x_1+x_2\\ge0$\n\n​\t\t\t         \t\t\t\t$x_1+x_2\\ge3$\n\nThe magenta line is called **Decision Boundary**.\n\n> The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.\n\n#### Non-linear decision boundaries\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\nAssuming hypothesis likes this: \n\n$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$\n\nAnd assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 1\\end{bmatrix}$\n\nThen, predict \"$y=1$\" if $-1+x_1^2+x_2^2\\ge0$\n\n​                                               $x_1^2+x_2^2\\ge1$\n\n> The training set used to fit the parameters $\\theta$\n\n\n\n### 6.4 Cost function\n\n- How to automatically choose the parameters $\\theta$ to a training set.\n\n- Define the optimization objective or the cost function that used to fit the parameters.\n\n\n\nHere is to supervised learning problem of fitting a logistic regression model.\n\nTraining set: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})\\}$\n\n$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in\\{0,1\\}$\n\n$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\nHow to choose parameters $\\theta$ ? (The next sections will focus on this problem)\n\n\n\n#### Cost function - Logistic regression cost function\n\n​\tLinear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_θ(x^{(i)})-y^{(i)})^2$\n\n​\t$Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$\n\n> 我们先尝试直接将线性回归函数转化为逻辑回归函数，事实上后者将会是一个参数为$\\theta$的非凸函数（non-convex function），这是因为这一部分（$\\frac1{1+e^{-\\theta^T x}}$）是很复杂的非线性函数。\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n**Logistic regression cost function**\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases}\n$$\n\n\nIf y = 1\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\nIf y = 0\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n### 6.5 simplified cost function and gradient descent\n\n- Figure out a simpler way to write the cost function\n\n- Also figure out how to apply gradient descent to fit the parameters of logistic regression\n\n#### Logistic regression cost function\n\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})\n$$\n\n$$\nCost(h_\\theta(x),y)=\n\\begin{cases}\n-\\log(h_\\theta(x)) \\quad & if \\;y=1  \\\\[1ex]\n-\\log(1-h_\\theta(x))\\quad & if \\;y=0\n\\end{cases} \\\\ \\;\\\\\nNote:y=0\\;or\\;1\\;always\n$$\n\nCompress them into one equation:\n\n$$\nCost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))\n$$\n\n\n**Logistic regression cost function**\n$$\nJ(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})\\\\\n\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\;\\;\\; \n=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]\n$$\n\n> 为什么用这个函数作为逻辑回归的代价函数：这个式子是从统计学中的极大似然法（the principle maximum likelihood estimation）得来的，它是统计学中为不同模型快速寻找参数的方法。并且它还拥有一个良好的性质：它是凸函数。\n\nTo fit parameters $\\theta$: \n$$\n\\min_\\theta J(\\theta)\n$$\n\n> find the $\\theta$ which minimizes $J(\\theta)$\n\nTo make prediction given new $x$:\n\n​\tOutput $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$\n\n> So how to minimize $J(\\theta)$, or how to choose parameter $ \\theta$ ?\n\n\n\n#### Implementation of logistic regression\n\n**Gradient Descent**\n\n$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$\n\nWhat $\\min_\\theta J(\\theta)$:\t\n\n​\tRepeat {\n\n​\t\t\t\t$\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$\t\t\n\n​\t\t}\t\t(simultaneously update for $\\theta_j$ for $j=0,...,n$)\n\n> Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.\n\n\n\n### 6.6 Advanced optimization\n\n- Some advanced optimization algorithms \n- Some advanced optimization concepts\n\n> 大大提高逻辑回归的计算速度。:see_no_evil:\n\n#### Optimization algorithm\n\nCost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.\n\nGiven $\\theta$, we have code that compute\n\n- $J(\\theta)$\n- $\\frac∂{∂\\theta_j}J(\\theta)$      (For $j=0,1,…,n$)\n\nOptimization algorithms:\n\n- Gradient descent\n- Conjugate gradient\n- BFGS (共轭梯度法)\n- L-BFGS\n\nOthers algorithm compare to gradient descent\n\n| Advantages                         | Disadvantages |\n| ---------------------------------- | ------------- |\n| No need to manually pick $\\alpha$  | More complex  |\n| Often faster than gradient descent |               |\n\n> These complex algorithms have a \"clever inner-loop\" called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.\n>\n> :older_man:建议不要试图实现这些算法，除非你是数值计算方面的专家，甚至你不必完全理解这些算法就能很好的使用它们。\n\n\n\n### 6.7 Multi-class classification: One-vs-all (unfinished)\n\n- How to get logistic regression to work for multi-class classification problems\n- One-versus-all classification algorithm\n\n#### Multiclass classification\n\n<!--unfinished-->\n\n\n\n## 7 Regularization (unfinished)\n\n> 正则化，是解决（改善）过拟合问题的手段之一\n\n### 7.1 The problem of overfitting\n\n- Explain what is overfitting problem\n\nExample: Linear regression (housing prices)\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n> \"Underfit\"\"High bias\",高偏差：强行用直线拟合曲线分布的数据，就像“持有偏见，固执认为房价变化就是线性的”，导致拟合结果偏差很大\n\n> \"Overfit\"\"High variance\",高方差\n\n#### Overfitting\n\nIf we have too many features, the learned hypothesis may fit the training set very well ($J(θ)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).\n\nExample: Logistic regression\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n#### Addressing overfitting\n\nIf we have la lot of features, and very little training data, then overfitting can become a problem.\n\nTwo main options:\n\n1. Reduce number of features\n   - Manually select which features to keep (but some feature must be abandoned)\n   - Model selection algorithm (later will explain)\n2. Regularization\n   - Keep all the features, but reduce magnitude/values of parameters $\\theta_j$\n   - Works well when we have a lot of features, each of which contributes a bit to prediction $y$.\n\n<!--unfinished-->\n\n\n\n## 8 Neural Networks: Representation\n\n### 8.1 Non-linear hypothesis\n\nWhy we need Neural Networks?\n\n> 当特征很多，线性回归和逻辑回归就不那么好用了。即使他们得出了能够拟合当前样本的结论，该结果也很有可能是过拟合的。\n\nThe neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.\n\n### 8.2 Neurons and the brain\n\n**History of  neural networks**\n\n- Origins: Algorithms that try to mimic the brain.\n- WAs very widely used in 80s and early 90s; popularity diminished in late 90s.\n- Recent resurgence: State-of-the-art technique for many applications.\n\n**The \"one learning algorithm\" hypothesis**\n\n- Neuro-rewiring experiments\n- Sensor representations in the brain\n\n\n\n### 8.3 Model representation I\n\n- How we represent  Neural Networks (hypothesis or model).\n\n#### Neuron model: Logistic unit\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n**Sigmoid (logistic) activation function**\n\n> 这里的激活函数(activation function)是指代非线性函数$g(z)=\\frac1{1+e^{-z}}$的另一个术语\n\nSometimes we add an extra $x_0$ node (if necessary) called bias unit (偏置单元) or the bias neuron (偏置神经元). It's always equal to 1 so sometime we don't draw it.\n\nIn the neural networks literature, the parameters of model $\\theta$ is also called **weights of a model**.\n\n#### Neural Network\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\nLayer 1: Input layer\n\nLayer 2 : Hidden layer\n\nLayer 3: Output layer\n\n$a_i^{(j)}=$ \"activation\" of unit $i$ in layer $j$.\n\n$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$\n\n> 可以形象的看作神经网络被这些矩阵（$\\Theta^{(j)}$）参数化，因而这些矩阵也被称作权重矩阵（matrix of weights）。权重矩阵控制着从某一层到下一层的映射。\n\n$$\na_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\\\\na_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\\\\na_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\\\\nh_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})\n$$\n\nIf networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.\n\n> The superscript $j$ in parentheses means that these values associated with layer $j$\n\n\n\n### 8.4 Model representation II\n\n- How to carry out computation efficiently and show a vectorized implementation.\n- Intuition about why these neural network representation\n\n#### Forward propagation: Vectorized implementation\n\n**Define:**\n\n$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$\n\nThus,\n\n$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$\n\nWe observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.\n\n**Define:**\n$$\nx=\\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\\\ z_2^{(2)} \\\\ z_3^{(2)}\\end{bmatrix}\n$$\nFurther towards vectorization: \n$$\nz^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\\\\na^{(2)}=g(z^{(2)})\n$$\n\n>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.\n\nNext, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$\n\n$z^{(3)}=\\Theta^{(2)}a^{(2)}$\n\n$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$\n\n>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.\n\nThis process of computing $h(x)$ is also called **forward propagation**, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.\n\n#### Neural Network learning its own features\n\n> 如果只关注第二层和第三层，那么神经网络的行为类似于逻辑回归。然而神经网络输出层的输入是隐藏层计算或是说学习的结果（$a_1^{(2)},a_2^{(2)},a_3^{(2)}...$），而不是逻辑回归中初始的特征项$x_1,x_2,x_3...$ 。前者较后者更适合作为假设参数，因此神经网络算法具备灵活快速尝试学习任意特征项，处理更多复杂特征的能力。\n\n#### Other network architectures\n\nThe way that neural networks are connected are called the architecture (神经网络的架构). So the architecture refers to how different neurons are connected to each other.\n\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n### 8.5 Examples and intoitions (unfinished)\n\n- A detail example which shows how a neural network can compute a complex nonlinear function of the input\n\n- Why neural network can be used to learn complex nonlinear hypothesis\n\n\n\n## 9 Neural Networks: Learning (unfinished)\n\n\n\n## 10 Advice for applying machine learning (unfinished)\n\n10.1 Decide what to try next\n\n10.2 Evaluating a hypothesis\n\n10.3 Model selection and training/validation/test sets\n\n\n\n## 11 Machine Learning system design (unfinished)\n\n11.1 Prioritizing what to work on: Spam classification example\n\n\n\n## 12 Support Vector Machines (unfinished)\n\n12.1 Optimizaion object\n\n- Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions\n\n\n\n## 13 Clustering\n\n### 13.0 Notation\n\nTrain set: $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$  (without labels)\n\n\n\n### 13.1 K-means algorithm\n\n> K均值算法\n\nK-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (聚类中心). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.\n\n**Input**:\n\n- $K$ (number of clusters)\n- Training set $\\{x^{(1)},x^{(2)},x^{(3)},...x^{(m)}\\}$\n\n$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)\n\n#### K-means algorithm\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\t// <u>cluster assignment step</u>\n\n​\tfor $i$ = 1 to $m$\n\n​\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)\n\n​\t// <u>move centroids step</u>\n\n​\tfor $k$ = 1 to $K$\n\n​\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n> $K$ means the number of centroids and the $k$ means the index of each centriod.\n>\n> If you have a cluster with no points assigned to it, the usual practice is to delete it.\n\n\n\n### 13.2 Optimization objective\n\n- How we can use it to help K-means algorithm find better clusters and avoid local optima.\n\n#### K-means optimization objective\n\n$c^{(i)}$ = index of cluster (1,2,…,$K$) to which example $x^{(i)}$ is currently assigned\n\n$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)\n\n$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n\n#### Optimization objective:\n\n$$\nJ(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\\\\n\\min_{c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K} J(c^{(1)},... c^{(m)},\\mu_1,...,\\mu_K)\n$$\n\n> The cost function $J$ is also called discotion function\n\n**Review K-means algorithm**\n\nRandomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\nRepeat {\n\n​\t//minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed\n\n​\tfor $i$ = 1 to $m$\n\n​\t\t$c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$\n\n​\t//minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed\n\n​\tfor $k$ = 1 to $K$\n\n​\t\t$\\mu_k$ := average (mean) of points assigned to cluster $k$ \n}\n\n\n\n### 13.3 Randomly initialization\n\n- How to initialize K-means\n- How to avoid local optima\n\n> Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,...,\\mu_K \\in \\mathbb R^n$\n\n**Randomly initialization**\n\nUsually, we have $K<m$\n\nRandomsly pick $K$ training examples.\n\nSet $\\mu_1,…,\\mu_K$ equal to these $K$ examples.\n\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n> Local optima we should avoid, but randomly initialization may cause this situation.\n\n> 简而言之，为了避免这种情况的发生，我们可以先初始化一千遍，然后选择畸变最小的那一种情况，也就是最有潜力，最不可能陷入局部最优的情况来进行接下来的运算。\n\n\n\n### 13.4 Choosing the number of cluster\n\n- Manual\n- Elbow method\n- Later downstream purpose\n\n\n\n## 14 Dimensionaliy Reduction (unfinished)\n\n14.1 Motivation I: Data Compression\n\n14.2 Motivation II: Visualization\n\n14.3 Principle Component Analysis problem formulation (PCA)\n\n> 主成分分析法\n\n- Compression algorithm\n\n14.4 Principle Component Analysis algorithm\n\nData preprocessing\n\n\n\n## 15 Anomaly Detection (unfinished)\n\n15.1 Problem motivation\n\n\n\n## 16 Recommeder Systems (unfinished)\n\n\n\n## 17 Large Scale Machine Learning (unfinished)\n\n### 17.1 Learning with large datasets\n\n$$\n\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}\n$$\n\n> ​\t对于上亿的数据规模来说，计算梯度下降中的求和函数是难以承受的负担\n\n\n\n### 17.2 Stochastic gradient descent\n\n#### Review linear regression with gradient descent\n\n<!-- unfinished -->\n\n#### Batch gradient descent\n\n#### Stochastic gradient descent\n\n\n\n### 17.3 Mini-batch gradient descent\n\n\n\n\n\n\n\n## Markdown\n\n\\begin{bmatrix}\\end{bmatrix}\n\n\\mathbb R\n\n[公式](https://www.jianshu.com/p/25f0139637b7)\n\n[公式2](https://www.jianshu.com/p/e74eb43960a1)\n\n[语法](https://www.jianshu.com/p/191d1e21f7ed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Machine Learning","published":1,"date":"2021-05-25T08:58:33.000Z","updated":"2021-06-13T01:25:59.000Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"ckqkodcle0001033t13up2na6","content":"<h1 id=\"Machine-Learning\"><a href=\"#Machine-Learning\" class=\"headerlink\" title=\"Machine Learning\"></a><a href=\"https://www.bilibili.com/video/BV164411b7dx\">Machine Learning</a></h1><h2 id=\"Index\"><a href=\"#Index\" class=\"headerlink\" title=\"Index\"></a>Index</h2><p>[toc]</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 Introduction\"></a>1 Introduction</h2><h3 id=\"1-1-Supervised-Learn\"><a href=\"#1-1-Supervised-Learn\" class=\"headerlink\" title=\"1.1 Supervised Learn\"></a>1.1 Supervised Learn</h3><p>A “right answer” given</p>\n<h4 id=\"Regression\"><a href=\"#Regression\" class=\"headerlink\" title=\"Regression\"></a>Regression</h4><p>Predict continuous valued output (e.g. housing price)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Linear regression</li>\n<li>Neural Networks</li>\n<li>Nearest Neighbor</li>\n</ul>\n<h4 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h4><p>Discrete valued output (0 or 1)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Logistic regression</li>\n<li>K-Nearest Neighbor (KNN)</li>\n<li>Support Vector Machines (SVM)</li>\n<li>Naïve Bayes</li>\n<li>Decision Trees</li>\n<li>Neural Networks</li>\n</ul>\n<h3 id=\"1-2-Unsupervised-Learn\"><a href=\"#1-2-Unsupervised-Learn\" class=\"headerlink\" title=\"1.2 Unsupervised Learn\"></a>1.2 Unsupervised Learn</h3><h4 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a><a href=\"https://zhuanlan.zhihu.com/p/78382376\">Cluster</a></h4><p><strong>Applications</strong>:</p>\n<blockquote>\n<p>聚类：将相似的对象归到同一个簇中，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。</p>\n</blockquote>\n<ul>\n<li>Market segmentation</li>\n<li>Social network analysis</li>\n<li>Organize computing cluster</li>\n<li>Astronomical data analysis</li>\n</ul>\n<p><a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\"><strong>Related algorithms</strong></a>:</p>\n<ul>\n<li>K-Means Clustering</li>\n<li>Mean-Shift Clustering</li>\n<li>Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</li>\n<li>Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)</li>\n</ul>\n<h2 id=\"2-Linear-Regression-with-One-Variable\"><a href=\"#2-Linear-Regression-with-One-Variable\" class=\"headerlink\" title=\"2 Linear Regression with One Variable\"></a>2 Linear Regression with One Variable</h2><h3 id=\"2-0-Model-Representation-Notation\"><a href=\"#2-0-Model-Representation-Notation\" class=\"headerlink\" title=\"2.0 Model Representation - Notation\"></a>2.0 Model Representation - Notation</h3><p>$m$: number of training examples</p>\n<p>$x$’s: input variable/feature</p>\n<p>$y$’s: output variable/feature</p>\n<p>$(x,y)$: a training example</p>\n<p>$(x^i,y^i)$: $i$ represents the $i^{th}$</p>\n<h3 id=\"2-1-Model-and-Cost-Function\"><a href=\"#2-1-Model-and-Cost-Function\" class=\"headerlink\" title=\"2.1 Model and Cost Function\"></a>2.1 Model and Cost Function</h3><p><strong>Hypothesis 假设函数</strong>:<br>$$<br>h_θ(x)=θ_0+θ_1x<br>$$<br><strong>Parameters</strong>: $\\theta_0,\\theta_1$</p>\n<blockquote>\n<p>$\\theta_1$代表斜率而$\\theta_0$则代表由代价函数计算出的差值</p>\n</blockquote>\n<p><a href=\"https://www.cnblogs.com/geaozhang/p/11442343.html\"><strong>Cost Function 代价函数</strong></a>:<br>$$<br>J(θ_0,θ_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2<br>$$</p>\n<blockquote>\n<p>minimize squared error cost function (最小化平方差代价函数)</p>\n</blockquote>\n<p><strong>Goal</strong>: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$</p>\n<h3 id=\"2-2-Parameter-Learning-Gradient-Descent\"><a href=\"#2-2-Parameter-Learning-Gradient-Descent\" class=\"headerlink\" title=\"2.2 Parameter Learning - Gradient Descent\"></a>2.2 Parameter Learning - Gradient Descent</h3><h4 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h4><ul>\n<li><p>Start with some $\\theta_0,\\theta_1$</p>\n</li>\n<li><p>Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum</p>\n</li>\n</ul>\n<h4 id=\"Gradient-descent-algorithm\"><a href=\"#Gradient-descent-algorithm\" class=\"headerlink\" title=\"Gradient descent algorithm\"></a>Gradient descent algorithm</h4><p>​    Repeat until convergence {</p>\n<p>​        $\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$    (for j=0 and j=1) </p>\n<p>}</p>\n<blockquote>\n<p>$:=$ colon equals, which used to denote assignment (赋值运算符)</p>\n<p>$\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent</p>\n<p>$\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term (导数项)</p>\n<p><strong>Assert</strong>: simultaneous update $\\theta_0,\\theta_1$ “at the same time”</p>\n</blockquote>\n<h4 id=\"Gradient-descent-intuition\"><a href=\"#Gradient-descent-intuition\" class=\"headerlink\" title=\"Gradient descent intuition\"></a>Gradient descent intuition</h4><p>$$<br>\\theta_1:=\\theta_1-\\alpha\\frac∂{∂\\theta_j}J(\\theta_1)<br>$$</p>\n<ol>\n<li><p>$\\alpha$</p>\n<p>if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.</p>\n<p>if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.</p>\n</li>\n<li><p>Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.</p>\n</li>\n<li><p>As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.</p>\n</li>\n</ol>\n<h4 id=\"Gradient-descent-for-linear-regression\"><a href=\"#Gradient-descent-for-linear-regression\" class=\"headerlink\" title=\"Gradient descent for linear regression\"></a>Gradient descent for linear regression</h4><blockquote>\n<p>Apply gradient descent to minimize squared error cost function </p>\n</blockquote>\n<p>$$\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)=\\frac∂{∂\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$        <em>Expanding the formula</em></p>\n<p>Substituting 0 and 1 into $j$</p>\n<p>$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$</p>\n<p>$$\\theta_1:j=1:\\frac∂{∂\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$</p>\n<p>When work out the derivatives, which is the slope of the cost function <em>J</em>, plug them back in to gradient descent algorithm (remember to update simultaneously).</p>\n<p><strong>Convex function</strong></p>\n<blockquote>\n<p>It doesn’t have any local optimum except for the global optimum</p>\n</blockquote>\n<p><strong>“Batch” Gradient Descent</strong></p>\n<blockquote>\n<p>“Batch”: Each step of gradient descent uses all the training examples (entire training set)</p>\n</blockquote>\n<h2 id=\"3-Linear-Algebra-review-optional\"><a href=\"#3-Linear-Algebra-review-optional\" class=\"headerlink\" title=\"3 Linear Algebra review (optional)\"></a>3 Linear Algebra review (optional)</h2><h3 id=\"3-1-Matrices-and-vectors\"><a href=\"#3-1-Matrices-and-vectors\" class=\"headerlink\" title=\"3.1 Matrices and vectors\"></a>3.1 Matrices and vectors</h3><h4 id=\"Matrix\"><a href=\"#Matrix\" class=\"headerlink\" title=\"Matrix\"></a>Matrix</h4><blockquote>\n<p>Rectangular array of numbers</p>\n</blockquote>\n<p>3×2 matrix: $\\begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\\ 5&amp;6\\end{bmatrix}$    2×3 matrix: $\\begin{bmatrix}1 &amp;2&amp;3 \\6&amp; 3 &amp; 4\\ \\end{bmatrix}$</p>\n<p><strong>Dimension of matrix</strong>: number of rows $\\times$ number of columns</p>\n<blockquote>\n<p>The above matrix can be also write as $\\mathbb R^{3\\times2}$ </p>\n</blockquote>\n<p>**Refer to specific elements of the matrix **(entries of matrix)</p>\n<p>$A=\\begin{bmatrix}1402&amp;191 \\ 1371 &amp;821\\ 949&amp;1437\\147&amp;1448\\end{bmatrix}$</p>\n<p>$A_{ij}=$ “$i$,$j$ entry” in the $i^{th}$ row, $j^{th}$ column.</p>\n<p>$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$</p>\n<h4 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h4><blockquote>\n<p>An $n\\times1$ matrix</p>\n</blockquote>\n<p>$y=\\begin{bmatrix}460\\232\\315\\178\\end{bmatrix}$</p>\n<p>$y_i=i^{th}$ element</p>\n<blockquote>\n<p>It is often customary to use uppercase letters for matrices and lowercase letters for vectors</p>\n</blockquote>\n<h3 id=\"3-2-Addition-and-Scalar-multiplication\"><a href=\"#3-2-Addition-and-Scalar-multiplication\" class=\"headerlink\" title=\"3.2 Addition and Scalar multiplication\"></a>3.2 Addition and Scalar multiplication</h3><h3 id=\"3-3-Matrix-vector-multiplication\"><a href=\"#3-3-Matrix-vector-multiplication\" class=\"headerlink\" title=\"3.3 Matrix-vector multiplication\"></a>3.3 Matrix-vector multiplication</h3><p><strong>Details</strong>:</p>\n<p>​            $A$           $\\times$   $x$       $=$       $y$</p>\n<p>$\\begin{bmatrix}&amp;&amp;&amp;&amp;&amp;\\ \\ \\ \\end{bmatrix}\\times\\begin{bmatrix}\\ \\ \\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\ \\ \\ \\ \\end{bmatrix}$</p>\n<p>   m$\\times$n matrix        n$\\times$1    m-dimensional vector</p>\n<p>To get $y_i$, multiply $A$’ $i^{th}$ row with elements of vector $x$, and add them up.</p>\n<p><strong>Calculation Tips</strong>：</p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Competing hypotheses: $h_\\theta(x)=-40+0.25x$</p>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\0.25\\end{bmatrix}$ </p>\n<h3 id=\"3-4-Matrix-matrix-multiplication\"><a href=\"#3-4-Matrix-matrix-multiplication\" class=\"headerlink\" title=\"3.4 Matrix-matrix multiplication\"></a>3.4 Matrix-matrix multiplication</h3><p><strong>Details</strong>:</p>\n<p>$A\\times B=C$</p>\n<p>[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o</p>\n<p>The $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,…,0)</p>\n<p><strong>Example</strong>:</p>\n<p>$\\begin{bmatrix}1&amp;3\\2&amp;5\\end{bmatrix}\\begin{bmatrix}0&amp;1\\3&amp;2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&amp;1\\times1+3\\times2 \\2\\times0+5\\times3&amp;2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&amp;7\\15&amp;12\\end{bmatrix}$</p>\n<p><strong>Calculation Tips II</strong>：</p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Three competing hypotheses: </p>\n<ol>\n<li>$h_\\theta(x)=-40+0.25x$</li>\n<li>$h_\\theta(x)=200+0.1x$</li>\n<li>$h_\\theta(x)=-150+0.4x$</li>\n</ol>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40&amp;200&amp;-150 \\ 0.25&amp;0.1&amp;0.4 \\end{bmatrix}$</p>\n<h3 id=\"3-5-Matrix-multiplication-properties\"><a href=\"#3-5-Matrix-multiplication-properties\" class=\"headerlink\" title=\"3.5 Matrix multiplication properties\"></a>3.5 Matrix multiplication properties</h3><p>Let $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (<strong>Not commutative</strong>) </p>\n<h4 id=\"Identity-Matrix\"><a href=\"#Identity-Matrix\" class=\"headerlink\" title=\"Identity Matrix\"></a>Identity Matrix</h4><p>Denoted $I$ (or $I_{n\\times n}$).</p>\n<p>Example of identity matrices:</p>\n<p>2$\\times$2: $\\begin{bmatrix}1&amp;0 \\ 0&amp;1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&amp;0&amp;0 \\ 0&amp;1&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$      $\\cdots$</p>\n<p>For any matrix $A$,</p>\n<p>$$<br>A\\cdot I=I\\cdot A=A<br>$$</p>\n<blockquote>\n<p>Implicit conditions of the formula: </p>\n<p>$A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$</p>\n</blockquote>\n<h3 id=\"3-6-Inverse-and-Transpose\"><a href=\"#3-6-Inverse-and-Transpose\" class=\"headerlink\" title=\"3.6 Inverse and Transpose\"></a>3.6 Inverse and Transpose</h3><blockquote>\n<p>矩阵的逆和矩阵的转置</p>\n</blockquote>\n<p>Not all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&amp;0 \\ 0&amp;0\\end{bmatrix}$)</p>\n<blockquote>\n<p>Matrices that don’t have an inverse are “singular” or “degenerate”.</p>\n</blockquote>\n<h4 id=\"Matrix-inverse\"><a href=\"#Matrix-inverse\" class=\"headerlink\" title=\"Matrix inverse\"></a>Matrix inverse</h4><p>If A is an m$\\times$m matrix, and if it has an inverse,<br>$$<br>AA^{-1}=A^{-1}A=I<br>$$</p>\n<blockquote>\n<p>An m$\\times$m matrix called a square matrix (方阵), only square matrix has an inverse. </p>\n</blockquote>\n<h4 id=\"Matrix-transpose\"><a href=\"#Matrix-transpose\" class=\"headerlink\" title=\"Matrix transpose\"></a>Matrix transpose</h4><p>Example:<br>$$<br>A=\\begin{bmatrix}1&amp;2&amp;0 \\ 3&amp;5&amp;9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&amp;3 \\ 2&amp;5 \\ 0&amp;9 \\end{bmatrix}<br>$$<br>Let $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.</p>\n<h2 id=\"4-Linear-Regression-with-Multiple-Variables\"><a href=\"#4-Linear-Regression-with-Multiple-Variables\" class=\"headerlink\" title=\"4 Linear Regression with Multiple Variables\"></a>4 Linear Regression with Multiple Variables</h2><h3 id=\"4-1-Multiple-feature\"><a href=\"#4-1-Multiple-feature\" class=\"headerlink\" title=\"4.1 Multiple feature\"></a>4.1 Multiple feature</h3><h4 id=\"Notation\"><a href=\"#Notation\" class=\"headerlink\" title=\"Notation\"></a>Notation</h4><p>$n$ = number of features</p>\n<p>$x^{(i)}$ = input (features) of $i^{th}$ training example</p>\n<p>$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example</p>\n<h4 id=\"Multivariate-linear-regression\"><a href=\"#Multivariate-linear-regression\" class=\"headerlink\" title=\"Multivariate linear regression\"></a>Multivariate linear regression</h4><blockquote>\n<p>多元线性回归</p>\n</blockquote>\n<p><strong>Hypothesis</strong>:</p>\n<p>$h_θ(x)=θ_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>For convenience of notation, define $x_0=1$. Then</p>\n<p>$h_θ(x)=θ_0x_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>​          $=\\theta^Tx$</p>\n<p>​          $=\\begin{bmatrix}\\theta_0&amp;\\theta_1&amp;\\cdots&amp;\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix}$</p>\n<h3 id=\"4-2-Gradient-descent-for-multiple-variables\"><a href=\"#4-2-Gradient-descent-for-multiple-variables\" class=\"headerlink\" title=\"4.2 Gradient descent for multiple variables\"></a>4.2 Gradient descent for multiple variables</h3><ul>\n<li><p>如何设定假设的参数</p>\n</li>\n<li><p>如何使用梯度下降法来处理多元线性回归</p>\n</li>\n</ul>\n<p><strong>Hypothesis</strong>: $h_θ(x)=\\theta^Tx=θ_0x_1+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p><strong>Parameters</strong>: $\\theta_0$,$\\theta_1$,…,$\\theta_n$</p>\n<p><strong>Cost function</strong>: $J(\\theta_0$,$\\theta_1$,…,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$</p>\n<p><strong>Gradient descent</strong>:</p>\n<p>​    Repeat {</p>\n<p>​        $\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,…,\\theta_n)$        </p>\n<p>}        (simultaneously update for every $j=0,…,n$)</p>\n<blockquote>\n<p>$J(\\theta_0,…,\\theta_n)$ can be instead by $J(\\theta)$</p>\n</blockquote>\n<p><strong>New algorithm</strong> for $n\\ge1$:</p>\n<p>​    Repeat {</p>\n<p>​        $\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>}        (simultaneously update for $\\theta_j$ for $j=0,…,n$)</p>\n<blockquote>\n<p>Previously $(n=1)$ is in 2.2 </p>\n</blockquote>\n<h3 id=\"4-3-Gradient-descent-in-practice-I-Feature-Scaling\"><a href=\"#4-3-Gradient-descent-in-practice-I-Feature-Scaling\" class=\"headerlink\" title=\"4.3 Gradient descent in practice I: Feature Scaling\"></a>4.3 Gradient descent in practice I: Feature Scaling</h3><blockquote>\n<p>梯度下降运算中的实用技巧——特征缩放</p>\n</blockquote>\n<h4 id=\"Feature-Scaling\"><a href=\"#Feature-Scaling\" class=\"headerlink\" title=\"Feature Scaling\"></a>Feature Scaling</h4><blockquote>\n<p><strong>Idea</strong>: Make sure flatten are on a similar scale.</p>\n</blockquote>\n<p>E.g. $x_1$= size(0-2000 $feet^2$)</p>\n<p>​      $x_2$= number of bedrooms (1-5)</p>\n<p>It will be better to limit both $x_1$ and $x_2$ in [0,1]</p>\n<p>$x_1=\\frac{size(feet^2)}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom}{5}$</p>\n<blockquote>\n<p>椭圆相较于正圆需要更多的时间来计算梯度下降</p>\n</blockquote>\n<p>More general, get every feature into approximately a $-1\\le x_i\\le1$ range.</p>\n<blockquote>\n<p>范围的上下限并不是被严格限制的，不过越接近-1和1越好。过大和过小都是不合适的。</p>\n</blockquote>\n<h4 id=\"Mean-normalization\"><a href=\"#Mean-normalization\" class=\"headerlink\" title=\"Mean normalization\"></a>Mean normalization</h4><p>Replace $x_i$ with $x_i-µ_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)<br><strong>E.g.</strong> </p>\n<p>$x_1=\\frac{size(feet^2)-1000}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom-2}{5}$</p>\n<blockquote>\n<p>$µ_i$ (1000 and 2) is considered as the average value of $x_i$ in training set</p>\n</blockquote>\n<p>$$<br>x_i\\leftarrow \\frac{x_i-µ_i}{range(max-min)}<br>$$</p>\n<h3 id=\"4-4-Gradient-descent-in-practice-II-Learning-rate\"><a href=\"#4-4-Gradient-descent-in-practice-II-Learning-rate\" class=\"headerlink\" title=\"4.4 Gradient descent in practice II: Learning rate\"></a>4.4 Gradient descent in practice II: Learning rate</h3><ul>\n<li>The chapter will center around the learning rate $\\alpha$</li>\n</ul>\n<p><strong>Gradient descent</strong></p>\n<ul>\n<li>$\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta)$</li>\n<li>“Debugging”: How to make sure gradient descent is working correctly.</li>\n<li>How to choose learning rate $\\alpha$</li>\n</ul>\n<p>Declare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.</p>\n<blockquote>\n<p>这是因为若将$min_\\theta J(\\theta)$作为纵轴，将迭代次数作为横轴，那么得到的是是一个近似$y=|\\frac 1x|$的图像。当迭代次数达到一定量$(\\epsilon)$后，梯度下降的量就几乎可以忽略不计了，所以该测试就判断函数已经收敛。不过要选择一个合适的阈值（threshold，$\\epsilon$）并不容易。</p>\n</blockquote>\n<blockquote>\n<p>如果你的图像并不是上述的样子，那么说明你的$\\alpha$值选取的不恰当。例如图像在0点附近形如指数函数图像，或者呈波浪形，就意味着你的$\\alpha$值过大了，函数无法收敛。</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<ul>\n<li>If $\\alpha$ is too small: slow convergence.</li>\n<li>If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.</li>\n</ul>\n<p>Recommended choices for $\\alpha$:</p>\n<p>…, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1,… </p>\n<h3 id=\"4-5-Features-and-polynomial-regression\"><a href=\"#4-5-Features-and-polynomial-regression\" class=\"headerlink\" title=\"4.5 Features and polynomial regression\"></a>4.5 Features and polynomial regression</h3><blockquote>\n<p>根据特征选择算法以提高效率；使用多项式回归来拟合复杂函数</p>\n</blockquote>\n<p>Housing prices prediction:</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth<br>$$</p>\n<blockquote>\n<p>It’s better to use $area$ which is equal to $frontage\\times depth$ as new feature.</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times area<br>$$</p>\n<h4 id=\"Choice-of-feature\"><a href=\"#Choice-of-feature\" class=\"headerlink\" title=\"Choice of feature\"></a>Choice of feature</h4><p>Suppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2<br>$$</p>\n<blockquote>\n<p>使用二次函数来拟合房价-面积曲线可能在一定范围内是合适的，但是二次函数曲线一定会在达到顶点后下降，房价却不会。所以这并不是一个好的选择。</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3<br>$$</p>\n<blockquote>\n<p>也许是可行的，但要注意应用特征缩放，从而使三个特征值$(size,(size)^2,(size)^3)$都在大致相同的范围内</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}<br>$$</p>\n<blockquote>\n<p>上升的曲线，斜率随着x（area）变大逐渐变小，增长趋于平缓，似乎也不错</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<p>You have a choice in what features to use to fix more complex functions to your data!</p>\n<h3 id=\"4-6-Normal-equation-unfinished\"><a href=\"#4-6-Normal-equation-unfinished\" class=\"headerlink\" title=\"4.6 Normal equation (unfinished)\"></a>4.6 Normal equation (unfinished)</h3><blockquote>\n<p>对于某些线性回归问题，求取参数$\\theta$最优值的方法。不同与以往的迭代算法（梯度下降的多次迭代来收敛到全局最小值），正规方程提供了一种解析解法，一次性求解$\\theta$的最优值。</p>\n</blockquote>\n<p><strong>Normal equation</strong>: Method to solve for $\\theta$ analytically.</p>\n<h4 id=\"Compare-to-Gradient-Descent\"><a href=\"#Compare-to-Gradient-Descent\" class=\"headerlink\" title=\"Compare to Gradient Descent\"></a>Compare to Gradient Descent</h4><p>$m$ training example, $n$ features.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Descent</th>\n<th>Normal Equation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Need to choice a $\\alpha$</td>\n<td>No need to choice a $\\alpha$</td>\n</tr>\n<tr>\n<td>Needs many iterations</td>\n<td>Don’t need to iterate</td>\n</tr>\n<tr>\n<td>Work well even $n$ is large</td>\n<td>Need to compute $(X^TX)^{-1}$</td>\n</tr>\n<tr>\n<td></td>\n<td>Slow if $n$ is very large</td>\n</tr>\n</tbody></table>\n<p>$$<br>\\theta=(X^TX)^{-1}X^Ty<br>$$<br>$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$</p>\n<p><strong>Octave</strong>: <code>pinv(X&#39;*X)*X&#39;*y</code> </p>\n<blockquote>\n<p><code>X&#39;</code> is the transpose of $X$</p>\n<p><code>pinv</code> is a function  to compute the inverse of a matrix</p>\n</blockquote>\n<!--unfinished-->\n\n<h3 id=\"4-7-Normal-equation-and-non-invertibility-optional-unfinished\"><a href=\"#4-7-Normal-equation-and-non-invertibility-optional-unfinished\" class=\"headerlink\" title=\"4.7 Normal equation and non-invertibility (optional) (unfinished)\"></a>4.7 Normal equation and non-invertibility (optional) (unfinished)</h3><!--unfinished-->\n\n\n\n<h2 id=\"5-Octave-Tutorial-ignored\"><a href=\"#5-Octave-Tutorial-ignored\" class=\"headerlink\" title=\"5 Octave Tutorial (ignored)\"></a>5 Octave Tutorial (ignored)</h2><!--unfinished-->\n\n\n\n<h2 id=\"6-Logistic-Regression\"><a href=\"#6-Logistic-Regression\" class=\"headerlink\" title=\"6 Logistic Regression\"></a>6 Logistic Regression</h2><blockquote>\n<p>回归算法</p>\n</blockquote>\n<h3 id=\"6-1-Classification\"><a href=\"#6-1-Classification\" class=\"headerlink\" title=\"6.1 Classification\"></a>6.1 Classification</h3><p><strong>Classification</strong></p>\n<p>$y\\in {0,1} $</p>\n<p>0: “Negative Class” (e.g., benign tumor)</p>\n<p>1: “Positive Class” (e.g., malignant tumor)</p>\n<blockquote>\n<p>There are multi-class problems as well that y can take value from 0, 1, 2, 3,…</p>\n</blockquote>\n<p>Learning regression isn’t fit the classification problem</p>\n<p>In the <a href=\"https://www.bilibili.com/video/BV164411b7dx?p=32&t=160\">video</a> there is an example to explain it. </p>\n<p>Another example:</p>\n<p>Classification: y = 0 or 1</p>\n<p>​    $H_\\theta(x)$ can be $&gt;1$ or $&lt;0$ if we use the linear regression</p>\n<blockquote>\n<p>Obviously, the label is either 0 or 1.</p>\n</blockquote>\n<p>Logistic Regression: $0\\le h_\\theta(x)\\le1$</p>\n<blockquote>\n<p>This is a classification algorithm whose output always between 1 and 0. Besides, it’s a classification algorithm instead of linear regression algorithm though there is a “regression” in its name.</p>\n</blockquote>\n<h3 id=\"6-2-Hypothesis-Representation\"><a href=\"#6-2-Hypothesis-Representation\" class=\"headerlink\" title=\"6.2 Hypothesis Representation\"></a>6.2 Hypothesis Representation</h3><blockquote>\n<p>假设陈述</p>\n</blockquote>\n<ul>\n<li>What is the function we’re going to use to representation hypothesis when we have a classification problem.</li>\n</ul>\n<h4 id=\"Logistic-Regression-Model\"><a href=\"#Logistic-Regression-Model\" class=\"headerlink\" title=\"Logistic Regression Model\"></a>Logistic Regression Model</h4><p>​    Want $0\\le h_\\theta(x)\\le1$</p>\n<p>$$<br>h_\\theta(x)=g(\\theta^Tx)<br>$$<br><strong>Sigmoid function (Logistic function)</strong>:<br>$$<br>g(z)=\\frac1{1+e^{-z}}<br>$$<br><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" /></p>\n<blockquote>\n<p>It’s graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$</p>\n</blockquote>\n<p>Thus,<br>$$<br>h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}<br>$$</p>\n<blockquote>\n<p>$\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx&lt;0$ then  $h_\\theta(x)=0$</p>\n</blockquote>\n<p><strong>Interpretation of Hypothesis Output</strong></p>\n<p>$h_\\theta(x)=$ estimated probability that y = 1 on input x</p>\n<p>Example: if $x=\\begin{bmatrix}x_0 \\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\ tumorSize\\end{bmatrix}$</p>\n<p>​                $h_\\theta(x)=0.7$</p>\n<p>Tell patient that 70% chance of tumor being malignant.</p>\n<h4 id=\"Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\"><a href=\"#Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\" class=\"headerlink\" title=\"Mathematical formula definition of the hypothesis for logistic regression\"></a>Mathematical formula definition of the hypothesis for logistic regression</h4><p>“Probability that y=1, given x, parameterized by $\\theta$”:<br>$$<br>P(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\<br>P(y=0|x;\\theta)=1-P(y=1|x;\\theta)<br>$$</p>\n<h3 id=\"6-3-Decision-boundary\"><a href=\"#6-3-Decision-boundary\" class=\"headerlink\" title=\"6.3 Decision boundary\"></a>6.3 Decision boundary</h3><blockquote>\n<p>决策界限</p>\n</blockquote>\n<ul>\n<li>What logistic regression hypothesis function is computing?</li>\n</ul>\n<p>According to Logistic regression, </p>\n<p>suppose predict “$y=1$” If $h_\\theta(x)\\ge0.5$</p>\n<p>predict “$y=0$” If $h_\\theta(x)\\le0.5$</p>\n<h4 id=\"Decision-Boundary\"><a href=\"#Decision-Boundary\" class=\"headerlink\" title=\"Decision Boundary\"></a>Decision Boundary</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\n<p>Suppose the variable procedure to be specified. </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$</p>\n<p>And, $\\theta=\\begin{bmatrix}-3 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Predict “$y=1$”if $-3+x_1+x_2\\ge0$</p>\n<p>​                                     $x_1+x_2\\ge3$</p>\n<p>The magenta line is called <strong>Decision Boundary</strong>.</p>\n<blockquote>\n<p>The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.</p>\n</blockquote>\n<h4 id=\"Non-linear-decision-boundaries\"><a href=\"#Non-linear-decision-boundaries\" class=\"headerlink\" title=\"Non-linear decision boundaries\"></a>Non-linear decision boundaries</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\n<p>Assuming hypothesis likes this: </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$</p>\n<p>And assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\ 0 \\ 0 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Then, predict “$y=1$” if $-1+x_1^2+x_2^2\\ge0$</p>\n<p>​                                               $x_1^2+x_2^2\\ge1$</p>\n<blockquote>\n<p>The training set used to fit the parameters $\\theta$</p>\n</blockquote>\n<h3 id=\"6-4-Cost-function\"><a href=\"#6-4-Cost-function\" class=\"headerlink\" title=\"6.4 Cost function\"></a>6.4 Cost function</h3><ul>\n<li><p>How to automatically choose the parameters $\\theta$ to a training set.</p>\n</li>\n<li><p>Define the optimization objective or the cost function that used to fit the parameters.</p>\n</li>\n</ul>\n<p>Here is to supervised learning problem of fitting a logistic regression model.</p>\n<p>Training set: ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})}$</p>\n<p>$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in{0,1}$</p>\n<p>$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<p>How to choose parameters $\\theta$ ? (The next sections will focus on this problem)</p>\n<h4 id=\"Cost-function-Logistic-regression-cost-function\"><a href=\"#Cost-function-Logistic-regression-cost-function\" class=\"headerlink\" title=\"Cost function - Logistic regression cost function\"></a>Cost function - Logistic regression cost function</h4><p>​    Linear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_θ(x^{(i)})-y^{(i)})^2$</p>\n<p>​    $Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$</p>\n<blockquote>\n<p>我们先尝试直接将线性回归函数转化为逻辑回归函数，事实上后者将会是一个参数为$\\theta$的非凸函数（non-convex function），这是因为这一部分（$\\frac1{1+e^{-\\theta^T x}}$）是很复杂的非线性函数。</p>\n</blockquote>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n<p><strong>Logistic regression cost function</strong><br>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases}<br>$$</p>\n<p>If y = 1</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\n<p>If y = 0</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n<h3 id=\"6-5-simplified-cost-function-and-gradient-descent\"><a href=\"#6-5-simplified-cost-function-and-gradient-descent\" class=\"headerlink\" title=\"6.5 simplified cost function and gradient descent\"></a>6.5 simplified cost function and gradient descent</h3><ul>\n<li><p>Figure out a simpler way to write the cost function</p>\n</li>\n<li><p>Also figure out how to apply gradient descent to fit the parameters of logistic regression</p>\n</li>\n</ul>\n<h4 id=\"Logistic-regression-cost-function\"><a href=\"#Logistic-regression-cost-function\" class=\"headerlink\" title=\"Logistic regression cost function\"></a>Logistic regression cost function</h4><p>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})<br>$$</p>\n<p>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases} \\ ;\\<br>Note:y=0;or;1;always<br>$$</p>\n<p>Compress them into one equation:</p>\n<p>$$<br>Cost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))<br>$$</p>\n<p><strong>Logistic regression cost function</strong><br>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})\\<br>\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad;;;<br>=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]<br>$$</p>\n<blockquote>\n<p>为什么用这个函数作为逻辑回归的代价函数：这个式子是从统计学中的极大似然法（the principle maximum likelihood estimation）得来的，它是统计学中为不同模型快速寻找参数的方法。并且它还拥有一个良好的性质：它是凸函数。</p>\n</blockquote>\n<p>To fit parameters $\\theta$:<br>$$<br>\\min_\\theta J(\\theta)<br>$$</p>\n<blockquote>\n<p>find the $\\theta$ which minimizes $J(\\theta)$</p>\n</blockquote>\n<p>To make prediction given new $x$:</p>\n<p>​    Output $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<blockquote>\n<p>So how to minimize $J(\\theta)$, or how to choose parameter $ \\theta$ ?</p>\n</blockquote>\n<h4 id=\"Implementation-of-logistic-regression\"><a href=\"#Implementation-of-logistic-regression\" class=\"headerlink\" title=\"Implementation of logistic regression\"></a>Implementation of logistic regression</h4><p><strong>Gradient Descent</strong></p>\n<p>$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$</p>\n<p>What $\\min_\\theta J(\\theta)$:    </p>\n<p>​    Repeat {</p>\n<p>​                $\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>​        }        (simultaneously update for $\\theta_j$ for $j=0,…,n$)</p>\n<blockquote>\n<p>Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.</p>\n</blockquote>\n<h3 id=\"6-6-Advanced-optimization\"><a href=\"#6-6-Advanced-optimization\" class=\"headerlink\" title=\"6.6 Advanced optimization\"></a>6.6 Advanced optimization</h3><ul>\n<li>Some advanced optimization algorithms </li>\n<li>Some advanced optimization concepts</li>\n</ul>\n<blockquote>\n<p>大大提高逻辑回归的计算速度。:see_no_evil:</p>\n</blockquote>\n<h4 id=\"Optimization-algorithm\"><a href=\"#Optimization-algorithm\" class=\"headerlink\" title=\"Optimization algorithm\"></a>Optimization algorithm</h4><p>Cost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.</p>\n<p>Given $\\theta$, we have code that compute</p>\n<ul>\n<li>$J(\\theta)$</li>\n<li>$\\frac∂{∂\\theta_j}J(\\theta)$      (For $j=0,1,…,n$)</li>\n</ul>\n<p>Optimization algorithms:</p>\n<ul>\n<li>Gradient descent</li>\n<li>Conjugate gradient</li>\n<li>BFGS (共轭梯度法)</li>\n<li>L-BFGS</li>\n</ul>\n<p>Others algorithm compare to gradient descent</p>\n<table>\n<thead>\n<tr>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No need to manually pick $\\alpha$</td>\n<td>More complex</td>\n</tr>\n<tr>\n<td>Often faster than gradient descent</td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>These complex algorithms have a “clever inner-loop” called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.</p>\n<p>:older_man:建议不要试图实现这些算法，除非你是数值计算方面的专家，甚至你不必完全理解这些算法就能很好的使用它们。</p>\n</blockquote>\n<h3 id=\"6-7-Multi-class-classification-One-vs-all-unfinished\"><a href=\"#6-7-Multi-class-classification-One-vs-all-unfinished\" class=\"headerlink\" title=\"6.7 Multi-class classification: One-vs-all (unfinished)\"></a>6.7 Multi-class classification: One-vs-all (unfinished)</h3><ul>\n<li>How to get logistic regression to work for multi-class classification problems</li>\n<li>One-versus-all classification algorithm</li>\n</ul>\n<h4 id=\"Multiclass-classification\"><a href=\"#Multiclass-classification\" class=\"headerlink\" title=\"Multiclass classification\"></a>Multiclass classification</h4><!--unfinished-->\n\n\n\n<h2 id=\"7-Regularization-unfinished\"><a href=\"#7-Regularization-unfinished\" class=\"headerlink\" title=\"7 Regularization (unfinished)\"></a>7 Regularization (unfinished)</h2><blockquote>\n<p>正则化，是解决（改善）过拟合问题的手段之一</p>\n</blockquote>\n<h3 id=\"7-1-The-problem-of-overfitting\"><a href=\"#7-1-The-problem-of-overfitting\" class=\"headerlink\" title=\"7.1 The problem of overfitting\"></a>7.1 The problem of overfitting</h3><ul>\n<li>Explain what is overfitting problem</li>\n</ul>\n<p>Example: Linear regression (housing prices)</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>“Underfit””High bias”,高偏差：强行用直线拟合曲线分布的数据，就像“持有偏见，固执认为房价变化就是线性的”，导致拟合结果偏差很大</p>\n</blockquote>\n<blockquote>\n<p>“Overfit””High variance”,高方差</p>\n</blockquote>\n<h4 id=\"Overfitting\"><a href=\"#Overfitting\" class=\"headerlink\" title=\"Overfitting\"></a>Overfitting</h4><p>If we have too many features, the learned hypothesis may fit the training set very well ($J(θ)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).</p>\n<p>Example: Logistic regression</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n<h4 id=\"Addressing-overfitting\"><a href=\"#Addressing-overfitting\" class=\"headerlink\" title=\"Addressing overfitting\"></a>Addressing overfitting</h4><p>If we have la lot of features, and very little training data, then overfitting can become a problem.</p>\n<p>Two main options:</p>\n<ol>\n<li>Reduce number of features<ul>\n<li>Manually select which features to keep (but some feature must be abandoned)</li>\n<li>Model selection algorithm (later will explain)</li>\n</ul>\n</li>\n<li>Regularization<ul>\n<li>Keep all the features, but reduce magnitude/values of parameters $\\theta_j$</li>\n<li>Works well when we have a lot of features, each of which contributes a bit to prediction $y$.</li>\n</ul>\n</li>\n</ol>\n<!--unfinished-->\n\n\n\n<h2 id=\"8-Neural-Networks-Representation\"><a href=\"#8-Neural-Networks-Representation\" class=\"headerlink\" title=\"8 Neural Networks: Representation\"></a>8 Neural Networks: Representation</h2><h3 id=\"8-1-Non-linear-hypothesis\"><a href=\"#8-1-Non-linear-hypothesis\" class=\"headerlink\" title=\"8.1 Non-linear hypothesis\"></a>8.1 Non-linear hypothesis</h3><p>Why we need Neural Networks?</p>\n<blockquote>\n<p>当特征很多，线性回归和逻辑回归就不那么好用了。即使他们得出了能够拟合当前样本的结论，该结果也很有可能是过拟合的。</p>\n</blockquote>\n<p>The neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.</p>\n<h3 id=\"8-2-Neurons-and-the-brain\"><a href=\"#8-2-Neurons-and-the-brain\" class=\"headerlink\" title=\"8.2 Neurons and the brain\"></a>8.2 Neurons and the brain</h3><p><strong>History of  neural networks</strong></p>\n<ul>\n<li>Origins: Algorithms that try to mimic the brain.</li>\n<li>WAs very widely used in 80s and early 90s; popularity diminished in late 90s.</li>\n<li>Recent resurgence: State-of-the-art technique for many applications.</li>\n</ul>\n<p><strong>The “one learning algorithm” hypothesis</strong></p>\n<ul>\n<li>Neuro-rewiring experiments</li>\n<li>Sensor representations in the brain</li>\n</ul>\n<h3 id=\"8-3-Model-representation-I\"><a href=\"#8-3-Model-representation-I\" class=\"headerlink\" title=\"8.3 Model representation I\"></a>8.3 Model representation I</h3><ul>\n<li>How we represent  Neural Networks (hypothesis or model).</li>\n</ul>\n<h4 id=\"Neuron-model-Logistic-unit\"><a href=\"#Neuron-model-Logistic-unit\" class=\"headerlink\" title=\"Neuron model: Logistic unit\"></a>Neuron model: Logistic unit</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n<p><strong>Sigmoid (logistic) activation function</strong></p>\n<blockquote>\n<p>这里的激活函数(activation function)是指代非线性函数$g(z)=\\frac1{1+e^{-z}}$的另一个术语</p>\n</blockquote>\n<p>Sometimes we add an extra $x_0$ node (if necessary) called bias unit (偏置单元) or the bias neuron (偏置神经元). It’s always equal to 1 so sometime we don’t draw it.</p>\n<p>In the neural networks literature, the parameters of model $\\theta$ is also called <strong>weights of a model</strong>.</p>\n<h4 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\n<p>Layer 1: Input layer</p>\n<p>Layer 2 : Hidden layer</p>\n<p>Layer 3: Output layer</p>\n<p>$a_i^{(j)}=$ “activation” of unit $i$ in layer $j$.</p>\n<p>$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$</p>\n<blockquote>\n<p>可以形象的看作神经网络被这些矩阵（$\\Theta^{(j)}$）参数化，因而这些矩阵也被称作权重矩阵（matrix of weights）。权重矩阵控制着从某一层到下一层的映射。</p>\n</blockquote>\n<p>$$<br>a_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\<br>a_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\<br>a_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\<br>h_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})<br>$$</p>\n<p>If networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.</p>\n<blockquote>\n<p>The superscript $j$ in parentheses means that these values associated with layer $j$</p>\n</blockquote>\n<h3 id=\"8-4-Model-representation-II\"><a href=\"#8-4-Model-representation-II\" class=\"headerlink\" title=\"8.4 Model representation II\"></a>8.4 Model representation II</h3><ul>\n<li>How to carry out computation efficiently and show a vectorized implementation.</li>\n<li>Intuition about why these neural network representation</li>\n</ul>\n<h4 id=\"Forward-propagation-Vectorized-implementation\"><a href=\"#Forward-propagation-Vectorized-implementation\" class=\"headerlink\" title=\"Forward propagation: Vectorized implementation\"></a>Forward propagation: Vectorized implementation</h4><p><strong>Define:</strong></p>\n<p>$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$</p>\n<p>Thus,</p>\n<p>$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$</p>\n<p>We observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.</p>\n<p><strong>Define:</strong><br>$$<br>x=\\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)}\\end{bmatrix}<br>$$<br>Further towards vectorization:<br>$$<br>z^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\<br>a^{(2)}=g(z^{(2)})<br>$$</p>\n<blockquote>\n<p>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.</p>\n</blockquote>\n<p>Next, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$</p>\n<p>$z^{(3)}=\\Theta^{(2)}a^{(2)}$</p>\n<p>$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$</p>\n<blockquote>\n<p>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.</p>\n</blockquote>\n<p>This process of computing $h(x)$ is also called <strong>forward propagation</strong>, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.</p>\n<h4 id=\"Neural-Network-learning-its-own-features\"><a href=\"#Neural-Network-learning-its-own-features\" class=\"headerlink\" title=\"Neural Network learning its own features\"></a>Neural Network learning its own features</h4><blockquote>\n<p>如果只关注第二层和第三层，那么神经网络的行为类似于逻辑回归。然而神经网络输出层的输入是隐藏层计算或是说学习的结果（$a_1^{(2)},a_2^{(2)},a_3^{(2)}…$），而不是逻辑回归中初始的特征项$x_1,x_2,x_3…$ 。前者较后者更适合作为假设参数，因此神经网络算法具备灵活快速尝试学习任意特征项，处理更多复杂特征的能力。</p>\n</blockquote>\n<h4 id=\"Other-network-architectures\"><a href=\"#Other-network-architectures\" class=\"headerlink\" title=\"Other network architectures\"></a>Other network architectures</h4><p>The way that neural networks are connected are called the architecture (神经网络的架构). So the architecture refers to how different neurons are connected to each other.</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n<h3 id=\"8-5-Examples-and-intoitions-unfinished\"><a href=\"#8-5-Examples-and-intoitions-unfinished\" class=\"headerlink\" title=\"8.5 Examples and intoitions (unfinished)\"></a>8.5 Examples and intoitions (unfinished)</h3><ul>\n<li><p>A detail example which shows how a neural network can compute a complex nonlinear function of the input</p>\n</li>\n<li><p>Why neural network can be used to learn complex nonlinear hypothesis</p>\n</li>\n</ul>\n<h2 id=\"9-Neural-Networks-Learning-unfinished\"><a href=\"#9-Neural-Networks-Learning-unfinished\" class=\"headerlink\" title=\"9 Neural Networks: Learning (unfinished)\"></a>9 Neural Networks: Learning (unfinished)</h2><h2 id=\"10-Advice-for-applying-machine-learning-unfinished\"><a href=\"#10-Advice-for-applying-machine-learning-unfinished\" class=\"headerlink\" title=\"10 Advice for applying machine learning (unfinished)\"></a>10 Advice for applying machine learning (unfinished)</h2><p>10.1 Decide what to try next</p>\n<p>10.2 Evaluating a hypothesis</p>\n<p>10.3 Model selection and training/validation/test sets</p>\n<h2 id=\"11-Machine-Learning-system-design-unfinished\"><a href=\"#11-Machine-Learning-system-design-unfinished\" class=\"headerlink\" title=\"11 Machine Learning system design (unfinished)\"></a>11 Machine Learning system design (unfinished)</h2><p>11.1 Prioritizing what to work on: Spam classification example</p>\n<h2 id=\"12-Support-Vector-Machines-unfinished\"><a href=\"#12-Support-Vector-Machines-unfinished\" class=\"headerlink\" title=\"12 Support Vector Machines (unfinished)\"></a>12 Support Vector Machines (unfinished)</h2><p>12.1 Optimizaion object</p>\n<ul>\n<li>Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions</li>\n</ul>\n<h2 id=\"13-Clustering\"><a href=\"#13-Clustering\" class=\"headerlink\" title=\"13 Clustering\"></a>13 Clustering</h2><h3 id=\"13-0-Notation\"><a href=\"#13-0-Notation\" class=\"headerlink\" title=\"13.0 Notation\"></a>13.0 Notation</h3><p>Train set: ${x^{(1)},x^{(2)},x^{(3)},…x^{(m)}}$  (without labels)</p>\n<h3 id=\"13-1-K-means-algorithm\"><a href=\"#13-1-K-means-algorithm\" class=\"headerlink\" title=\"13.1 K-means algorithm\"></a>13.1 K-means algorithm</h3><blockquote>\n<p>K均值算法</p>\n</blockquote>\n<p>K-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (聚类中心). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.</p>\n<p><strong>Input</strong>:</p>\n<ul>\n<li>$K$ (number of clusters)</li>\n<li>Training set ${x^{(1)},x^{(2)},x^{(3)},…x^{(m)}}$</li>\n</ul>\n<p>$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)</p>\n<h4 id=\"K-means-algorithm\"><a href=\"#K-means-algorithm\" class=\"headerlink\" title=\"K-means algorithm\"></a>K-means algorithm</h4><p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,…,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {<br>    // <u>cluster assignment step</u></p>\n<p>​    for $i$ = 1 to $m$</p>\n<p>​        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)</p>\n<p>​    // <u>move centroids step</u></p>\n<p>​    for $k$ = 1 to $K$</p>\n<p>​        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<blockquote>\n<p>$K$ means the number of centroids and the $k$ means the index of each centriod.</p>\n<p>If you have a cluster with no points assigned to it, the usual practice is to delete it.</p>\n</blockquote>\n<h3 id=\"13-2-Optimization-objective\"><a href=\"#13-2-Optimization-objective\" class=\"headerlink\" title=\"13.2 Optimization objective\"></a>13.2 Optimization objective</h3><ul>\n<li>How we can use it to help K-means algorithm find better clusters and avoid local optima.</li>\n</ul>\n<h4 id=\"K-means-optimization-objective\"><a href=\"#K-means-optimization-objective\" class=\"headerlink\" title=\"K-means optimization objective\"></a>K-means optimization objective</h4><p>$c^{(i)}$ = index of cluster (1,2,…,$K$) to which example $x^{(i)}$ is currently assigned</p>\n<p>$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)</p>\n<p>$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p>\n<h4 id=\"Optimization-objective\"><a href=\"#Optimization-objective\" class=\"headerlink\" title=\"Optimization objective:\"></a>Optimization objective:</h4><p>$$<br>J(c^{(1)},… c^{(m)},\\mu_1,…,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\<br>\\min_{c^{(1)},… c^{(m)},\\mu_1,…,\\mu_K} J(c^{(1)},… c^{(m)},\\mu_1,…,\\mu_K)<br>$$</p>\n<blockquote>\n<p>The cost function $J$ is also called discotion function</p>\n</blockquote>\n<p><strong>Review K-means algorithm</strong></p>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,…,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {</p>\n<p>​    //minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed</p>\n<p>​    for $i$ = 1 to $m$</p>\n<p>​        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$</p>\n<p>​    //minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed</p>\n<p>​    for $k$ = 1 to $K$</p>\n<p>​        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<h3 id=\"13-3-Randomly-initialization\"><a href=\"#13-3-Randomly-initialization\" class=\"headerlink\" title=\"13.3 Randomly initialization\"></a>13.3 Randomly initialization</h3><ul>\n<li>How to initialize K-means</li>\n<li>How to avoid local optima</li>\n</ul>\n<blockquote>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,…,\\mu_K \\in \\mathbb R^n$</p>\n</blockquote>\n<p><strong>Randomly initialization</strong></p>\n<p>Usually, we have $K&lt;m$</p>\n<p>Randomsly pick $K$ training examples.</p>\n<p>Set $\\mu_1,…,\\mu_K$ equal to these $K$ examples.</p>\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>Local optima we should avoid, but randomly initialization may cause this situation.</p>\n</blockquote>\n<blockquote>\n<p>简而言之，为了避免这种情况的发生，我们可以先初始化一千遍，然后选择畸变最小的那一种情况，也就是最有潜力，最不可能陷入局部最优的情况来进行接下来的运算。</p>\n</blockquote>\n<h3 id=\"13-4-Choosing-the-number-of-cluster\"><a href=\"#13-4-Choosing-the-number-of-cluster\" class=\"headerlink\" title=\"13.4 Choosing the number of cluster\"></a>13.4 Choosing the number of cluster</h3><ul>\n<li>Manual</li>\n<li>Elbow method</li>\n<li>Later downstream purpose</li>\n</ul>\n<h2 id=\"14-Dimensionaliy-Reduction-unfinished\"><a href=\"#14-Dimensionaliy-Reduction-unfinished\" class=\"headerlink\" title=\"14 Dimensionaliy Reduction (unfinished)\"></a>14 Dimensionaliy Reduction (unfinished)</h2><p>14.1 Motivation I: Data Compression</p>\n<p>14.2 Motivation II: Visualization</p>\n<p>14.3 Principle Component Analysis problem formulation (PCA)</p>\n<blockquote>\n<p>主成分分析法</p>\n</blockquote>\n<ul>\n<li>Compression algorithm</li>\n</ul>\n<p>14.4 Principle Component Analysis algorithm</p>\n<p>Data preprocessing</p>\n<h2 id=\"15-Anomaly-Detection-unfinished\"><a href=\"#15-Anomaly-Detection-unfinished\" class=\"headerlink\" title=\"15 Anomaly Detection (unfinished)\"></a>15 Anomaly Detection (unfinished)</h2><p>15.1 Problem motivation</p>\n<h2 id=\"16-Recommeder-Systems-unfinished\"><a href=\"#16-Recommeder-Systems-unfinished\" class=\"headerlink\" title=\"16 Recommeder Systems (unfinished)\"></a>16 Recommeder Systems (unfinished)</h2><h2 id=\"17-Large-Scale-Machine-Learning-unfinished\"><a href=\"#17-Large-Scale-Machine-Learning-unfinished\" class=\"headerlink\" title=\"17 Large Scale Machine Learning (unfinished)\"></a>17 Large Scale Machine Learning (unfinished)</h2><h3 id=\"17-1-Learning-with-large-datasets\"><a href=\"#17-1-Learning-with-large-datasets\" class=\"headerlink\" title=\"17.1 Learning with large datasets\"></a>17.1 Learning with large datasets</h3><p>$$<br>\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p>\n<blockquote>\n<p>​    对于上亿的数据规模来说，计算梯度下降中的求和函数是难以承受的负担</p>\n</blockquote>\n<h3 id=\"17-2-Stochastic-gradient-descent\"><a href=\"#17-2-Stochastic-gradient-descent\" class=\"headerlink\" title=\"17.2 Stochastic gradient descent\"></a>17.2 Stochastic gradient descent</h3><h4 id=\"Review-linear-regression-with-gradient-descent\"><a href=\"#Review-linear-regression-with-gradient-descent\" class=\"headerlink\" title=\"Review linear regression with gradient descent\"></a>Review linear regression with gradient descent</h4><!-- unfinished -->\n\n<h4 id=\"Batch-gradient-descent\"><a href=\"#Batch-gradient-descent\" class=\"headerlink\" title=\"Batch gradient descent\"></a>Batch gradient descent</h4><h4 id=\"Stochastic-gradient-descent\"><a href=\"#Stochastic-gradient-descent\" class=\"headerlink\" title=\"Stochastic gradient descent\"></a>Stochastic gradient descent</h4><h3 id=\"17-3-Mini-batch-gradient-descent\"><a href=\"#17-3-Mini-batch-gradient-descent\" class=\"headerlink\" title=\"17.3 Mini-batch gradient descent\"></a>17.3 Mini-batch gradient descent</h3><h2 id=\"Markdown\"><a href=\"#Markdown\" class=\"headerlink\" title=\"Markdown\"></a>Markdown</h2><p>\\begin{bmatrix}\\end{bmatrix}</p>\n<p>\\mathbb R</p>\n<p><a href=\"https://www.jianshu.com/p/25f0139637b7\">公式</a></p>\n<p><a href=\"https://www.jianshu.com/p/e74eb43960a1\">公式2</a></p>\n<p><a href=\"https://www.jianshu.com/p/191d1e21f7ed\">语法</a></p>\n","site":{"data":{}},"length":24179,"excerpt":"","more":"<h1 id=\"Machine-Learning\"><a href=\"#Machine-Learning\" class=\"headerlink\" title=\"Machine Learning\"></a><a href=\"https://www.bilibili.com/video/BV164411b7dx\">Machine Learning</a></h1><h2 id=\"Index\"><a href=\"#Index\" class=\"headerlink\" title=\"Index\"></a>Index</h2><p>[toc]</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 Introduction\"></a>1 Introduction</h2><h3 id=\"1-1-Supervised-Learn\"><a href=\"#1-1-Supervised-Learn\" class=\"headerlink\" title=\"1.1 Supervised Learn\"></a>1.1 Supervised Learn</h3><p>A “right answer” given</p>\n<h4 id=\"Regression\"><a href=\"#Regression\" class=\"headerlink\" title=\"Regression\"></a>Regression</h4><p>Predict continuous valued output (e.g. housing price)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Linear regression</li>\n<li>Neural Networks</li>\n<li>Nearest Neighbor</li>\n</ul>\n<h4 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h4><p>Discrete valued output (0 or 1)</p>\n<p><strong>Related algorithms</strong>:</p>\n<ul>\n<li>Logistic regression</li>\n<li>K-Nearest Neighbor (KNN)</li>\n<li>Support Vector Machines (SVM)</li>\n<li>Naïve Bayes</li>\n<li>Decision Trees</li>\n<li>Neural Networks</li>\n</ul>\n<h3 id=\"1-2-Unsupervised-Learn\"><a href=\"#1-2-Unsupervised-Learn\" class=\"headerlink\" title=\"1.2 Unsupervised Learn\"></a>1.2 Unsupervised Learn</h3><h4 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a><a href=\"https://zhuanlan.zhihu.com/p/78382376\">Cluster</a></h4><p><strong>Applications</strong>:</p>\n<blockquote>\n<p>聚类：将相似的对象归到同一个簇中，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。</p>\n</blockquote>\n<ul>\n<li>Market segmentation</li>\n<li>Social network analysis</li>\n<li>Organize computing cluster</li>\n<li>Astronomical data analysis</li>\n</ul>\n<p><a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\"><strong>Related algorithms</strong></a>:</p>\n<ul>\n<li>K-Means Clustering</li>\n<li>Mean-Shift Clustering</li>\n<li>Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</li>\n<li>Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)</li>\n</ul>\n<h2 id=\"2-Linear-Regression-with-One-Variable\"><a href=\"#2-Linear-Regression-with-One-Variable\" class=\"headerlink\" title=\"2 Linear Regression with One Variable\"></a>2 Linear Regression with One Variable</h2><h3 id=\"2-0-Model-Representation-Notation\"><a href=\"#2-0-Model-Representation-Notation\" class=\"headerlink\" title=\"2.0 Model Representation - Notation\"></a>2.0 Model Representation - Notation</h3><p>$m$: number of training examples</p>\n<p>$x$’s: input variable/feature</p>\n<p>$y$’s: output variable/feature</p>\n<p>$(x,y)$: a training example</p>\n<p>$(x^i,y^i)$: $i$ represents the $i^{th}$</p>\n<h3 id=\"2-1-Model-and-Cost-Function\"><a href=\"#2-1-Model-and-Cost-Function\" class=\"headerlink\" title=\"2.1 Model and Cost Function\"></a>2.1 Model and Cost Function</h3><p><strong>Hypothesis 假设函数</strong>:<br>$$<br>h_θ(x)=θ_0+θ_1x<br>$$<br><strong>Parameters</strong>: $\\theta_0,\\theta_1$</p>\n<blockquote>\n<p>$\\theta_1$代表斜率而$\\theta_0$则代表由代价函数计算出的差值</p>\n</blockquote>\n<p><a href=\"https://www.cnblogs.com/geaozhang/p/11442343.html\"><strong>Cost Function 代价函数</strong></a>:<br>$$<br>J(θ_0,θ_1)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2<br>$$</p>\n<blockquote>\n<p>minimize squared error cost function (最小化平方差代价函数)</p>\n</blockquote>\n<p><strong>Goal</strong>: $\\min_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$</p>\n<h3 id=\"2-2-Parameter-Learning-Gradient-Descent\"><a href=\"#2-2-Parameter-Learning-Gradient-Descent\" class=\"headerlink\" title=\"2.2 Parameter Learning - Gradient Descent\"></a>2.2 Parameter Learning - Gradient Descent</h3><h4 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h4><ul>\n<li><p>Start with some $\\theta_0,\\theta_1$</p>\n</li>\n<li><p>Keep changing $\\theta_0,\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until we hopefully end up at a minimum</p>\n</li>\n</ul>\n<h4 id=\"Gradient-descent-algorithm\"><a href=\"#Gradient-descent-algorithm\" class=\"headerlink\" title=\"Gradient descent algorithm\"></a>Gradient descent algorithm</h4><p>​    Repeat until convergence {</p>\n<p>​        $\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$    (for j=0 and j=1) </p>\n<p>}</p>\n<blockquote>\n<p>$:=$ colon equals, which used to denote assignment (赋值运算符)</p>\n<p>$\\alpha$ is called the learning rate, determined how big a step we take downhill with gradient descent</p>\n<p>$\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)$ is a derivative term (导数项)</p>\n<p><strong>Assert</strong>: simultaneous update $\\theta_0,\\theta_1$ “at the same time”</p>\n</blockquote>\n<h4 id=\"Gradient-descent-intuition\"><a href=\"#Gradient-descent-intuition\" class=\"headerlink\" title=\"Gradient descent intuition\"></a>Gradient descent intuition</h4><p>$$<br>\\theta_1:=\\theta_1-\\alpha\\frac∂{∂\\theta_j}J(\\theta_1)<br>$$</p>\n<ol>\n<li><p>$\\alpha$</p>\n<p>if the $\\alpha$ is too small, gradient descent can be very <u>slow</u>.</p>\n<p>if $\\alpha$ is too large, gradient descent can <u>overshoot</u> the minimum. It may fail to converge, or even diverge.</p>\n</li>\n<li><p>Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.</p>\n</li>\n<li><p>As we approach a local minimum, gradient descent will automatically take smaller step. So, no need to decrease $\\alpha$ over time.</p>\n</li>\n</ol>\n<h4 id=\"Gradient-descent-for-linear-regression\"><a href=\"#Gradient-descent-for-linear-regression\" class=\"headerlink\" title=\"Gradient descent for linear regression\"></a>Gradient descent for linear regression</h4><blockquote>\n<p>Apply gradient descent to minimize squared error cost function </p>\n</blockquote>\n<p>$$\\frac∂{∂\\theta_j}J(\\theta_0,\\theta_1)=\\frac∂{∂\\theta_j}\\frac {1}{2m}\\sum_{i=1}^m(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$$        <em>Expanding the formula</em></p>\n<p>Substituting 0 and 1 into $j$</p>\n<p>$$\\theta_0:j=0:\\theta=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$</p>\n<p>$$\\theta_1:j=1:\\frac∂{∂\\theta_1}J(\\theta_0,\\theta_1)=\\frac1m\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$</p>\n<p>When work out the derivatives, which is the slope of the cost function <em>J</em>, plug them back in to gradient descent algorithm (remember to update simultaneously).</p>\n<p><strong>Convex function</strong></p>\n<blockquote>\n<p>It doesn’t have any local optimum except for the global optimum</p>\n</blockquote>\n<p><strong>“Batch” Gradient Descent</strong></p>\n<blockquote>\n<p>“Batch”: Each step of gradient descent uses all the training examples (entire training set)</p>\n</blockquote>\n<h2 id=\"3-Linear-Algebra-review-optional\"><a href=\"#3-Linear-Algebra-review-optional\" class=\"headerlink\" title=\"3 Linear Algebra review (optional)\"></a>3 Linear Algebra review (optional)</h2><h3 id=\"3-1-Matrices-and-vectors\"><a href=\"#3-1-Matrices-and-vectors\" class=\"headerlink\" title=\"3.1 Matrices and vectors\"></a>3.1 Matrices and vectors</h3><h4 id=\"Matrix\"><a href=\"#Matrix\" class=\"headerlink\" title=\"Matrix\"></a>Matrix</h4><blockquote>\n<p>Rectangular array of numbers</p>\n</blockquote>\n<p>3×2 matrix: $\\begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\\ 5&amp;6\\end{bmatrix}$    2×3 matrix: $\\begin{bmatrix}1 &amp;2&amp;3 \\6&amp; 3 &amp; 4\\ \\end{bmatrix}$</p>\n<p><strong>Dimension of matrix</strong>: number of rows $\\times$ number of columns</p>\n<blockquote>\n<p>The above matrix can be also write as $\\mathbb R^{3\\times2}$ </p>\n</blockquote>\n<p>**Refer to specific elements of the matrix **(entries of matrix)</p>\n<p>$A=\\begin{bmatrix}1402&amp;191 \\ 1371 &amp;821\\ 949&amp;1437\\147&amp;1448\\end{bmatrix}$</p>\n<p>$A_{ij}=$ “$i$,$j$ entry” in the $i^{th}$ row, $j^{th}$ column.</p>\n<p>$A_{11}=1402$, $A_{12}=191$, $A_{41}=147$</p>\n<h4 id=\"Vector\"><a href=\"#Vector\" class=\"headerlink\" title=\"Vector\"></a>Vector</h4><blockquote>\n<p>An $n\\times1$ matrix</p>\n</blockquote>\n<p>$y=\\begin{bmatrix}460\\232\\315\\178\\end{bmatrix}$</p>\n<p>$y_i=i^{th}$ element</p>\n<blockquote>\n<p>It is often customary to use uppercase letters for matrices and lowercase letters for vectors</p>\n</blockquote>\n<h3 id=\"3-2-Addition-and-Scalar-multiplication\"><a href=\"#3-2-Addition-and-Scalar-multiplication\" class=\"headerlink\" title=\"3.2 Addition and Scalar multiplication\"></a>3.2 Addition and Scalar multiplication</h3><h3 id=\"3-3-Matrix-vector-multiplication\"><a href=\"#3-3-Matrix-vector-multiplication\" class=\"headerlink\" title=\"3.3 Matrix-vector multiplication\"></a>3.3 Matrix-vector multiplication</h3><p><strong>Details</strong>:</p>\n<p>​            $A$           $\\times$   $x$       $=$       $y$</p>\n<p>$\\begin{bmatrix}&amp;&amp;&amp;&amp;&amp;\\ \\ \\ \\end{bmatrix}\\times\\begin{bmatrix}\\ \\ \\ \\end{bmatrix} \\quad=\\quad \\begin{bmatrix}\\ \\ \\ \\ \\end{bmatrix}$</p>\n<p>   m$\\times$n matrix        n$\\times$1    m-dimensional vector</p>\n<p>To get $y_i$, multiply $A$’ $i^{th}$ row with elements of vector $x$, and add them up.</p>\n<p><strong>Calculation Tips</strong>：</p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Competing hypotheses: $h_\\theta(x)=-40+0.25x$</p>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40\\0.25\\end{bmatrix}$ </p>\n<h3 id=\"3-4-Matrix-matrix-multiplication\"><a href=\"#3-4-Matrix-matrix-multiplication\" class=\"headerlink\" title=\"3.4 Matrix-matrix multiplication\"></a>3.4 Matrix-matrix multiplication</h3><p><strong>Details</strong>:</p>\n<p>$A\\times B=C$</p>\n<p>[m$\\times$n]$\\times$[n$\\times$o]=m$\\times$o</p>\n<p>The $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (For $i$=1,2,…,0)</p>\n<p><strong>Example</strong>:</p>\n<p>$\\begin{bmatrix}1&amp;3\\2&amp;5\\end{bmatrix}\\begin{bmatrix}0&amp;1\\3&amp;2\\end{bmatrix}=\\begin{bmatrix}1\\times0+3\\times3&amp;1\\times1+3\\times2 \\2\\times0+5\\times3&amp;2\\times1+5\\times2\\end{bmatrix}=\\begin{bmatrix}9&amp;7\\15&amp;12\\end{bmatrix}$</p>\n<p><strong>Calculation Tips II</strong>：</p>\n<p>House sizes: 2104,1216, 1534, 852</p>\n<p>Three competing hypotheses: </p>\n<ol>\n<li>$h_\\theta(x)=-40+0.25x$</li>\n<li>$h_\\theta(x)=200+0.1x$</li>\n<li>$h_\\theta(x)=-150+0.4x$</li>\n</ol>\n<p>It can be calculated as $\\begin{bmatrix}1&amp;2140\\1&amp;1416\\1&amp;1534\\1&amp;852 \\end{bmatrix}\\times\\begin{bmatrix}-40&amp;200&amp;-150 \\ 0.25&amp;0.1&amp;0.4 \\end{bmatrix}$</p>\n<h3 id=\"3-5-Matrix-multiplication-properties\"><a href=\"#3-5-Matrix-multiplication-properties\" class=\"headerlink\" title=\"3.5 Matrix multiplication properties\"></a>3.5 Matrix multiplication properties</h3><p>Let $A$ and $B$ are matrices. then is general, $A\\times B\\ne B\\times A$. (<strong>Not commutative</strong>) </p>\n<h4 id=\"Identity-Matrix\"><a href=\"#Identity-Matrix\" class=\"headerlink\" title=\"Identity Matrix\"></a>Identity Matrix</h4><p>Denoted $I$ (or $I_{n\\times n}$).</p>\n<p>Example of identity matrices:</p>\n<p>2$\\times$2: $\\begin{bmatrix}1&amp;0 \\ 0&amp;1\\end{bmatrix}$      3$\\times$3:$\\begin{bmatrix}1&amp;0&amp;0 \\ 0&amp;1&amp;0 \\ 0&amp;0&amp;1\\end{bmatrix}$      $\\cdots$</p>\n<p>For any matrix $A$,</p>\n<p>$$<br>A\\cdot I=I\\cdot A=A<br>$$</p>\n<blockquote>\n<p>Implicit conditions of the formula: </p>\n<p>$A(m\\times n)\\cdot I(n\\times n)=I(m\\times m)\\cdot A(m\\times n)=A(m\\times n)$</p>\n</blockquote>\n<h3 id=\"3-6-Inverse-and-Transpose\"><a href=\"#3-6-Inverse-and-Transpose\" class=\"headerlink\" title=\"3.6 Inverse and Transpose\"></a>3.6 Inverse and Transpose</h3><blockquote>\n<p>矩阵的逆和矩阵的转置</p>\n</blockquote>\n<p>Not all numbers have an inverse. (e.g. 0) Likely, not all matrix has an inverse.(e.g.$\\begin{bmatrix}0&amp;0 \\ 0&amp;0\\end{bmatrix}$)</p>\n<blockquote>\n<p>Matrices that don’t have an inverse are “singular” or “degenerate”.</p>\n</blockquote>\n<h4 id=\"Matrix-inverse\"><a href=\"#Matrix-inverse\" class=\"headerlink\" title=\"Matrix inverse\"></a>Matrix inverse</h4><p>If A is an m$\\times$m matrix, and if it has an inverse,<br>$$<br>AA^{-1}=A^{-1}A=I<br>$$</p>\n<blockquote>\n<p>An m$\\times$m matrix called a square matrix (方阵), only square matrix has an inverse. </p>\n</blockquote>\n<h4 id=\"Matrix-transpose\"><a href=\"#Matrix-transpose\" class=\"headerlink\" title=\"Matrix transpose\"></a>Matrix transpose</h4><p>Example:<br>$$<br>A=\\begin{bmatrix}1&amp;2&amp;0 \\ 3&amp;5&amp;9\\end{bmatrix} \\qquad A^T=\\begin{bmatrix}1&amp;3 \\ 2&amp;5 \\ 0&amp;9 \\end{bmatrix}<br>$$<br>Let $A$ be an $m\\times n$ matrix, and let $B=A^T$. Then $B$ is an $n\\times m$ matrix, and $B_{ij}=A_{ji}$.</p>\n<h2 id=\"4-Linear-Regression-with-Multiple-Variables\"><a href=\"#4-Linear-Regression-with-Multiple-Variables\" class=\"headerlink\" title=\"4 Linear Regression with Multiple Variables\"></a>4 Linear Regression with Multiple Variables</h2><h3 id=\"4-1-Multiple-feature\"><a href=\"#4-1-Multiple-feature\" class=\"headerlink\" title=\"4.1 Multiple feature\"></a>4.1 Multiple feature</h3><h4 id=\"Notation\"><a href=\"#Notation\" class=\"headerlink\" title=\"Notation\"></a>Notation</h4><p>$n$ = number of features</p>\n<p>$x^{(i)}$ = input (features) of $i^{th}$ training example</p>\n<p>$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example</p>\n<h4 id=\"Multivariate-linear-regression\"><a href=\"#Multivariate-linear-regression\" class=\"headerlink\" title=\"Multivariate linear regression\"></a>Multivariate linear regression</h4><blockquote>\n<p>多元线性回归</p>\n</blockquote>\n<p><strong>Hypothesis</strong>:</p>\n<p>$h_θ(x)=θ_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>For convenience of notation, define $x_0=1$. Then</p>\n<p>$h_θ(x)=θ_0x_0+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p>​          $=\\theta^Tx$</p>\n<p>​          $=\\begin{bmatrix}\\theta_0&amp;\\theta_1&amp;\\cdots&amp;\\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix}$</p>\n<h3 id=\"4-2-Gradient-descent-for-multiple-variables\"><a href=\"#4-2-Gradient-descent-for-multiple-variables\" class=\"headerlink\" title=\"4.2 Gradient descent for multiple variables\"></a>4.2 Gradient descent for multiple variables</h3><ul>\n<li><p>如何设定假设的参数</p>\n</li>\n<li><p>如何使用梯度下降法来处理多元线性回归</p>\n</li>\n</ul>\n<p><strong>Hypothesis</strong>: $h_θ(x)=\\theta^Tx=θ_0x_1+θ_1x_1+\\theta_2x_2+\\theta_3x_3+\\cdots+\\theta_nx_n$</p>\n<p><strong>Parameters</strong>: $\\theta_0$,$\\theta_1$,…,$\\theta_n$</p>\n<p><strong>Cost function</strong>: $J(\\theta_0$,$\\theta_1$,…,$\\theta_n)$$=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$</p>\n<p><strong>Gradient descent</strong>:</p>\n<p>​    Repeat {</p>\n<p>​        $\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta_0,…,\\theta_n)$        </p>\n<p>}        (simultaneously update for every $j=0,…,n$)</p>\n<blockquote>\n<p>$J(\\theta_0,…,\\theta_n)$ can be instead by $J(\\theta)$</p>\n</blockquote>\n<p><strong>New algorithm</strong> for $n\\ge1$:</p>\n<p>​    Repeat {</p>\n<p>​        $\\theta_j:=\\theta_j-\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>}        (simultaneously update for $\\theta_j$ for $j=0,…,n$)</p>\n<blockquote>\n<p>Previously $(n=1)$ is in 2.2 </p>\n</blockquote>\n<h3 id=\"4-3-Gradient-descent-in-practice-I-Feature-Scaling\"><a href=\"#4-3-Gradient-descent-in-practice-I-Feature-Scaling\" class=\"headerlink\" title=\"4.3 Gradient descent in practice I: Feature Scaling\"></a>4.3 Gradient descent in practice I: Feature Scaling</h3><blockquote>\n<p>梯度下降运算中的实用技巧——特征缩放</p>\n</blockquote>\n<h4 id=\"Feature-Scaling\"><a href=\"#Feature-Scaling\" class=\"headerlink\" title=\"Feature Scaling\"></a>Feature Scaling</h4><blockquote>\n<p><strong>Idea</strong>: Make sure flatten are on a similar scale.</p>\n</blockquote>\n<p>E.g. $x_1$= size(0-2000 $feet^2$)</p>\n<p>​      $x_2$= number of bedrooms (1-5)</p>\n<p>It will be better to limit both $x_1$ and $x_2$ in [0,1]</p>\n<p>$x_1=\\frac{size(feet^2)}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom}{5}$</p>\n<blockquote>\n<p>椭圆相较于正圆需要更多的时间来计算梯度下降</p>\n</blockquote>\n<p>More general, get every feature into approximately a $-1\\le x_i\\le1$ range.</p>\n<blockquote>\n<p>范围的上下限并不是被严格限制的，不过越接近-1和1越好。过大和过小都是不合适的。</p>\n</blockquote>\n<h4 id=\"Mean-normalization\"><a href=\"#Mean-normalization\" class=\"headerlink\" title=\"Mean normalization\"></a>Mean normalization</h4><p>Replace $x_i$ with $x_i-µ_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)<br><strong>E.g.</strong> </p>\n<p>$x_1=\\frac{size(feet^2)-1000}{2000}$</p>\n<p>$x_2=\\frac{numberOfBedroom-2}{5}$</p>\n<blockquote>\n<p>$µ_i$ (1000 and 2) is considered as the average value of $x_i$ in training set</p>\n</blockquote>\n<p>$$<br>x_i\\leftarrow \\frac{x_i-µ_i}{range(max-min)}<br>$$</p>\n<h3 id=\"4-4-Gradient-descent-in-practice-II-Learning-rate\"><a href=\"#4-4-Gradient-descent-in-practice-II-Learning-rate\" class=\"headerlink\" title=\"4.4 Gradient descent in practice II: Learning rate\"></a>4.4 Gradient descent in practice II: Learning rate</h3><ul>\n<li>The chapter will center around the learning rate $\\alpha$</li>\n</ul>\n<p><strong>Gradient descent</strong></p>\n<ul>\n<li>$\\theta_j:=\\theta_j-\\alpha \\frac∂{∂\\theta_j}J(\\theta)$</li>\n<li>“Debugging”: How to make sure gradient descent is working correctly.</li>\n<li>How to choose learning rate $\\alpha$</li>\n</ul>\n<p>Declare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration.</p>\n<blockquote>\n<p>这是因为若将$min_\\theta J(\\theta)$作为纵轴，将迭代次数作为横轴，那么得到的是是一个近似$y=|\\frac 1x|$的图像。当迭代次数达到一定量$(\\epsilon)$后，梯度下降的量就几乎可以忽略不计了，所以该测试就判断函数已经收敛。不过要选择一个合适的阈值（threshold，$\\epsilon$）并不容易。</p>\n</blockquote>\n<blockquote>\n<p>如果你的图像并不是上述的样子，那么说明你的$\\alpha$值选取的不恰当。例如图像在0点附近形如指数函数图像，或者呈波浪形，就意味着你的$\\alpha$值过大了，函数无法收敛。</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<ul>\n<li>If $\\alpha$ is too small: slow convergence.</li>\n<li>If $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.</li>\n</ul>\n<p>Recommended choices for $\\alpha$:</p>\n<p>…, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1,… </p>\n<h3 id=\"4-5-Features-and-polynomial-regression\"><a href=\"#4-5-Features-and-polynomial-regression\" class=\"headerlink\" title=\"4.5 Features and polynomial regression\"></a>4.5 Features and polynomial regression</h3><blockquote>\n<p>根据特征选择算法以提高效率；使用多项式回归来拟合复杂函数</p>\n</blockquote>\n<p>Housing prices prediction:</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times frontage+\\theta_2\\times depth<br>$$</p>\n<blockquote>\n<p>It’s better to use $area$ which is equal to $frontage\\times depth$ as new feature.</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1\\times area<br>$$</p>\n<h4 id=\"Choice-of-feature\"><a href=\"#Choice-of-feature\" class=\"headerlink\" title=\"Choice of feature\"></a>Choice of feature</h4><p>Suppose we have a graph with the price of a house on the vertical axis and the area (size) on the horizontal axis, and we need to choose the function to fit the data recorded on the graph.</p>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2<br>$$</p>\n<blockquote>\n<p>使用二次函数来拟合房价-面积曲线可能在一定范围内是合适的，但是二次函数曲线一定会在达到顶点后下降，房价却不会。所以这并不是一个好的选择。</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2(size)^2+\\theta_3(size)^3<br>$$</p>\n<blockquote>\n<p>也许是可行的，但要注意应用特征缩放，从而使三个特征值$(size,(size)^2,(size)^3)$都在大致相同的范围内</p>\n</blockquote>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1(size)+\\theta_2\\sqrt{(size)}<br>$$</p>\n<blockquote>\n<p>上升的曲线，斜率随着x（area）变大逐渐变小，增长趋于平缓，似乎也不错</p>\n</blockquote>\n<p><strong>Summary</strong>:</p>\n<p>You have a choice in what features to use to fix more complex functions to your data!</p>\n<h3 id=\"4-6-Normal-equation-unfinished\"><a href=\"#4-6-Normal-equation-unfinished\" class=\"headerlink\" title=\"4.6 Normal equation (unfinished)\"></a>4.6 Normal equation (unfinished)</h3><blockquote>\n<p>对于某些线性回归问题，求取参数$\\theta$最优值的方法。不同与以往的迭代算法（梯度下降的多次迭代来收敛到全局最小值），正规方程提供了一种解析解法，一次性求解$\\theta$的最优值。</p>\n</blockquote>\n<p><strong>Normal equation</strong>: Method to solve for $\\theta$ analytically.</p>\n<h4 id=\"Compare-to-Gradient-Descent\"><a href=\"#Compare-to-Gradient-Descent\" class=\"headerlink\" title=\"Compare to Gradient Descent\"></a>Compare to Gradient Descent</h4><p>$m$ training example, $n$ features.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Descent</th>\n<th>Normal Equation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Need to choice a $\\alpha$</td>\n<td>No need to choice a $\\alpha$</td>\n</tr>\n<tr>\n<td>Needs many iterations</td>\n<td>Don’t need to iterate</td>\n</tr>\n<tr>\n<td>Work well even $n$ is large</td>\n<td>Need to compute $(X^TX)^{-1}$</td>\n</tr>\n<tr>\n<td></td>\n<td>Slow if $n$ is very large</td>\n</tr>\n</tbody></table>\n<p>$$<br>\\theta=(X^TX)^{-1}X^Ty<br>$$<br>$(X^TX)^{-1}$ is inverse of matrix $(X^TX)$</p>\n<p><strong>Octave</strong>: <code>pinv(X&#39;*X)*X&#39;*y</code> </p>\n<blockquote>\n<p><code>X&#39;</code> is the transpose of $X$</p>\n<p><code>pinv</code> is a function  to compute the inverse of a matrix</p>\n</blockquote>\n<!--unfinished-->\n\n<h3 id=\"4-7-Normal-equation-and-non-invertibility-optional-unfinished\"><a href=\"#4-7-Normal-equation-and-non-invertibility-optional-unfinished\" class=\"headerlink\" title=\"4.7 Normal equation and non-invertibility (optional) (unfinished)\"></a>4.7 Normal equation and non-invertibility (optional) (unfinished)</h3><!--unfinished-->\n\n\n\n<h2 id=\"5-Octave-Tutorial-ignored\"><a href=\"#5-Octave-Tutorial-ignored\" class=\"headerlink\" title=\"5 Octave Tutorial (ignored)\"></a>5 Octave Tutorial (ignored)</h2><!--unfinished-->\n\n\n\n<h2 id=\"6-Logistic-Regression\"><a href=\"#6-Logistic-Regression\" class=\"headerlink\" title=\"6 Logistic Regression\"></a>6 Logistic Regression</h2><blockquote>\n<p>回归算法</p>\n</blockquote>\n<h3 id=\"6-1-Classification\"><a href=\"#6-1-Classification\" class=\"headerlink\" title=\"6.1 Classification\"></a>6.1 Classification</h3><p><strong>Classification</strong></p>\n<p>$y\\in {0,1} $</p>\n<p>0: “Negative Class” (e.g., benign tumor)</p>\n<p>1: “Positive Class” (e.g., malignant tumor)</p>\n<blockquote>\n<p>There are multi-class problems as well that y can take value from 0, 1, 2, 3,…</p>\n</blockquote>\n<p>Learning regression isn’t fit the classification problem</p>\n<p>In the <a href=\"https://www.bilibili.com/video/BV164411b7dx?p=32&t=160\">video</a> there is an example to explain it. </p>\n<p>Another example:</p>\n<p>Classification: y = 0 or 1</p>\n<p>​    $H_\\theta(x)$ can be $&gt;1$ or $&lt;0$ if we use the linear regression</p>\n<blockquote>\n<p>Obviously, the label is either 0 or 1.</p>\n</blockquote>\n<p>Logistic Regression: $0\\le h_\\theta(x)\\le1$</p>\n<blockquote>\n<p>This is a classification algorithm whose output always between 1 and 0. Besides, it’s a classification algorithm instead of linear regression algorithm though there is a “regression” in its name.</p>\n</blockquote>\n<h3 id=\"6-2-Hypothesis-Representation\"><a href=\"#6-2-Hypothesis-Representation\" class=\"headerlink\" title=\"6.2 Hypothesis Representation\"></a>6.2 Hypothesis Representation</h3><blockquote>\n<p>假设陈述</p>\n</blockquote>\n<ul>\n<li>What is the function we’re going to use to representation hypothesis when we have a classification problem.</li>\n</ul>\n<h4 id=\"Logistic-Regression-Model\"><a href=\"#Logistic-Regression-Model\" class=\"headerlink\" title=\"Logistic Regression Model\"></a>Logistic Regression Model</h4><p>​    Want $0\\le h_\\theta(x)\\le1$</p>\n<p>$$<br>h_\\theta(x)=g(\\theta^Tx)<br>$$<br><strong>Sigmoid function (Logistic function)</strong>:<br>$$<br>g(z)=\\frac1{1+e^{-z}}<br>$$<br><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114255.png\" alt=\"image-20210523152323634\" style=\"zoom:67%;\" /></p>\n<blockquote>\n<p>It’s graph is likely function $y=\\frac12\\tan^{-1}x+\\frac12$, it has two asymptote at 0 and 1. And, $h_\\theta(0)=0.5$</p>\n</blockquote>\n<p>Thus,<br>$$<br>h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}<br>$$</p>\n<blockquote>\n<p>$\\theta^Tx\\ge0$ then  $h_\\theta(x)=1$, $\\theta^Tx&lt;0$ then  $h_\\theta(x)=0$</p>\n</blockquote>\n<p><strong>Interpretation of Hypothesis Output</strong></p>\n<p>$h_\\theta(x)=$ estimated probability that y = 1 on input x</p>\n<p>Example: if $x=\\begin{bmatrix}x_0 \\x_1 \\end{bmatrix}=\\begin{bmatrix}1 \\ tumorSize\\end{bmatrix}$</p>\n<p>​                $h_\\theta(x)=0.7$</p>\n<p>Tell patient that 70% chance of tumor being malignant.</p>\n<h4 id=\"Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\"><a href=\"#Mathematical-formula-definition-of-the-hypothesis-for-logistic-regression\" class=\"headerlink\" title=\"Mathematical formula definition of the hypothesis for logistic regression\"></a>Mathematical formula definition of the hypothesis for logistic regression</h4><p>“Probability that y=1, given x, parameterized by $\\theta$”:<br>$$<br>P(y=0|x;\\theta)+P(y=1|x;\\theta)=1\\<br>P(y=0|x;\\theta)=1-P(y=1|x;\\theta)<br>$$</p>\n<h3 id=\"6-3-Decision-boundary\"><a href=\"#6-3-Decision-boundary\" class=\"headerlink\" title=\"6.3 Decision boundary\"></a>6.3 Decision boundary</h3><blockquote>\n<p>决策界限</p>\n</blockquote>\n<ul>\n<li>What logistic regression hypothesis function is computing?</li>\n</ul>\n<p>According to Logistic regression, </p>\n<p>suppose predict “$y=1$” If $h_\\theta(x)\\ge0.5$</p>\n<p>predict “$y=0$” If $h_\\theta(x)\\le0.5$</p>\n<h4 id=\"Decision-Boundary\"><a href=\"#Decision-Boundary\" class=\"headerlink\" title=\"Decision Boundary\"></a>Decision Boundary</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524114302.png\" alt=\"image-20210523184323276\" style=\"zoom:67%;\" />\n\n<p>Suppose the variable procedure to be specified. </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)$</p>\n<p>And, $\\theta=\\begin{bmatrix}-3 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Predict “$y=1$”if $-3+x_1+x_2\\ge0$</p>\n<p>​                                     $x_1+x_2\\ge3$</p>\n<p>The magenta line is called <strong>Decision Boundary</strong>.</p>\n<blockquote>\n<p>The decision boundary line is the property of the hypothesis and of the parameters, and not a property of a data set.</p>\n</blockquote>\n<h4 id=\"Non-linear-decision-boundaries\"><a href=\"#Non-linear-decision-boundaries\" class=\"headerlink\" title=\"Non-linear decision boundaries\"></a>Non-linear decision boundaries</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524160449.png\" alt=\"image-20210524160448985\" style=\"zoom:50%;\" />\n\n<p>Assuming hypothesis likes this: </p>\n<p>$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2)$</p>\n<p>And assuming chosen parameters as $\\theta=\\begin{bmatrix}-1 \\ 0 \\ 0 \\ 1\\ 1\\end{bmatrix}$</p>\n<p>Then, predict “$y=1$” if $-1+x_1^2+x_2^2\\ge0$</p>\n<p>​                                               $x_1^2+x_2^2\\ge1$</p>\n<blockquote>\n<p>The training set used to fit the parameters $\\theta$</p>\n</blockquote>\n<h3 id=\"6-4-Cost-function\"><a href=\"#6-4-Cost-function\" class=\"headerlink\" title=\"6.4 Cost function\"></a>6.4 Cost function</h3><ul>\n<li><p>How to automatically choose the parameters $\\theta$ to a training set.</p>\n</li>\n<li><p>Define the optimization objective or the cost function that used to fit the parameters.</p>\n</li>\n</ul>\n<p>Here is to supervised learning problem of fitting a logistic regression model.</p>\n<p>Training set: ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})}$</p>\n<p>$m$ examples $\\qquad x\\in\\begin{bmatrix}x_0 \\ x_1 \\ \\cdots \\ x_n\\end{bmatrix} \\qquad x_0=1,y\\in{0,1}$</p>\n<p>$h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<p>How to choose parameters $\\theta$ ? (The next sections will focus on this problem)</p>\n<h4 id=\"Cost-function-Logistic-regression-cost-function\"><a href=\"#Cost-function-Logistic-regression-cost-function\" class=\"headerlink\" title=\"Cost function - Logistic regression cost function\"></a>Cost function - Logistic regression cost function</h4><p>​    Linear regression: $J(\\theta) =\\frac {1}{m}\\sum_{i=1}^m\\frac12(h_θ(x^{(i)})-y^{(i)})^2$</p>\n<p>​    $Cost(h_\\theta(x),y)=\\frac12(h_\\theta(x),y)^2$</p>\n<blockquote>\n<p>我们先尝试直接将线性回归函数转化为逻辑回归函数，事实上后者将会是一个参数为$\\theta$的非凸函数（non-convex function），这是因为这一部分（$\\frac1{1+e^{-\\theta^T x}}$）是很复杂的非线性函数。</p>\n</blockquote>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524170532.png\" alt=\"image-20210524170532375\" style=\"zoom:80%;\" />\n\n\n\n<p><strong>Logistic regression cost function</strong><br>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases}<br>$$</p>\n<p>If y = 1</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171531.png\" alt=\"image-20210524171531895\" style=\"zoom: 80%;\" />\n\n<p>If y = 0</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524171740.png\" alt=\"image-20210524171740822\" style=\"zoom:80%;\" />\n\n\n\n<h3 id=\"6-5-simplified-cost-function-and-gradient-descent\"><a href=\"#6-5-simplified-cost-function-and-gradient-descent\" class=\"headerlink\" title=\"6.5 simplified cost function and gradient descent\"></a>6.5 simplified cost function and gradient descent</h3><ul>\n<li><p>Figure out a simpler way to write the cost function</p>\n</li>\n<li><p>Also figure out how to apply gradient descent to fit the parameters of logistic regression</p>\n</li>\n</ul>\n<h4 id=\"Logistic-regression-cost-function\"><a href=\"#Logistic-regression-cost-function\" class=\"headerlink\" title=\"Logistic regression cost function\"></a>Logistic regression cost function</h4><p>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})<br>$$</p>\n<p>$$<br>Cost(h_\\theta(x),y)=<br>\\begin{cases}<br>-\\log(h_\\theta(x)) \\quad &amp; if ;y=1  \\[1ex]<br>-\\log(1-h_\\theta(x))\\quad &amp; if ;y=0<br>\\end{cases} \\ ;\\<br>Note:y=0;or;1;always<br>$$</p>\n<p>Compress them into one equation:</p>\n<p>$$<br>Cost(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))<br>$$</p>\n<p><strong>Logistic regression cost function</strong><br>$$<br>J(\\theta) =\\frac {1}{m}\\sum_{i=1}^mCost(h_θ(x^{(i)})-y^{(i)})\\<br>\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad;;;<br>=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]<br>$$</p>\n<blockquote>\n<p>为什么用这个函数作为逻辑回归的代价函数：这个式子是从统计学中的极大似然法（the principle maximum likelihood estimation）得来的，它是统计学中为不同模型快速寻找参数的方法。并且它还拥有一个良好的性质：它是凸函数。</p>\n</blockquote>\n<p>To fit parameters $\\theta$:<br>$$<br>\\min_\\theta J(\\theta)<br>$$</p>\n<blockquote>\n<p>find the $\\theta$ which minimizes $J(\\theta)$</p>\n</blockquote>\n<p>To make prediction given new $x$:</p>\n<p>​    Output $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$</p>\n<blockquote>\n<p>So how to minimize $J(\\theta)$, or how to choose parameter $ \\theta$ ?</p>\n</blockquote>\n<h4 id=\"Implementation-of-logistic-regression\"><a href=\"#Implementation-of-logistic-regression\" class=\"headerlink\" title=\"Implementation of logistic regression\"></a>Implementation of logistic regression</h4><p><strong>Gradient Descent</strong></p>\n<p>$J(\\theta)=-\\frac1m[\\sum^m_{i=1}y^{(i)}\\log{h_\\theta(x^{(i)})} +(1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]$</p>\n<p>What $\\min_\\theta J(\\theta)$:    </p>\n<p>​    Repeat {</p>\n<p>​                $\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j$        </p>\n<p>​        }        (simultaneously update for $\\theta_j$ for $j=0,…,n$)</p>\n<blockquote>\n<p>Algorithm looks identical to linear regression! But pay attention to $h_\\theta(x)$. In linear regression, $h_\\theta(x)=\\theta^Tx$, and in logistic regression, $h_\\theta(x)=\\frac1{1+e^{-\\theta^T x}}$. The definition of hypothesis has changed, thus actually they are two different things.</p>\n</blockquote>\n<h3 id=\"6-6-Advanced-optimization\"><a href=\"#6-6-Advanced-optimization\" class=\"headerlink\" title=\"6.6 Advanced optimization\"></a>6.6 Advanced optimization</h3><ul>\n<li>Some advanced optimization algorithms </li>\n<li>Some advanced optimization concepts</li>\n</ul>\n<blockquote>\n<p>大大提高逻辑回归的计算速度。:see_no_evil:</p>\n</blockquote>\n<h4 id=\"Optimization-algorithm\"><a href=\"#Optimization-algorithm\" class=\"headerlink\" title=\"Optimization algorithm\"></a>Optimization algorithm</h4><p>Cost function $J(\\theta)$. Want $\\min_\\theta J(\\theta)$.</p>\n<p>Given $\\theta$, we have code that compute</p>\n<ul>\n<li>$J(\\theta)$</li>\n<li>$\\frac∂{∂\\theta_j}J(\\theta)$      (For $j=0,1,…,n$)</li>\n</ul>\n<p>Optimization algorithms:</p>\n<ul>\n<li>Gradient descent</li>\n<li>Conjugate gradient</li>\n<li>BFGS (共轭梯度法)</li>\n<li>L-BFGS</li>\n</ul>\n<p>Others algorithm compare to gradient descent</p>\n<table>\n<thead>\n<tr>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No need to manually pick $\\alpha$</td>\n<td>More complex</td>\n</tr>\n<tr>\n<td>Often faster than gradient descent</td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>These complex algorithms have a “clever inner-loop” called line search algorithm that automatically tries out different values for learning rate $\\alpha$ and automatically pick a good one. In fact these algorithms do much more than that.</p>\n<p>:older_man:建议不要试图实现这些算法，除非你是数值计算方面的专家，甚至你不必完全理解这些算法就能很好的使用它们。</p>\n</blockquote>\n<h3 id=\"6-7-Multi-class-classification-One-vs-all-unfinished\"><a href=\"#6-7-Multi-class-classification-One-vs-all-unfinished\" class=\"headerlink\" title=\"6.7 Multi-class classification: One-vs-all (unfinished)\"></a>6.7 Multi-class classification: One-vs-all (unfinished)</h3><ul>\n<li>How to get logistic regression to work for multi-class classification problems</li>\n<li>One-versus-all classification algorithm</li>\n</ul>\n<h4 id=\"Multiclass-classification\"><a href=\"#Multiclass-classification\" class=\"headerlink\" title=\"Multiclass classification\"></a>Multiclass classification</h4><!--unfinished-->\n\n\n\n<h2 id=\"7-Regularization-unfinished\"><a href=\"#7-Regularization-unfinished\" class=\"headerlink\" title=\"7 Regularization (unfinished)\"></a>7 Regularization (unfinished)</h2><blockquote>\n<p>正则化，是解决（改善）过拟合问题的手段之一</p>\n</blockquote>\n<h3 id=\"7-1-The-problem-of-overfitting\"><a href=\"#7-1-The-problem-of-overfitting\" class=\"headerlink\" title=\"7.1 The problem of overfitting\"></a>7.1 The problem of overfitting</h3><ul>\n<li>Explain what is overfitting problem</li>\n</ul>\n<p>Example: Linear regression (housing prices)</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524215954.png\" alt=\"image-20210524215953904\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>“Underfit””High bias”,高偏差：强行用直线拟合曲线分布的数据，就像“持有偏见，固执认为房价变化就是线性的”，导致拟合结果偏差很大</p>\n</blockquote>\n<blockquote>\n<p>“Overfit””High variance”,高方差</p>\n</blockquote>\n<h4 id=\"Overfitting\"><a href=\"#Overfitting\" class=\"headerlink\" title=\"Overfitting\"></a>Overfitting</h4><p>If we have too many features, the learned hypothesis may fit the training set very well ($J(θ)=\\frac {1}{2m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples(predict prices on new examples).</p>\n<p>Example: Logistic regression</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210524221122.png\" alt=\"image-20210524221122755\" style=\"zoom:33%;\" />\n\n<h4 id=\"Addressing-overfitting\"><a href=\"#Addressing-overfitting\" class=\"headerlink\" title=\"Addressing overfitting\"></a>Addressing overfitting</h4><p>If we have la lot of features, and very little training data, then overfitting can become a problem.</p>\n<p>Two main options:</p>\n<ol>\n<li>Reduce number of features<ul>\n<li>Manually select which features to keep (but some feature must be abandoned)</li>\n<li>Model selection algorithm (later will explain)</li>\n</ul>\n</li>\n<li>Regularization<ul>\n<li>Keep all the features, but reduce magnitude/values of parameters $\\theta_j$</li>\n<li>Works well when we have a lot of features, each of which contributes a bit to prediction $y$.</li>\n</ul>\n</li>\n</ol>\n<!--unfinished-->\n\n\n\n<h2 id=\"8-Neural-Networks-Representation\"><a href=\"#8-Neural-Networks-Representation\" class=\"headerlink\" title=\"8 Neural Networks: Representation\"></a>8 Neural Networks: Representation</h2><h3 id=\"8-1-Non-linear-hypothesis\"><a href=\"#8-1-Non-linear-hypothesis\" class=\"headerlink\" title=\"8.1 Non-linear hypothesis\"></a>8.1 Non-linear hypothesis</h3><p>Why we need Neural Networks?</p>\n<blockquote>\n<p>当特征很多，线性回归和逻辑回归就不那么好用了。即使他们得出了能够拟合当前样本的结论，该结果也很有可能是过拟合的。</p>\n</blockquote>\n<p>The neural networks which turns out to be a much better way to learn complex nonlinear hypothesis, even when your input feature space (n) is large.</p>\n<h3 id=\"8-2-Neurons-and-the-brain\"><a href=\"#8-2-Neurons-and-the-brain\" class=\"headerlink\" title=\"8.2 Neurons and the brain\"></a>8.2 Neurons and the brain</h3><p><strong>History of  neural networks</strong></p>\n<ul>\n<li>Origins: Algorithms that try to mimic the brain.</li>\n<li>WAs very widely used in 80s and early 90s; popularity diminished in late 90s.</li>\n<li>Recent resurgence: State-of-the-art technique for many applications.</li>\n</ul>\n<p><strong>The “one learning algorithm” hypothesis</strong></p>\n<ul>\n<li>Neuro-rewiring experiments</li>\n<li>Sensor representations in the brain</li>\n</ul>\n<h3 id=\"8-3-Model-representation-I\"><a href=\"#8-3-Model-representation-I\" class=\"headerlink\" title=\"8.3 Model representation I\"></a>8.3 Model representation I</h3><ul>\n<li>How we represent  Neural Networks (hypothesis or model).</li>\n</ul>\n<h4 id=\"Neuron-model-Logistic-unit\"><a href=\"#Neuron-model-Logistic-unit\" class=\"headerlink\" title=\"Neuron model: Logistic unit\"></a>Neuron model: Logistic unit</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151235.png\" alt=\"image-20210525151235213\" style=\"zoom: 67%;\" />\n\n<p><strong>Sigmoid (logistic) activation function</strong></p>\n<blockquote>\n<p>这里的激活函数(activation function)是指代非线性函数$g(z)=\\frac1{1+e^{-z}}$的另一个术语</p>\n</blockquote>\n<p>Sometimes we add an extra $x_0$ node (if necessary) called bias unit (偏置单元) or the bias neuron (偏置神经元). It’s always equal to 1 so sometime we don’t draw it.</p>\n<p>In the neural networks literature, the parameters of model $\\theta$ is also called <strong>weights of a model</strong>.</p>\n<h4 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h4><img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525151843.png\" alt=\"image-20210525151843255\" style=\"zoom:67%;\" />\n\n<p>Layer 1: Input layer</p>\n<p>Layer 2 : Hidden layer</p>\n<p>Layer 3: Output layer</p>\n<p>$a_i^{(j)}=$ “activation” of unit $i$ in layer $j$.</p>\n<p>$\\Theta^{(j)}=$ matrix of weights controlling function mapping form layer $j$ to layer $j+1$</p>\n<blockquote>\n<p>可以形象的看作神经网络被这些矩阵（$\\Theta^{(j)}$）参数化，因而这些矩阵也被称作权重矩阵（matrix of weights）。权重矩阵控制着从某一层到下一层的映射。</p>\n</blockquote>\n<p>$$<br>a_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3) \\<br>a_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2+\\Theta_{23}^{(1)}x_3) \\<br>a_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2+\\Theta_{33}^{(1)}x_3) \\<br>h_\\Theta(x)=a_1^{(3)}=g(\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)})<br>$$</p>\n<p>If networks has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$.</p>\n<blockquote>\n<p>The superscript $j$ in parentheses means that these values associated with layer $j$</p>\n</blockquote>\n<h3 id=\"8-4-Model-representation-II\"><a href=\"#8-4-Model-representation-II\" class=\"headerlink\" title=\"8.4 Model representation II\"></a>8.4 Model representation II</h3><ul>\n<li>How to carry out computation efficiently and show a vectorized implementation.</li>\n<li>Intuition about why these neural network representation</li>\n</ul>\n<h4 id=\"Forward-propagation-Vectorized-implementation\"><a href=\"#Forward-propagation-Vectorized-implementation\" class=\"headerlink\" title=\"Forward propagation: Vectorized implementation\"></a>Forward propagation: Vectorized implementation</h4><p><strong>Define:</strong></p>\n<p>$z_1^{(2)}=\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2+\\Theta_{13}^{(1)}x_3$</p>\n<p>Thus,</p>\n<p>$a_1^{(2)}=z_1^{(2)}$ Similarly, $a_2^{(2)}=z_2^{(2)}$, $a_3^{(2)}=z_3^{(2)}$</p>\n<p>We observe that these equations are very much like matrix multiplication. Therefore we try to vectorize the neural network computation.</p>\n<p><strong>Define:</strong><br>$$<br>x=\\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ x_3\\end{bmatrix} \\qquad z^{(2)}=\\begin{bmatrix}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)}\\end{bmatrix}<br>$$<br>Further towards vectorization:<br>$$<br>z^{(2)}=\\Theta^{(1)}x=\\Theta^{(1)}a^{(1)} \\<br>a^{(2)}=g(z^{(2)})<br>$$</p>\n<blockquote>\n<p>$z^{(2)}$ and $a^{(2)}$ are both 3-dimensional vectors. Function $g$ will process each element in $z^{(2)}$ one by one.</p>\n</blockquote>\n<p>Next, add bias unit: $a_0^{(2)}=1$. Notice that $a^{(2)}\\in \\mathbb R^4$</p>\n<p>$z^{(3)}=\\Theta^{(2)}a^{(2)}$</p>\n<p>$h_\\Theta(x)=a^{(3)}=g(z^{(3)})$</p>\n<blockquote>\n<p>$z^{(3)}=\\Theta_{10}^{(2)}a_0^{(2)}+\\Theta_{11}^{(2)}a_1^{(2)}+\\Theta_{12}^{(2)}a_2^{(2)}+\\Theta_{13}^{(2)}a_3^{(2)}$ if you review to the neural networks function.</p>\n</blockquote>\n<p>This process of computing $h(x)$ is also called <strong>forward propagation</strong>, because we start off with the activations of the input-units and then we sort of forward-propagation that to the hidden layer and repeat this process until arriving output layer. The formula we have got is relatively an efficient  way of computing $h(x)$.</p>\n<h4 id=\"Neural-Network-learning-its-own-features\"><a href=\"#Neural-Network-learning-its-own-features\" class=\"headerlink\" title=\"Neural Network learning its own features\"></a>Neural Network learning its own features</h4><blockquote>\n<p>如果只关注第二层和第三层，那么神经网络的行为类似于逻辑回归。然而神经网络输出层的输入是隐藏层计算或是说学习的结果（$a_1^{(2)},a_2^{(2)},a_3^{(2)}…$），而不是逻辑回归中初始的特征项$x_1,x_2,x_3…$ 。前者较后者更适合作为假设参数，因此神经网络算法具备灵活快速尝试学习任意特征项，处理更多复杂特征的能力。</p>\n</blockquote>\n<h4 id=\"Other-network-architectures\"><a href=\"#Other-network-architectures\" class=\"headerlink\" title=\"Other network architectures\"></a>Other network architectures</h4><p>The way that neural networks are connected are called the architecture (神经网络的架构). So the architecture refers to how different neurons are connected to each other.</p>\n<img src=\"https://raw.githubusercontent.com/zhuyihapi/picture/main/20210525165812.png\" alt=\"image-20210525165812274\" style=\"zoom:50%;\" />\n\n\n\n<h3 id=\"8-5-Examples-and-intoitions-unfinished\"><a href=\"#8-5-Examples-and-intoitions-unfinished\" class=\"headerlink\" title=\"8.5 Examples and intoitions (unfinished)\"></a>8.5 Examples and intoitions (unfinished)</h3><ul>\n<li><p>A detail example which shows how a neural network can compute a complex nonlinear function of the input</p>\n</li>\n<li><p>Why neural network can be used to learn complex nonlinear hypothesis</p>\n</li>\n</ul>\n<h2 id=\"9-Neural-Networks-Learning-unfinished\"><a href=\"#9-Neural-Networks-Learning-unfinished\" class=\"headerlink\" title=\"9 Neural Networks: Learning (unfinished)\"></a>9 Neural Networks: Learning (unfinished)</h2><h2 id=\"10-Advice-for-applying-machine-learning-unfinished\"><a href=\"#10-Advice-for-applying-machine-learning-unfinished\" class=\"headerlink\" title=\"10 Advice for applying machine learning (unfinished)\"></a>10 Advice for applying machine learning (unfinished)</h2><p>10.1 Decide what to try next</p>\n<p>10.2 Evaluating a hypothesis</p>\n<p>10.3 Model selection and training/validation/test sets</p>\n<h2 id=\"11-Machine-Learning-system-design-unfinished\"><a href=\"#11-Machine-Learning-system-design-unfinished\" class=\"headerlink\" title=\"11 Machine Learning system design (unfinished)\"></a>11 Machine Learning system design (unfinished)</h2><p>11.1 Prioritizing what to work on: Spam classification example</p>\n<h2 id=\"12-Support-Vector-Machines-unfinished\"><a href=\"#12-Support-Vector-Machines-unfinished\" class=\"headerlink\" title=\"12 Support Vector Machines (unfinished)\"></a>12 Support Vector Machines (unfinished)</h2><p>12.1 Optimizaion object</p>\n<ul>\n<li>Sometimes gives a cleaner and a more powerful way of learning complex nonlinear functions</li>\n</ul>\n<h2 id=\"13-Clustering\"><a href=\"#13-Clustering\" class=\"headerlink\" title=\"13 Clustering\"></a>13 Clustering</h2><h3 id=\"13-0-Notation\"><a href=\"#13-0-Notation\" class=\"headerlink\" title=\"13.0 Notation\"></a>13.0 Notation</h3><p>Train set: ${x^{(1)},x^{(2)},x^{(3)},…x^{(m)}}$  (without labels)</p>\n<h3 id=\"13-1-K-means-algorithm\"><a href=\"#13-1-K-means-algorithm\" class=\"headerlink\" title=\"13.1 K-means algorithm\"></a>13.1 K-means algorithm</h3><blockquote>\n<p>K均值算法</p>\n</blockquote>\n<p>K-means is a iterative algorithm. The preparation of the algorithm is to randomly initialize two (depends on how many cluster you want to assign) point, which called the cluster centroids (聚类中心). Then go through each point, detect and record which centre point they are closer to. Second is a move centroid step to the center of all the points in the same group. Repeat these two steps until the grouping of the points no longer changes.</p>\n<p><strong>Input</strong>:</p>\n<ul>\n<li>$K$ (number of clusters)</li>\n<li>Training set ${x^{(1)},x^{(2)},x^{(3)},…x^{(m)}}$</li>\n</ul>\n<p>$x^{(i)}\\in \\mathbb R^n$ (drop $x_0=1$ convention)</p>\n<h4 id=\"K-means-algorithm\"><a href=\"#K-means-algorithm\" class=\"headerlink\" title=\"K-means algorithm\"></a>K-means algorithm</h4><p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,…,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {<br>    // <u>cluster assignment step</u></p>\n<p>​    for $i$ = 1 to $m$</p>\n<p>​        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$        (calculate $c^{(i)}$ though $\\min_k||x^{(i)}-\\mu_k||$)</p>\n<p>​    // <u>move centroids step</u></p>\n<p>​    for $k$ = 1 to $K$</p>\n<p>​        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<blockquote>\n<p>$K$ means the number of centroids and the $k$ means the index of each centriod.</p>\n<p>If you have a cluster with no points assigned to it, the usual practice is to delete it.</p>\n</blockquote>\n<h3 id=\"13-2-Optimization-objective\"><a href=\"#13-2-Optimization-objective\" class=\"headerlink\" title=\"13.2 Optimization objective\"></a>13.2 Optimization objective</h3><ul>\n<li>How we can use it to help K-means algorithm find better clusters and avoid local optima.</li>\n</ul>\n<h4 id=\"K-means-optimization-objective\"><a href=\"#K-means-optimization-objective\" class=\"headerlink\" title=\"K-means optimization objective\"></a>K-means optimization objective</h4><p>$c^{(i)}$ = index of cluster (1,2,…,$K$) to which example $x^{(i)}$ is currently assigned</p>\n<p>$\\mu_k$ = cluster centroid $k$ ($\\mu_k\\in \\mathbb R^n$)</p>\n<p>$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p>\n<h4 id=\"Optimization-objective\"><a href=\"#Optimization-objective\" class=\"headerlink\" title=\"Optimization objective:\"></a>Optimization objective:</h4><p>$$<br>J(c^{(1)},… c^{(m)},\\mu_1,…,\\mu_K)=\\frac1m\\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2 \\<br>\\min_{c^{(1)},… c^{(m)},\\mu_1,…,\\mu_K} J(c^{(1)},… c^{(m)},\\mu_1,…,\\mu_K)<br>$$</p>\n<blockquote>\n<p>The cost function $J$ is also called discotion function</p>\n</blockquote>\n<p><strong>Review K-means algorithm</strong></p>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,…,\\mu_K \\in \\mathbb R^n$</p>\n<p>Repeat {</p>\n<p>​    //minimize $J$ though $c^{(i)}$, $\\mu_k$ fixed</p>\n<p>​    for $i$ = 1 to $m$</p>\n<p>​        $c^{(i)}$ := index (form 1 to $K$) of cluster controid close to $x^{(i)}$</p>\n<p>​    //minimize $J$ though $\\mu_k$, $c^{(i)}$ fixed</p>\n<p>​    for $k$ = 1 to $K$</p>\n<p>​        $\\mu_k$ := average (mean) of points assigned to cluster $k$<br>}</p>\n<h3 id=\"13-3-Randomly-initialization\"><a href=\"#13-3-Randomly-initialization\" class=\"headerlink\" title=\"13.3 Randomly initialization\"></a>13.3 Randomly initialization</h3><ul>\n<li>How to initialize K-means</li>\n<li>How to avoid local optima</li>\n</ul>\n<blockquote>\n<p>Randomly initialize $K$ cluster centroids $\\mu_1,\\mu_2,…,\\mu_K \\in \\mathbb R^n$</p>\n</blockquote>\n<p><strong>Randomly initialization</strong></p>\n<p>Usually, we have $K&lt;m$</p>\n<p>Randomsly pick $K$ training examples.</p>\n<p>Set $\\mu_1,…,\\mu_K$ equal to these $K$ examples.</p>\n<img src=\"../../../../../Library/Application Support/typora-user-images/image-20210528155332940.png\" alt=\"image-20210528155332940\" style=\"zoom: 33%;\" />\n\n<blockquote>\n<p>Local optima we should avoid, but randomly initialization may cause this situation.</p>\n</blockquote>\n<blockquote>\n<p>简而言之，为了避免这种情况的发生，我们可以先初始化一千遍，然后选择畸变最小的那一种情况，也就是最有潜力，最不可能陷入局部最优的情况来进行接下来的运算。</p>\n</blockquote>\n<h3 id=\"13-4-Choosing-the-number-of-cluster\"><a href=\"#13-4-Choosing-the-number-of-cluster\" class=\"headerlink\" title=\"13.4 Choosing the number of cluster\"></a>13.4 Choosing the number of cluster</h3><ul>\n<li>Manual</li>\n<li>Elbow method</li>\n<li>Later downstream purpose</li>\n</ul>\n<h2 id=\"14-Dimensionaliy-Reduction-unfinished\"><a href=\"#14-Dimensionaliy-Reduction-unfinished\" class=\"headerlink\" title=\"14 Dimensionaliy Reduction (unfinished)\"></a>14 Dimensionaliy Reduction (unfinished)</h2><p>14.1 Motivation I: Data Compression</p>\n<p>14.2 Motivation II: Visualization</p>\n<p>14.3 Principle Component Analysis problem formulation (PCA)</p>\n<blockquote>\n<p>主成分分析法</p>\n</blockquote>\n<ul>\n<li>Compression algorithm</li>\n</ul>\n<p>14.4 Principle Component Analysis algorithm</p>\n<p>Data preprocessing</p>\n<h2 id=\"15-Anomaly-Detection-unfinished\"><a href=\"#15-Anomaly-Detection-unfinished\" class=\"headerlink\" title=\"15 Anomaly Detection (unfinished)\"></a>15 Anomaly Detection (unfinished)</h2><p>15.1 Problem motivation</p>\n<h2 id=\"16-Recommeder-Systems-unfinished\"><a href=\"#16-Recommeder-Systems-unfinished\" class=\"headerlink\" title=\"16 Recommeder Systems (unfinished)\"></a>16 Recommeder Systems (unfinished)</h2><h2 id=\"17-Large-Scale-Machine-Learning-unfinished\"><a href=\"#17-Large-Scale-Machine-Learning-unfinished\" class=\"headerlink\" title=\"17 Large Scale Machine Learning (unfinished)\"></a>17 Large Scale Machine Learning (unfinished)</h2><h3 id=\"17-1-Learning-with-large-datasets\"><a href=\"#17-1-Learning-with-large-datasets\" class=\"headerlink\" title=\"17.1 Learning with large datasets\"></a>17.1 Learning with large datasets</h3><p>$$<br>\\theta_j :=\\theta_j -\\alpha\\frac {1}{m}\\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p>\n<blockquote>\n<p>​    对于上亿的数据规模来说，计算梯度下降中的求和函数是难以承受的负担</p>\n</blockquote>\n<h3 id=\"17-2-Stochastic-gradient-descent\"><a href=\"#17-2-Stochastic-gradient-descent\" class=\"headerlink\" title=\"17.2 Stochastic gradient descent\"></a>17.2 Stochastic gradient descent</h3><h4 id=\"Review-linear-regression-with-gradient-descent\"><a href=\"#Review-linear-regression-with-gradient-descent\" class=\"headerlink\" title=\"Review linear regression with gradient descent\"></a>Review linear regression with gradient descent</h4><!-- unfinished -->\n\n<h4 id=\"Batch-gradient-descent\"><a href=\"#Batch-gradient-descent\" class=\"headerlink\" title=\"Batch gradient descent\"></a>Batch gradient descent</h4><h4 id=\"Stochastic-gradient-descent\"><a href=\"#Stochastic-gradient-descent\" class=\"headerlink\" title=\"Stochastic gradient descent\"></a>Stochastic gradient descent</h4><h3 id=\"17-3-Mini-batch-gradient-descent\"><a href=\"#17-3-Mini-batch-gradient-descent\" class=\"headerlink\" title=\"17.3 Mini-batch gradient descent\"></a>17.3 Mini-batch gradient descent</h3><h2 id=\"Markdown\"><a href=\"#Markdown\" class=\"headerlink\" title=\"Markdown\"></a>Markdown</h2><p>\\begin{bmatrix}\\end{bmatrix}</p>\n<p>\\mathbb R</p>\n<p><a href=\"https://www.jianshu.com/p/25f0139637b7\">公式</a></p>\n<p><a href=\"https://www.jianshu.com/p/e74eb43960a1\">公式2</a></p>\n<p><a href=\"https://www.jianshu.com/p/191d1e21f7ed\">语法</a></p>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2021-07-01T06:33:06.646Z","updated":"2021-07-01T06:33:06.646Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckqkodclf0002033t2wi6dw1f","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"length":367,"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}